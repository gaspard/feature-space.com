{
  "info": {
    "id": "733f7947",
    "title": "Normes sur Rⁿ et suites convergentes - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Normes sur Rⁿ et suites convergentes",
    "course": "Topologie",
    "tags": [
      "normes",
      "suites",
      "convergence",
      "topologie",
      "analyse"
    ],
    "count": 11
  },
  "cards": [
    {
      "id": "1",
      "stackId": "733f7947",
      "content": "#### Preuve de l'inégalité triangulaire renversée\n\nDémontrer que pour toute norme $\\| \\cdot \\|$ sur $\\mathbb{R}^n$ et pour tous vecteurs $x, y \\in \\mathbb{R}^n$, on a l'inégalité suivante :\n\n$|\\|x\\| - \\|y\\|| \\le \\|x - y\\|$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nL'idée est d'utiliser l'inégalité triangulaire standard de deux manières différentes.\n\n1.  Écrivez $x$ comme $x = (x-y) + y$ et appliquez l'inégalité triangulaire pour obtenir une borne pour $\\|x\\|$.\n2.  Faites de même pour $y$ en l'écrivant comme $y = (y-x) + x$.\n3.  Combinez les deux inégalités obtenues pour encadrer $\\|x\\| - \\|y\\|$.\n\n</details>",
      "solution": "\n\nSoient $x, y \\in \\mathbb{R}^n$. Nous utilisons l'axiome de l'inégalité triangulaire de la norme $\\| \\cdot \\|$.\n\n**Étape 1 : Majorer $\\|x\\| - \\|y\\|$**\n\nOn écrit $x = (x-y) + y$. En appliquant l'inégalité triangulaire, on obtient :\n\n$\\|x\\| = \\|(x-y) + y\\| \\le \\|x-y\\| + \\|y\\|$.\n\nEn soustrayant $\\|y\\|$ des deux côtés, on a :\n\n$\\|x\\| - \\|y\\| \\le \\|x-y\\|$. (1)\n\n**Étape 2 : Minorer $\\|x\\| - \\|y\\|$**\n\nDe la même manière, on écrit $y = (y-x) + x$. En appliquant l'inégalité triangulaire :\n\n$\\|y\\| = \\|(y-x) + x\\| \\le \\|y-x\\| + \\|x\\|$.\n\nOn sait par l'axiome d'homogénéité que $\\|y-x\\| = \\|(-1)(x-y)\\| = |-1|\\|x-y\\| = \\|x-y\\|$.\n\nL'inégalité devient donc $\\|y\\| \\le \\|x-y\\| + \\|x\\|$.\n\nEn réarrangeant les termes, on obtient :\n\n$\\|y\\| - \\|x\\| \\le \\|x-y\\|$, ce qui est équivalent à $-(\\|x\\| - \\|y\\|) \\le \\|x-y\\|$. (2)\n\n**Conclusion**\n\nEn combinant les inégalités (1) et (2), on a :\n\n$- \\|x-y\\| \\le \\|x\\| - \\|y\\| \\le \\|x-y\\|$.\n\nCeci est exactement la définition de la valeur absolue, donc on peut conclure :\n\n$|\\|x\\| - \\|y\\|| \\le \\|x - y\\|$.\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "733f7947",
      "content": "#### Vérification qu'une application est une norme\n\nDémontrer que l'application $N: \\mathbb{R}^2 \\to \\mathbb{R}_+$ définie par $N(x, y) = |x + 2y| + 3|y|$ est une norme sur $\\mathbb{R}^2$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour prouver que $N$ est une norme, vous devez vérifier les trois axiomes un par un :\n\n1.  **Séparation** : Montrez que $N(x,y) = 0$ si et seulement si $(x,y) = (0,0)$. Rappelez-vous qu'une somme de termes positifs est nulle si et seulement si chaque terme est nul.\n2.  **Homogénéité** : Calculez $N(\\lambda x, \\lambda y)$ et utilisez les propriétés de la valeur absolue pour factoriser $|\\lambda|$.\n3.  **Inégalité triangulaire** : Calculez $N(x+x', y+y')$ et utilisez l'inégalité triangulaire de la valeur absolue sur $\\mathbb{R}$ ($|a+b| \\le |a|+|b|$) pour séparer les termes en $N(x,y)$ et $N(x',y')$.\n\n</details>",
      "solution": "\n\nSoient $v = (x, y)$ et $v' = (x', y')$ des vecteurs de $\\mathbb{R}^2$, et $\\lambda \\in \\mathbb{R}$ un scalaire. Nous vérifions les trois axiomes d'une norme.\n\n**1. Axiome de séparation : $N(v) = 0 \\iff v = (0,0)$**\n\n($\\implies$) Supposons que $N(x, y) = 0$. Par définition, cela signifie $|x + 2y| + 3|y| = 0$.\n\nComme $|x + 2y| \\ge 0$ et $3|y| \\ge 0$, leur somme est nulle si et seulement si les deux termes sont nuls :\n\n$|x + 2y| = 0$ et $3|y| = 0$.\n\nLa deuxième équation, $3|y|=0$, implique $|y|=0$, donc $y=0$.\n\nEn substituant $y=0$ dans la première équation, on obtient $|x + 2(0)| = 0$, soit $|x|=0$, ce qui implique $x=0$.\n\nAinsi, $(x, y) = (0,0)$.\n\n($\\impliedby$) Supposons que $(x, y) = (0,0)$. Alors $N(0,0) = |0 + 2(0)| + 3|0| = 0+0=0$.\n\nL'axiome de séparation est donc vérifié.\n\n**2. Axiome d'homogénéité : $N(\\lambda v) = |\\lambda| N(v)$**\n\nCalculons $N(\\lambda v) = N(\\lambda x, \\lambda y)$ :\n\n$$ \\begin{align*} N(\\lambda x, \\lambda y) &= |(\\lambda x) + 2(\\lambda y)| + 3|\\lambda y| \\\\ &= |\\lambda(x + 2y)| + 3|\\lambda||y| \\\\ &= |\\lambda| |x + 2y| + 3|\\lambda||y| \\quad \\text{(propriété de la valeur absolue)} \\\\ &= |\\lambda| (|x + 2y| + 3|y|) \\\\ &= |\\lambda| N(x,y). \\end{align*} $$\n\nL'axiome d'homogénéité est vérifié.\n\n**3. Inégalité triangulaire : $N(v+v') \\le N(v) + N(v')$**\n\nCalculons $N(v+v') = N(x+x', y+y')$ :\n\n$$ \\begin{align*} N(x+x', y+y') &= |(x+x') + 2(y+y')| + 3|y+y'| \\\\ &= |(x+2y) + (x'+2y')| + 3|y+y'| \\end{align*} $$\n\nEn utilisant l'inégalité triangulaire pour la valeur absolue sur $\\mathbb{R}$ (c'est-à-dire $|a+b| \\le |a|+|b|$), on peut majorer chaque terme :\n\n$$ \\begin{align*} N(x+x', y+y') &\\le \\left(|x+2y| + |x'+2y'|\\right) + 3\\left(|y| + |y'|\\right) \\\\ &= (|x+2y| + 3|y|) + (|x'+2y'| + 3|y'|) \\\\ &= N(x,y) + N(x',y'). \\end{align*} $$\n\nL'inégalité triangulaire est vérifiée.\n\n**Conclusion**\n\nPuisque les trois axiomes sont satisfaits, $N$ est bien une norme sur $\\mathbb{R}^2$.\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "733f7947",
      "content": "#### Preuve que la norme infinie est une norme\n\nDémontrer que l'application $\\| \\cdot \\|_\\infty$ définie pour tout $x = (x_1, \\dots, x_n) \\in \\mathbb{R}^n$ par $\\|x\\|_\\infty = \\max_{1 \\le j \\le n} |x_j|$ est une norme sur $\\mathbb{R}^n$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nVérifiez les trois axiomes.\n\n1.  **Séparation** : Si le maximum des valeurs absolues des composantes est zéro, que pouvez-vous dire de chaque composante ?\n2.  **Homogénéité** : Utilisez la propriété $\\max(c \\cdot a_j) = c \\cdot \\max(a_j)$ pour un scalaire positif $c$. Ici, $c = |\\lambda|$.\n3.  **Inégalité triangulaire** : Soit $x, y \\in \\mathbb{R}^n$. Pour n'importe quelle composante $j$, on a $|x_j + y_j| \\le |x_j| + |y_j|$. Comment pouvez-vous majorer $|x_j|$ et $|y_j|$ en utilisant $\\|x\\|_\\infty$ et $\\|y\\|_\\infty$ ? Appliquez ensuite le maximum sur $j$.\n\n</details>",
      "solution": "\n\nSoient $x, y \\in \\mathbb{R}^n$ et $\\lambda \\in \\mathbb{R}$.\n\n**1. Axiome de séparation : $\\|x\\|_\\infty = 0 \\iff x = 0_{\\mathbb{R}^n}$**\n\n($\\implies$) Supposons $\\|x\\|_\\infty = 0$. Cela signifie $\\max_{1 \\le j \\le n} |x_j| = 0$.\n\nComme $|x_j| \\ge 0$ pour tout $j$, le maximum ne peut être 0 que si toutes les composantes sont nulles. Donc, $|x_j| = 0$ pour tout $j \\in \\{1, \\dots, n\\}$.\n\nCela implique $x_j=0$ pour tout $j$, et donc $x = (0, \\dots, 0) = 0_{\\mathbb{R}^n}$.\n\n($\\impliedby$) Si $x = 0_{\\mathbb{R}^n}$, alors $x_j=0$ pour tout $j$. Ainsi $\\|x\\|_\\infty = \\max(|0|, \\dots, |0|) = 0$.\n\nL'axiome est vérifié.\n\n**2. Axiome d'homogénéité : $\\|\\lambda x\\|_\\infty = |\\lambda| \\|x\\|_\\infty$**\n\nLe vecteur $\\lambda x$ a pour composantes $(\\lambda x_1, \\dots, \\lambda x_n)$.\n\n$$ \\begin{align*} \\|\\lambda x\\|_\\infty &= \\max_{1 \\le j \\le n} |\\lambda x_j| \\\\ &= \\max_{1 \\le j \\le n} (|\\lambda| |x_j|) \\end{align*} $$\n\nComme $|\\lambda|$ est une constante non-négative, on peut la sortir du maximum :\n\n$$ \\|\\lambda x\\|_\\infty = |\\lambda| \\left(\\max_{1 \\le j \\le n} |x_j|\\right) = |\\lambda| \\|x\\|_\\infty. $$\n\nL'axiome est vérifié.\n\n**3. Inégalité triangulaire : $\\|x+y\\|_\\infty \\le \\|x\\|_\\infty + \\|y\\|_\\infty$**\n\nLe vecteur $x+y$ a pour composantes $(x_1+y_1, \\dots, x_n+y_n)$.\n\n$$ \\|x+y\\|_\\infty = \\max_{1 \\le j \\le n} |x_j + y_j| $$\n\nSoit $j_0$ l'indice où ce maximum est atteint : $\\|x+y\\|_\\infty = |x_{j_0} + y_{j_0}|$.\n\nEn utilisant l'inégalité triangulaire pour la valeur absolue sur $\\mathbb{R}$, on a :\n\n$$ |x_{j_0} + y_{j_0}| \\le |x_{j_0}| + |y_{j_0}| $$\n\nPar définition de la norme infinie, pour tout indice $j$, on a $|x_j| \\le \\max_{k} |x_k| = \\|x\\|_\\infty$ et $|y_j| \\le \\max_{k} |y_k| = \\|y\\|_\\infty$.\n\nCeci est vrai en particulier pour $j_0$ :\n\n$$ |x_{j_0}| \\le \\|x\\|_\\infty \\quad \\text{et} \\quad |y_{j_0}| \\le \\|y\\|_\\infty $$\n\nEn combinant ces inégalités, on obtient :\n\n$$ \\|x+y\\|_\\infty = |x_{j_0} + y_{j_0}| \\le |x_{j_0}| + |y_{j_0}| \\le \\|x\\|_\\infty + \\|y\\|_\\infty. $$\n\nL'axiome est vérifié.\n\n**Conclusion**\n\nLes trois axiomes étant satisfaits, $\\| \\cdot \\|_\\infty$ est une norme sur $\\mathbb{R}^n$.\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "733f7947",
      "content": "#### Preuve que la norme 1 est une norme\n\nDémontrer que l'application $\\| \\cdot \\|_1$ définie pour tout $x = (x_1, \\dots, x_n) \\in \\mathbb{R}^n$ par $\\|x\\|_1 = \\sum_{j=1}^n |x_j|$ est une norme sur $\\mathbb{R}^n$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nVérifiez les trois axiomes.\n\n1.  **Séparation** : $\\|x\\|_1$ est une somme de termes positifs ou nuls. Quand une telle somme est-elle nulle ?\n2.  **Homogénéité** : Utilisez les propriétés de la somme ($\\sum c \\cdot a_j = c \\sum a_j$) et de la valeur absolue.\n3.  **Inégalité triangulaire** : Appliquez l'inégalité triangulaire pour les nombres réels ($|a+b| \\le |a|+|b|$) à chaque composante $|x_j + y_j|$ avant de sommer.\n\n</details>",
      "solution": "\n\nSoient $x, y \\in \\mathbb{R}^n$ et $\\lambda \\in \\mathbb{R}$.\n\n**1. Axiome de séparation : $\\|x\\|_1 = 0 \\iff x = 0_{\\mathbb{R}^n}$**\n\n($\\implies$) Supposons $\\|x\\|_1 = 0$. Cela signifie $\\sum_{j=1}^n |x_j| = 0$.\n\nChaque terme $|x_j|$ est positif ou nul. Une somme de termes positifs ou nuls est égale à zéro si et seulement si chaque terme de la somme est nul.\n\nDonc, $|x_j| = 0$ pour tout $j \\in \\{1, \\dots, n\\}$, ce qui implique $x_j = 0$ pour tout $j$.\n\nAinsi, $x = (0, \\dots, 0) = 0_{\\mathbb{R}^n}$.\n\n($\\impliedby$) Si $x = 0_{\\mathbb{R}^n}$, alors $x_j=0$ pour tout $j$. Ainsi $\\|x\\|_1 = \\sum_{j=1}^n |0| = 0$.\n\nL'axiome est vérifié.\n\n**2. Axiome d'homogénéité : $\\|\\lambda x\\|_1 = |\\lambda| \\|x\\|_1$**\n\nLe vecteur $\\lambda x$ a pour composantes $(\\lambda x_1, \\dots, \\lambda x_n)$.\n\n$$ \\begin{align*} \\|\\lambda x\\|_1 &= \\sum_{j=1}^n |\\lambda x_j| \\\\ &= \\sum_{j=1}^n |\\lambda| |x_j| \\\\ &= |\\lambda| \\sum_{j=1}^n |x_j| \\quad \\text{(mise en facteur de la constante } |\\lambda| \\text{)} \\\\ &= |\\lambda| \\|x\\|_1. \\end{align*} $$\n\nL'axiome est vérifié.\n\n**3. Inégalité triangulaire : $\\|x+y\\|_1 \\le \\|x\\|_1 + \\|y\\|_1$**\n\nLe vecteur $x+y$ a pour composantes $(x_1+y_1, \\dots, x_n+y_n)$.\n\n$$ \\|x+y\\|_1 = \\sum_{j=1}^n |x_j + y_j| $$\n\nEn utilisant l'inégalité triangulaire pour la valeur absolue sur $\\mathbb{R}$ pour chaque composante $j$, on a $|x_j + y_j| \\le |x_j| + |y_j|$.\n\nEn sommant sur toutes les composantes :\n\n$$ \\sum_{j=1}^n |x_j + y_j| \\le \\sum_{j=1}^n (|x_j| + |y_j|) $$\n\nPar les propriétés de la somme, on peut la séparer en deux :\n\n$$ \\sum_{j=1}^n (|x_j| + |y_j|) = \\sum_{j=1}^n |x_j| + \\sum_{j=1}^n |y_j| = \\|x\\|_1 + \\|y\\|_1. $$\n\nOn a donc bien $\\|x+y\\|_1 \\le \\|x\\|_1 + \\|y\\|_1$. L'axiome est vérifié.\n\n**Conclusion**\n\nLes trois axiomes étant satisfaits, $\\| \\cdot \\|_1$ est une norme sur $\\mathbb{R}^n$.\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "733f7947",
      "content": "#### Inégalité de Minkowski (Inégalité triangulaire pour la norme 2)\n\nDémontrer l'inégalité triangulaire pour la norme euclidienne $\\| \\cdot \\|_2$ sur $\\mathbb{R}^n$. Autrement dit, prouver que pour tous $x, y \\in \\mathbb{R}^n$, on a $\\|x+y\\|_2 \\le \\|x\\|_2 + \\|y\\|_2$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nIl est plus facile de travailler avec les carrés des normes pour éviter les racines carrées. Commencez par développer l'expression $\\|x+y\\|_2^2 = \\sum_{j=1}^n (x_j+y_j)^2$.\n\nSéparez la somme en trois parties : $\\sum x_j^2$, $\\sum y_j^2$, et le terme croisé $2\\sum x_j y_j$.\n\nReconnaissez que le terme croisé est $2 \\langle x, y \\rangle$, le produit scalaire de $x$ et $y$. Utilisez l'**inégalité de Cauchy-Schwarz**, $|\\langle x, y \\rangle| \\le \\|x\\|_2 \\|y\\|_2$, pour majorer ce terme.\n\nEnfin, vous devriez arriver à une expression de la forme $(\\|x\\|_2 + \\|y\\|_2)^2$.\n\n</details>",
      "solution": "\n\nSoient $x, y \\in \\mathbb{R}^n$. L'inégalité que nous voulons prouver est $\\sqrt{\\sum (x_j+y_j)^2} \\le \\sqrt{\\sum x_j^2} + \\sqrt{\\sum y_j^2}$.\n\nComme les deux membres sont positifs, cette inégalité est équivalente à l'inégalité entre leurs carrés :\n\n$\\|x+y\\|_2^2 \\le (\\|x\\|_2 + \\|y\\|_2)^2$.\n\n**Étape 1 : Développer $\\|x+y\\|_2^2$**\n\nPar définition de la norme euclidienne :\n\n$$ \\begin{align*} \\|x+y\\|_2^2 &= \\sum_{j=1}^n (x_j+y_j)^2 \\\\ &= \\sum_{j=1}^n (x_j^2 + 2x_j y_j + y_j^2) \\\\ &= \\sum_{j=1}^n x_j^2 + 2 \\sum_{j=1}^n x_j y_j + \\sum_{j=1}^n y_j^2 \\end{align*} $$\n\n**Étape 2 : Reconnaître les termes**\n\nOn reconnaît les carrés des normes de $x$ et $y$, ainsi que le produit scalaire usuel $\\langle x, y \\rangle = \\sum_{j=1}^n x_j y_j$.\n\nL'expression devient :\n\n$\\|x+y\\|_2^2 = \\|x\\|_2^2 + 2 \\langle x, y \\rangle + \\|y\\|_2^2$.\n\n**Étape 3 : Appliquer l'inégalité de Cauchy-Schwarz**\n\nL'inégalité de Cauchy-Schwarz nous dit que $\\langle x, y \\rangle \\le |\\langle x, y \\rangle| \\le \\|x\\|_2 \\|y\\|_2$.\n\nEn utilisant cette majoration dans notre expression :\n\n$$ \\|x+y\\|_2^2 \\le \\|x\\|_2^2 + 2 \\|x\\|_2 \\|y\\|_2 + \\|y\\|_2^2 $$\n\n**Étape 4 : Conclure**\n\nLe membre de droite est une identité remarquable :\n\n$\\|x\\|_2^2 + 2 \\|x\\|_2 \\|y\\|_2 + \\|y\\|_2^2 = (\\|x\\|_2 + \\|y\\|_2)^2$.\n\nNous avons donc montré que $\\|x+y\\|_2^2 \\le (\\|x\\|_2 + \\|y\\|_2)^2$.\n\nPuisque la fonction racine carrée est croissante sur $\\mathbb{R}_+$, on peut prendre la racine carrée des deux côtés tout en préservant l'inégalité :\n\n$\\|x+y\\|_2 \\le \\|x\\|_2 + \\|y\\|_2$.\n\nCeci prouve l'inégalité triangulaire pour la norme euclidienne.\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "733f7947",
      "content": "#### Preuve de l'équivalence des normes 1 et infinie\n\nDémontrer que les normes $\\| \\cdot \\|_1$ et $\\| \\cdot \\|_\\infty$ sont équivalentes sur $\\mathbb{R}^n$. C'est-à-dire, trouver deux constantes réelles $\\alpha > 0$ et $\\beta > 0$ telles que pour tout $x \\in \\mathbb{R}^n$ :\n\n$\\alpha \\|x\\|_\\infty \\le \\|x\\|_1 \\le \\beta \\|x\\|_\\infty$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nLa preuve se fait en deux parties, en montrant les deux inégalités séparément.\n\n1.  **Pour l'inégalité $\\|x\\|_1 \\le \\beta \\|x\\|_\\infty$** :\n\n    Écrivez la définition de $\\|x\\|_1 = \\sum_{j=1}^n |x_j|$. Pour chaque $j$, comment pouvez-vous majorer $|x_j|$ en utilisant $\\|x\\|_\\infty = \\max_k |x_k|$ ? Appliquez cette majoration à chaque terme de la somme.\n\n2.  **Pour l'inégalité $\\alpha \\|x\\|_\\infty \\le \\|x\\|_1$** :\n\n    Soit $j_0$ l'indice pour lequel la valeur absolue de la composante est maximale, c'est-à-dire $|x_{j_0}| = \\|x\\|_\\infty$. Comparez ce terme seul à la somme de tous les termes (qui est $\\|x\\|_1$).\n\n</details>",
      "solution": "\n\nSoit $x = (x_1, \\dots, x_n) \\in \\mathbb{R}^n$. Nous devons trouver des constantes $\\alpha, \\beta > 0$ indépendantes de $x$.\n\n**Partie 1 : Majoration de $\\|x\\|_1$ par $\\|x\\|_\\infty$**\n\nPar définition, $\\|x\\|_1 = \\sum_{j=1}^n |x_j| = |x_1| + |x_2| + \\dots + |x_n|$.\n\nLa norme infinie est définie comme $\\|x\\|_\\infty = \\max_{1 \\le k \\le n} |x_k|$.\n\nPar définition du maximum, pour n'importe quel indice $j \\in \\{1, \\dots, n\\}$, on a :\n\n$|x_j| \\le \\|x\\|_\\infty$.\n\nEn appliquant cette majoration à chaque terme de la somme définissant $\\|x\\|_1$ :\n\n$$ \\|x\\|_1 = \\sum_{j=1}^n |x_j| \\le \\sum_{j=1}^n \\|x\\|_\\infty $$\n\nLa somme $\\sum_{j=1}^n \\|x\\|_\\infty$ est une somme de $n$ termes identiques, donc elle est égale à $n \\|x\\|_\\infty$.\n\nNous avons donc montré que :\n\n$\\|x\\|_1 \\le n \\|x\\|_\\infty$.\n\nOn peut choisir $\\beta = n$. Cette constante est bien strictement positive.\n\n**Partie 2 : Minoration de $\\|x\\|_1$ par $\\|x\\|_\\infty$**\n\nSoit $j_0 \\in \\{1, \\dots, n\\}$ un indice tel que le maximum de la norme infinie est atteint, c'est-à-dire :\n\n$|x_{j_0}| = \\max_{1 \\le k \\le n} |x_k| = \\|x\\|_\\infty$.\n\nConsidérons la somme définissant la norme 1 :\n\n$\\|x\\|_1 = |x_1| + |x_2| + \\dots + |x_{j_0}| + \\dots + |x_n|$.\n\nPuisque tous les termes $|x_j|$ sont non-négatifs, la somme est supérieure ou égale à n'importe lequel de ses termes. En particulier, elle est supérieure ou égale à $|x_{j_0}|$.\n\n$\\|x\\|_1 \\ge |x_{j_0}|$.\n\nEn utilisant le fait que $|x_{j_0}| = \\|x\\|_\\infty$, on obtient :\n\n$\\|x\\|_1 \\ge \\|x\\|_\\infty$.\n\nOn peut aussi l'écrire $1 \\cdot \\|x\\|_\\infty \\le \\|x\\|_1$.\n\nOn peut donc choisir $\\alpha = 1$. Cette constante est bien strictement positive.\n\n**Conclusion**\n\nNous avons trouvé deux constantes $\\alpha=1 > 0$ et $\\beta=n > 0$ telles que pour tout $x \\in \\mathbb{R}^n$ :\n\n$1 \\cdot \\|x\\|_\\infty \\le \\|x\\|_1 \\le n \\cdot \\|x\\|_\\infty$.\n\nLes normes $\\| \\cdot \\|_1$ et $\\| \\cdot \\|_\\infty$ sont donc équivalentes sur $\\mathbb{R}^n$.\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "733f7947",
      "content": "#### Unicité de la limite d'une suite\n\nDémontrer que si une suite $(x^k)_{k \\in \\mathbb{N}}$ de vecteurs de $\\mathbb{R}^n$ converge, alors sa limite est unique.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nRaisonnez par l'absurde. Supposez que la suite $(x^k)$ admette deux limites distinctes, $a$ et $b$, avec $a \\neq b$.\n\n1.  La distance $\\|a-b\\|$ est donc strictement positive.\n2.  Utilisez l'inégalité triangulaire en écrivant $\\|a-b\\| = \\|(a-x^k) + (x^k-b)\\|$.\n3.  Par définition de la convergence vers $a$ et vers $b$, les termes $\\|a-x^k\\|$ et $\\|x^k-b\\|$ peuvent être rendus aussi petits que l'on veut pour $k$ assez grand.\n4.  Montrez que cela conduit à une contradiction avec le fait que $\\|a-b\\| > 0$.\n\n</details>",
      "solution": "\n\nSoit $(x^k)_{k \\in \\mathbb{N}}$ une suite de $\\mathbb{R}^n$ munie d'une norme $\\| \\cdot \\|$.\n\nSupposons par l'absurde que la suite admette deux limites distinctes, $a \\in \\mathbb{R}^n$ et $b \\in \\mathbb{R}^n$, avec $a \\neq b$.\n\nPuisque $a \\neq b$, le vecteur $a-b$ n'est pas le vecteur nul. Par l'axiome de séparation de la norme, on a $\\|a-b\\| > 0$.\n\nPosons $\\varepsilon = \\frac{\\|a-b\\|}{2}$. Par construction, $\\varepsilon > 0$.\n\nPar définition de la convergence :\n\n1.  Puisque $x^k \\to a$, il existe un rang $k_1 \\in \\mathbb{N}$ tel que pour tout $k \\ge k_1$, on a $\\|x^k - a\\| < \\varepsilon$.\n2.  Puisque $x^k \\to b$, il existe un rang $k_2 \\in \\mathbb{N}$ tel que pour tout $k \\ge k_2$, on a $\\|x^k - b\\| < \\varepsilon$.\n\nSoit $k_0 = \\max(k_1, k_2)$. Pour tout entier $k \\ge k_0$, les deux conditions sont satisfaites simultanément.\n\nConsidérons la distance $\\|a-b\\|$. En utilisant l'astuce d'ajouter et de soustraire $x^k$ et en appliquant l'inégalité triangulaire, nous avons pour $k \\ge k_0$ :\n\n$$ \\|a-b\\| = \\|(a-x^k) + (x^k-b)\\| \\le \\|a-x^k\\| + \\|x^k-b\\| $$\n\nOn sait que $\\|a-x^k\\| = \\|x^k-a\\|$. Donc pour $k \\ge k_0$ :\n\n$$ \\|a-b\\| \\le \\|x^k-a\\| + \\|x^k-b\\| < \\varepsilon + \\varepsilon = 2\\varepsilon $$\n\nEn remplaçant $\\varepsilon$ par sa valeur $\\frac{\\|a-b\\|}{2}$, on obtient :\n\n$\\|a-b\\| < 2 \\left( \\frac{\\|a-b\\|}{2} \\right)$, ce qui simplifie en $\\|a-b\\| < \\|a-b\\|$.\n\nCeci est une contradiction stricte. L'hypothèse de départ (l'existence de deux limites distinctes) est donc fausse.\n\n**Conclusion**\n\nLa limite d'une suite convergente dans $\\mathbb{R}^n$ est unique.\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "733f7947",
      "content": "#### Convergence vectorielle et convergence des composantes (Partie 1)\n\nSoit $(x^k)_{k \\in \\mathbb{N}}$ une suite de vecteurs de $\\mathbb{R}^n$.\n\nDémontrer que si la suite $(x^k)$ converge vers un vecteur $a \\in \\mathbb{R}^n$, alors pour chaque composante $j \\in \\{1, \\dots, n\\}$, la suite réelle $(x_j^k)_{k \\in \\mathbb{N}}$ converge vers la composante correspondante $a_j$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nL'objectif est de montrer que $|x_j^k - a_j| \\to 0$ pour chaque $j$.\n\nPour ce faire, il faut relier la quantité $|x_j^k - a_j|$ à $\\|x^k - a\\|$, qui tend vers 0 par hypothèse.\n\nRappelez-vous la relation entre la valeur absolue d'une composante et les normes usuelles. Par exemple, pour tout vecteur $v = (v_1, ..., v_n)$, on a $|v_j| \\le \\|v\\|_\\infty$ et $|v_j| \\le \\|v\\|_2$. Utilisez l'une de ces inégalités avec le vecteur $v = x^k - a$.\n\n</details>",
      "solution": "\n\nSoit $(x^k)$ une suite de vecteurs dans $\\mathbb{R}^n$ qui converge vers $a \\in \\mathbb{R}^n$. Cela signifie que $\\lim_{k \\to \\infty} \\|x^k - a\\| = 0$ pour n'importe quelle norme sur $\\mathbb{R}^n$ (puisqu'elles sont toutes équivalentes).\n\nNous voulons montrer que pour tout $j \\in \\{1, \\dots, n\\}$, la suite réelle $(x_j^k)$ converge vers $a_j$, c'est-à-dire $\\lim_{k \\to \\infty} |x_j^k - a_j| = 0$.\n\nSoit $j$ un indice de composante fixé. Considérons le vecteur $v^k = x^k - a$. Ses composantes sont $v_j^k = x_j^k - a_j$.\n\nNous savons que pour tout vecteur $v \\in \\mathbb{R}^n$, et pour toute composante $j$, on a l'inégalité suivante :\n\n$|v_j| \\le \\max_{1 \\le i \\le n} |v_i| = \\|v\\|_\\infty$.\n\nAppliquons cette inégalité au vecteur $v^k = x^k - a$ :\n\n$|x_j^k - a_j| \\le \\|x^k - a\\|_\\infty$.\n\nNous avons donc l'encadrement suivant pour chaque $j$ :\n\n$0 \\le |x_j^k - a_j| \\le \\|x^k - a\\|_\\infty$.\n\nPar hypothèse, la suite de vecteurs $(x^k)$ converge vers $a$. Puisque toutes les normes sont équivalentes, la convergence est vraie pour la norme infinie. Donc :\n\n$\\lim_{k \\to \\infty} \\|x^k - a\\|_\\infty = 0$.\n\nPar le théorème des gendarmes (ou théorème d'encadrement) appliqué aux suites réelles, comme $|x_j^k - a_j|$ est encadré par $0$ et une suite qui tend vers $0$, on peut conclure que :\n\n$\\lim_{k \\to \\infty} |x_j^k - a_j| = 0$.\n\nCeci est vrai pour chaque composante $j \\in \\{1, \\dots, n\\}$. La convergence de la suite de vecteurs implique donc la convergence de chacune de ses suites de composantes.\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "733f7947",
      "content": "#### Convergence vectorielle et convergence des composantes (Partie 2)\n\nSoit $(x^k)_{k \\in \\mathbb{N}}$ une suite de vecteurs de $\\mathbb{R}^n$.\n\nDémontrer que si, pour chaque composante $j \\in \\{1, \\dots, n\\}$, la suite réelle $(x_j^k)_{k \\in \\mathbb{N}}$ converge vers $a_j$, alors la suite de vecteurs $(x^k)$ converge vers le vecteur $a = (a_1, \\dots, a_n)$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nL'objectif est de montrer que $\\|x^k - a\\| \\to 0$. Pour cela, il faut majorer $\\|x^k - a\\|$ par une expression qui dépend des quantités $|x_j^k - a_j|$, qui tendent vers 0 par hypothèse.\n\nChoisissez une norme pratique qui s'exprime facilement en fonction des composantes, comme la norme 1 ou la norme infinie.\n\nPar exemple, avec la norme 1 : $\\|x^k-a\\|_1 = \\sum_{j=1}^n |x_j^k - a_j|$. Vous avez une somme *finie* de termes qui tendent tous vers 0. Quelle est la limite d'une somme finie de suites convergentes ?\n\n</details>",
      "solution": "\n\nSupposons que pour chaque $j \\in \\{1, \\dots, n\\}$, la suite réelle $(x_j^k)$ converge vers $a_j$. Cela signifie que :\n\n$\\forall j \\in \\{1, \\dots, n\\}, \\quad \\lim_{k \\to \\infty} |x_j^k - a_j| = 0$.\n\nNous voulons montrer que la suite de vecteurs $(x^k)$ converge vers $a$, c'est-à-dire $\\lim_{k \\to \\infty} \\|x^k - a\\| = 0$. Grâce à l'équivalence de toutes les normes sur $\\mathbb{R}^n$, il suffit de le démontrer pour une seule norme de notre choix. Choisissons la norme 1, $\\| \\cdot \\|_1$, car son expression est pratique.\n\nLa norme 1 du vecteur $x^k - a$ est donnée par :\n\n$\\|x^k - a\\|_1 = \\sum_{j=1}^n |x_j^k - a_j|$.\n\nNous avons une somme de $n$ termes. Par hypothèse, pour chaque $j$, le terme $|x_j^k - a_j|$ tend vers 0 lorsque $k \\to \\infty$.\n\nLa limite d'une somme finie de suites convergentes est la somme de leurs limites.\n\nDonc :\n\n$$ \\lim_{k \\to \\infty} \\|x^k - a\\|_1 = \\lim_{k \\to \\infty} \\sum_{j=1}^n |x_j^k - a_j| $$\n\n$$ = \\sum_{j=1}^n \\left( \\lim_{k \\to \\infty} |x_j^k - a_j| \\right) $$\n\n$$ = \\sum_{j=1}^n 0 = 0 $$\n\nPuisque $\\lim_{k \\to \\infty} \\|x^k - a\\|_1 = 0$, la suite de vecteurs $(x^k)$ converge vers $a$ pour la norme 1.\n\nComme toutes les normes sur $\\mathbb{R}^n$ sont équivalentes, la convergence pour la norme 1 implique la convergence pour n'importe quelle autre norme.\n\n**Conclusion**\n\nLa convergence de toutes les suites de composantes implique la convergence de la suite de vecteurs.\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "733f7947",
      "content": "#### Une suite convergente est une suite de Cauchy\n\nDémontrer que toute suite convergente $(x^k)_{k \\in \\mathbb{N}}$ dans un espace vectoriel normé $(\\mathbb{R}^n, \\| \\cdot \\|)$ est une suite de Cauchy.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoit $a$ la limite de la suite $(x^k)$.\n\nLa définition de la convergence vous dit que pour $k$ grand, $x^k$ est proche de $a$.\n\nVous voulez montrer que pour $p, q$ grands, $x^p$ est proche de $x^q$.\n\nUtilisez l'inégalité triangulaire pour relier la distance $\\|x^p - x^q\\|$ aux distances $\\|x^p - a\\|$ et $\\|x^q - a\\|$. L'astuce consiste à écrire $x^p - x^q = (x^p - a) + (a - x^q)$.\n\n</details>",
      "solution": "\n\nSoit $(x^k)_{k \\in \\mathbb{N}}$ une suite de vecteurs dans $\\mathbb{R}^n$ qui converge vers une limite $a \\in \\mathbb{R}^n$.\n\nPar définition de la convergence, pour tout $\\delta > 0$, il existe un rang $k_\\delta \\in \\mathbb{N}$ tel que pour tout $k \\ge k_\\delta$, on a $\\|x^k - a\\| < \\delta$.\n\nNous voulons démontrer que $(x^k)$ est une suite de Cauchy. C'est-à-dire, nous devons montrer que :\n\n$\\forall \\varepsilon > 0, \\quad \\exists k_\\varepsilon \\in \\mathbb{N} \\text{ tel que } \\forall p, q \\ge k_\\varepsilon, \\quad \\|x^p - x^q\\| < \\varepsilon$.\n\nSoit $\\varepsilon > 0$ un réel quelconque.\n\nPosons $\\delta = \\frac{\\varepsilon}{2}$. Puisque $\\delta > 0$, la définition de la convergence nous assure qu'il existe un rang, que nous nommerons $k_\\varepsilon$, tel que pour tout indice $k \\ge k_\\varepsilon$ :\n\n$\\|x^k - a\\| < \\frac{\\varepsilon}{2}$.\n\nMaintenant, considérons deux indices quelconques $p$ et $q$ tels que $p \\ge k_\\varepsilon$ et $q \\ge k_\\varepsilon$.\n\nNous voulons majorer la distance $\\|x^p - x^q\\|$. En utilisant l'inégalité triangulaire, on a :\n\n$$ \\|x^p - x^q\\| = \\|(x^p - a) + (a - x^q)\\| \\le \\|x^p - a\\| + \\|a - x^q\\| $$\n\nOn sait que $\\|a - x^q\\| = \\|(-1)(x^q - a)\\| = |-1|\\|x^q - a\\| = \\|x^q - a\\|$.\n\nL'inégalité devient donc :\n\n$$ \\|x^p - x^q\\| \\le \\|x^p - a\\| + \\|x^q - a\\| $$\n\nPuisque $p \\ge k_\\varepsilon$ et $q \\ge k_\\varepsilon$, nous pouvons utiliser la propriété de convergence :\n\n$\\|x^p - a\\| < \\frac{\\varepsilon}{2}$\n\n$\\|x^q - a\\| < \\frac{\\varepsilon}{2}$\n\nEn substituant ces majorations dans notre inégalité, on obtient :\n\n$$ \\|x^p - x^q\\| < \\frac{\\varepsilon}{2} + \\frac{\\varepsilon}{2} = \\varepsilon $$\n\nNous avons donc montré que pour tout $\\varepsilon > 0$, il existe un rang $k_\\varepsilon$ tel que pour tous $p, q \\ge k_\\varepsilon$, on a $\\|x^p - x^q\\| < \\varepsilon$.\n\nCeci est précisément la définition d'une suite de Cauchy.\n\n**Conclusion**\n\nToute suite convergente dans $\\mathbb{R}^n$ est une suite de Cauchy.\n\n",
      "options": []
    },
    {
      "id": "11",
      "stackId": "733f7947",
      "content": "#### Transitivité de l'équivalence des normes\n\nDémontrer que la relation \"être équivalente à\" est transitive pour les normes sur $\\mathbb{R}^n$. Autrement dit, si une norme $N_1$ est équivalente à une norme $N_2$, et $N_2$ est équivalente à une norme $N_3$, alors $N_1$ est équivalente à $N_3$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nCommencez par écrire les définitions formelles des deux hypothèses.\n\n1.  $N_1$ est équivalente à $N_2$ : il existe $\\alpha_1, \\beta_1 > 0$ tels que $\\alpha_1 N_2(x) \\le N_1(x) \\le \\beta_1 N_2(x)$.\n2.  $N_2$ est équivalente à $N_3$ : il existe $\\alpha_2, \\beta_2 > 0$ tels que $\\alpha_2 N_3(x) \\le N_2(x) \\le \\beta_2 N_3(x)$.\n\nVotre objectif est de trouver des constantes $\\alpha_3, \\beta_3 > 0$ telles que $\\alpha_3 N_3(x) \\le N_1(x) \\le \\beta_3 N_3(x)$.\n\nPour cela, combinez les inégalités que vous avez écrites pour éliminer $N_2$. Par exemple, pour majorer $N_1$ en fonction de $N_3$, partez de $N_1(x) \\le \\beta_1 N_2(x)$ et majorez ensuite $N_2(x)$.\n\n</details>",
      "solution": "\n\nSoient $N_1, N_2, N_3$ trois normes sur $\\mathbb{R}^n$.\n\n**Hypothèses :**\n\n1.  $N_1$ est équivalente à $N_2$. Cela signifie qu'il existe deux constantes réelles $\\alpha_1 > 0$ et $\\beta_1 > 0$ telles que pour tout $x \\in \\mathbb{R}^n$ :\n\n    $\\alpha_1 N_2(x) \\le N_1(x) \\le \\beta_1 N_2(x)$. (I)\n\n2.  $N_2$ est équivalente à $N_3$. Cela signifie qu'il existe deux constantes réelles $\\alpha_2 > 0$ et $\\beta_2 > 0$ telles que pour tout $x \\in \\mathbb{R}^n$ :\n\n    $\\alpha_2 N_3(x) \\le N_2(x) \\le \\beta_2 N_3(x)$. (II)\n\n**Objectif :**\n\nNous voulons prouver que $N_1$ est équivalente à $N_3$. Nous devons donc trouver des constantes $\\alpha_3 > 0$ et $\\beta_3 > 0$ telles que pour tout $x \\in \\mathbb{R}^n$ :\n\n$\\alpha_3 N_3(x) \\le N_1(x) \\le \\beta_3 N_3(x)$.\n\n**Étape 1 : Trouver la majoration (la constante $\\beta_3$)**\n\nNous partons de l'inégalité de droite dans (I) :\n\n$N_1(x) \\le \\beta_1 N_2(x)$.\n\nMaintenant, nous utilisons l'inégalité de droite dans (II) pour majorer $N_2(x)$ :\n\n$N_2(x) \\le \\beta_2 N_3(x)$.\n\nEn combinant les deux, on obtient :\n\n$N_1(x) \\le \\beta_1 \\left( \\beta_2 N_3(x) \\right) = (\\beta_1 \\beta_2) N_3(x)$.\n\nPosons $\\beta_3 = \\beta_1 \\beta_2$. Puisque $\\beta_1 > 0$ et $\\beta_2 > 0$, leur produit $\\beta_3$ est aussi strictement positif. Nous avons donc la moitié de la preuve : $N_1(x) \\le \\beta_3 N_3(x)$.\n\n**Étape 2 : Trouver la minoration (la constante $\\alpha_3$)**\n\nNous partons de l'inégalité de gauche dans (I) :\n\n$\\alpha_1 N_2(x) \\le N_1(x)$.\n\nMaintenant, nous utilisons l'inégalité de gauche dans (II) pour minorer $N_2(x)$ :\n\n$\\alpha_2 N_3(x) \\le N_2(x)$.\n\nEn combinant les deux, on obtient :\n\n$N_1(x) \\ge \\alpha_1 N_2(x) \\ge \\alpha_1 \\left( \\alpha_2 N_3(x) \\right) = (\\alpha_1 \\alpha_2) N_3(x)$.\n\nPosons $\\alpha_3 = \\alpha_1 \\alpha_2$. Puisque $\\alpha_1 > 0$ et $\\alpha_2 > 0$, leur produit $\\alpha_3$ est aussi strictement positif. Nous avons donc l'autre moitié de la preuve : $\\alpha_3 N_3(x) \\le N_1(x)$.\n\n**Conclusion**\n\nNous avons trouvé deux constantes $\\alpha_3 = \\alpha_1 \\alpha_2 > 0$ et $\\beta_3 = \\beta_1 \\beta_2 > 0$ telles que pour tout $x \\in \\mathbb{R}^n$ :\n\n$\\alpha_3 N_3(x) \\le N_1(x) \\le \\beta_3 N_3(x)$.\n\nCeci prouve que $N_1$ est équivalente à $N_3$. La relation d'équivalence des normes est donc transitive.\n\n",
      "options": []
    }
  ]
}