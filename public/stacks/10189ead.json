{
  "info": {
    "id": "10189ead",
    "title": "Chapitre 5 Recherche d'extremum - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Chapitre 5 Recherche d'extremum",
    "course": "Topologie et Calcul Différentiel I",
    "tags": [
      "Mathématiques",
      "Analyse",
      "Optimisation",
      "Extremum",
      "Hessienne",
      "Points critiques"
    ],
    "count": 10
  },
  "cards": [
    {
      "id": "1",
      "stackId": "10189ead",
      "content": "#### Preuve : Extremum global implique extremum local\n\nProuver que si une fonction $f: D \\to \\mathbb{R}$ admet un maximum global en un point $a \\in D$, alors elle admet un maximum local en ce point.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nRevenez strictement aux définitions. La définition d'un maximum global impose une inégalité sur tout l'ensemble $D$. La définition locale impose la même inégalité sur une intersection de $D$ avec une boule.\n\nEst-ce que l'inégalité \"pour tout $y$\" implique l'inégalité \"pour certains $y$\" ?\n\n</details>",
      "solution": "\n\nNous devons montrer que la définition du maximum global implique celle du maximum local.\n\n**Étape 1 : Écrire l'hypothèse (Global)**\n\nSupposons que $f$ admet un maximum global en $a$. Par définition :\n\n$$ \\forall y \\in D, \\quad f(a) \\geq f(y) $$\n\n**Étape 2 : Écrire la conclusion souhaitée (Local)**\n\nNous cherchons à montrer qu'il existe un $\\varepsilon > 0$ tel que :\n\n$$ \\forall y \\in D \\cap B(a, \\varepsilon), \\quad f(a) \\geq f(y) $$\n\n**Étape 3 : Démonstration**\n\nPrenons n'importe quel $\\varepsilon > 0$ (par exemple $\\varepsilon = 1$).\n\nConsidérons un point $y$ quelconque appartenant au voisinage $D \\cap B(a, \\varepsilon)$.\n\nPuisque $y \\in D \\cap B(a, \\varepsilon)$, on a en particulier que $y \\in D$.\n\nOr, l'hypothèse de l'étape 1 nous dit que l'inégalité $f(a) \\geq f(y)$ est vraie pour **tous** les points de $D$. Elle est donc vraie *a fortiori* pour les points du sous-ensemble $D \\cap B(a, \\varepsilon)$.\n\n**Conclusion :**\n\nL'existence d'un maximum global implique directement l'existence d'un maximum local (pour n'importe quel rayon $\\varepsilon$).\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "10189ead",
      "content": "#### Condition nécessaire du premier ordre (Points critiques)\n\nSoit $U$ un ouvert de $\\mathbb{R}^n$ et $f: U \\to \\mathbb{R}$ une fonction différentiable en $a \\in U$.\n\nProuver que si $f$ admet un extremum local en $a$, alors $\\nabla f(a) = 0$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nL'idée est de se ramener au cas d'une seule variable (calcul différentiel classique).\n\nConsidérez les fonctions partielles $g_i(t) = f(a + t e_i)$ où $e_i$ est un vecteur de la base canonique.\n\nSi $f$ a un maximum en $a$, quel comportement a $g_i$ en $t=0$ ?\n\nUtilisez le théorème de Fermat pour les fonctions d'une variable réelle ($g'(0)=0$).\n\n</details>",
      "solution": "\n\nSupposons sans perte de généralité que $f$ admet un maximum local en $a$.\n\n**Étape 1 : Restriction à une variable**\n\nSoit $e_i = (0, \\dots, 1, \\dots, 0)$ le $i$-ème vecteur de la base canonique.\n\nComme $U$ est ouvert, il existe $\\varepsilon > 0$ tel que pour tout $t \\in ]-\\varepsilon, \\varepsilon[$, le point $a + t e_i$ appartient à $U$.\n\nDéfinissons la fonction d'une variable réelle :\n\n$$ g_i(t) = f(a + t e_i) $$\n\n**Étape 2 : Application du théorème de Fermat (1D)**\n\nPuisque $f$ admet un maximum local en $a$, pour $t$ suffisamment petit, on a :\n\n$$ g_i(t) = f(a + t e_i) \\leq f(a) = g_i(0) $$\n\nLa fonction $g_i$ admet donc un maximum local en $t=0$.\n\nDe plus, $f$ est différentiable, donc $g_i$ est dérivable en 0. D'après le théorème classique d'analyse réelle, sa dérivée s'annule :\n\n$$ g_i'(0) = 0 $$\n\n**Étape 3 : Lien avec les dérivées partielles**\n\nPar définition de la dérivée partielle, $g_i'(0)$ correspond exactement à la dérivée partielle de $f$ par rapport à $x_i$ au point $a$ :\n\n$$ g_i'(0) = \\lim_{t \\to 0} \\frac{f(a+te_i) - f(a)}{t} = \\frac{\\partial f}{\\partial x_i}(a) $$\n\nDonc, pour tout $i \\in \\{1, \\dots, n\\}$, on a $\\frac{\\partial f}{\\partial x_i}(a) = 0$.\n\n**Conclusion :**\n\nLe vecteur gradient, composé des dérivées partielles, est nul :\n\n$$ \\nabla f(a) = \\left( \\frac{\\partial f}{\\partial x_1}(a), \\dots, \\frac{\\partial f}{\\partial x_n}(a) \\right) = (0, \\dots, 0) $$\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "10189ead",
      "content": "#### Symétrie de la Hessienne (Théorème de Schwarz)\n\nSoit $f$ une fonction de classe $\\mathscr{C}^2$ sur un ouvert $U \\subset \\mathbb{R}^2$.\n\nProuver que $\\frac{\\partial^2 f}{\\partial x \\partial y}(a) = \\frac{\\partial^2 f}{\\partial y \\partial x}(a)$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nNous ne ferons pas la démonstration complète formelle qui est longue, mais l'argument clé.\n\nConsidérez la quantité \"différence seconde\" :\n\n$$ \\Delta = f(x+h, y+k) - f(x+h, y) - f(x, y+k) + f(x, y) $$\n\nL'idée est d'exprimer $\\Delta$ de deux façons différentes en utilisant le Théorème des Accroissements Finis (TAF) deux fois :\n\n1. D'abord en fixant $y$ et en faisant varier $x$.\n2. Puis en fixant $x$ et en faisant varier $y$.\n\n</details>",
      "solution": "\n\nCette preuve repose sur l'application répétée du Théorème des Accroissements Finis (TAF).\n\n**Étape 1 : Définition de la fonction auxiliaire**\n\nSoit $h, k$ petits. Posons la fonction auxiliaire $\\varphi(t) = f(x+t, y+k) - f(x+t, y)$.\n\nAlors la quantité $\\Delta$ (définie dans l'indice) s'écrit $\\Delta = \\varphi(h) - \\varphi(0)$.\n\n**Étape 2 : Première application du TAF**\n\nComme $f$ est dérivable, $\\varphi$ l'est aussi. Par le TAF, il existe $\\theta_1 \\in ]0, 1[$ tel que :\n\n$$ \\Delta = \\varphi(h) - \\varphi(0) = h \\varphi'(\\theta_1 h) $$\n\nOr $\\varphi'(t) = \\frac{\\partial f}{\\partial x}(x+t, y+k) - \\frac{\\partial f}{\\partial x}(x+t, y)$.\n\nDonc :\n\n$$ \\Delta = h \\left[ \\frac{\\partial f}{\\partial x}(x+\\theta_1 h, y+k) - \\frac{\\partial f}{\\partial x}(x+\\theta_1 h, y) \\right] $$\n\n**Étape 3 : Seconde application du TAF**\n\nAppliquons le TAF à la fonction $k \\mapsto \\frac{\\partial f}{\\partial x}(x+\\theta_1 h, y+k)$ entre $y$ et $y+k$. Il existe $\\theta_2 \\in ]0, 1[$ tel que :\n\n$$ \\frac{\\partial f}{\\partial x}(x+\\theta_1 h, y+k) - \\frac{\\partial f}{\\partial x}(x+\\theta_1 h, y) = k \\frac{\\partial}{\\partial y} \\left( \\frac{\\partial f}{\\partial x} \\right) (x+\\theta_1 h, y+\\theta_2 k) $$\n\nAinsi :\n\n$$ \\Delta = hk \\frac{\\partial^2 f}{\\partial y \\partial x}(x+\\theta_1 h, y+\\theta_2 k) $$\n\n**Étape 4 : Symétrie et Conclusion**\n\nSi nous avions commencé par définir $\\psi(t) = f(x+h, y+t) - f(x, y+t)$, nous aurions obtenu de manière symétrique (avec d'autres $\\theta_3, \\theta_4$) :\n\n$$ \\Delta = kh \\frac{\\partial^2 f}{\\partial x \\partial y}(x+\\theta_3 h, y+\\theta_4 k) $$\n\nEn égalant les deux expressions de $\\Delta$ et en divisant par $hk$ (non nul), puis en faisant tendre $(h, k) \\to (0, 0)$, la continuité des dérivées secondes (classe $\\mathscr{C}^2$) assure que les limites sont égales à la valeur au point $(x, y)$.\n\n**Conclusion :**\n\n$$ \\frac{\\partial^2 f}{\\partial y \\partial x}(x, y) = \\frac{\\partial^2 f}{\\partial x \\partial y}(x, y) $$\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "10189ead",
      "content": "#### Dérivation de la Formule de Taylor à l'ordre 2\n\nProuver la formule de Taylor à l'ordre 2 pour une fonction $f: U \\to \\mathbb{R}$ de classe $\\mathscr{C}^2$ au voisinage de $a$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez la paramétrisation du segment reliant $a$ à $a+h$.\n\nSoit $\\phi(t) = f(a + th)$ pour $t \\in [0, 1]$.\n\nAppliquez la formule de Maclaurin (Taylor-Young en 0) à la fonction $\\phi$ d'une seule variable :\n\n$\\phi(1) = \\phi(0) + \\phi'(0) + \\frac{1}{2}\\phi''(0) + o(1)$.\n\nLe travail consiste à calculer $\\phi'(t)$ et $\\phi''(t)$ en utilisant la règle de la chaîne (chain rule).\n\n</details>",
      "solution": "\n\nOn cherche à exprimer $f(a+h)$ en fonction de $f(a)$ et de ses dérivées.\n\n**Étape 1 : Paramétrisation et Taylor 1D**\n\nSoit $\\phi(t) = f(a + th)$. C'est une fonction de $\\mathbb{R}$ dans $\\mathbb{R}$.\n\nPuisque $f$ est $\\mathscr{C}^2$, $\\phi$ l'est aussi. La formule de Taylor-Young en $t=0$ pour $\\phi$ évaluée en $t=1$ donne :\n\n$$ \\phi(1) = \\phi(0) + \\phi'(0) \\cdot (1-0) + \\frac{1}{2}\\phi''(0) \\cdot (1-0)^2 + o(1) $$\n\nCe qui revient à dire, puisque $\\phi(1) = f(a+h)$ et $\\phi(0) = f(a)$ :\n\n$$ f(a+h) = f(a) + \\phi'(0) + \\frac{1}{2}\\phi''(0) + o(\\|h\\|^2) $$\n\n**Étape 2 : Calcul de la dérivée première**\n\nEn utilisant la règle de la chaîne pour la composée de fonctions :\n\n$$ \\phi'(t) = \\langle \\nabla f(a+th), h \\rangle = \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(a+th) h_i $$\n\nDonc pour $t=0$ :\n\n$$ \\phi'(0) = \\langle \\nabla f(a), h \\rangle $$\n\n**Étape 3 : Calcul de la dérivée seconde**\n\nDérivons $\\phi'(t)$ une seconde fois par rapport à $t$ :\n\n$$ \\phi''(t) = \\frac{d}{dt} \\left( \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(a+th) h_i \\right) $$\n\nOn applique à nouveau la règle de la chaîne sur chaque dérivée partielle :\n\n$$ \\phi''(t) = \\sum_{i=1}^n \\left( \\sum_{j=1}^n \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}(a+th) h_j \\right) h_i $$\n\nPour $t=0$ :\n\n$$ \\phi''(0) = \\sum_{i,j} \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}(a) h_i h_j = \\langle H_f(a)h, h \\rangle $$\n\n**Conclusion :**\n\nEn remplaçant dans l'expression de l'étape 1, on obtient la formule de Taylor vectorielle :\n\n$$ f(a+h) = f(a) + \\langle \\nabla f(a), h \\rangle + \\frac{1}{2} \\langle H_f(a)h, h \\rangle + o(\\|h\\|^2) $$\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "10189ead",
      "content": "#### Gradient orthogonal aux lignes de niveau\n\nProuver que si $c$ est une valeur régulière et $S = \\{ x \\in \\mathbb{R}^n \\mid f(x) = c \\}$ est la ligne de niveau associée, alors le gradient $\\nabla f(x)$ est orthogonal à $S$ en tout point $x \\in S$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nL'orthogonalité à une surface (ou courbe) se définit par l'orthogonalité au vecteur tangent d'une courbe tracée sur cette surface.\n\nConsidérez une courbe dérivable $\\gamma : ]-\\varepsilon, \\varepsilon[ \\to S$ telle que $\\gamma(0) = x$.\n\nQuelle équation vérifie la fonction composée $t \\mapsto f(\\gamma(t))$ ?\n\nDérivez cette équation par rapport à $t$.\n\n</details>",
      "solution": "\n\nNous devons montrer que le gradient est orthogonal à tout vecteur tangent à la ligne de niveau.\n\n**Étape 1 : Caractérisation de la courbe sur le niveau**\n\nSoit $\\gamma(t)$ une courbe différentiable tracée entièrement sur la ligne de niveau $S$, passant par $x$ à l'instant $t=0$ (c'est-à-dire $\\gamma(0) = x$).\n\nPar définition de la ligne de niveau, la valeur de $f$ est constante sur cette courbe :\n\n$$ \\forall t, \\quad f(\\gamma(t)) = c $$\n\n**Étape 2 : Dérivation**\n\nDérivons cette égalité par rapport à $t$ en utilisant la règle de la chaîne (chain rule) :\n\n$$ \\frac{d}{dt} (f(\\gamma(t))) = \\frac{d}{dt}(c) = 0 $$\n\nD'autre part :\n\n$$ \\frac{d}{dt} (f(\\gamma(t))) = \\langle \\nabla f(\\gamma(t)), \\gamma'(t) \\rangle $$\n\n**Étape 3 : Évaluation en $t=0$**\n\nEn $t=0$, on a $\\gamma(0) = x$. Le vecteur $\\gamma'(0)$ représente un vecteur tangent arbitraire à la surface $S$ au point $x$. L'équation devient :\n\n$$ \\langle \\nabla f(x), \\gamma'(0) \\rangle = 0 $$\n\n**Conclusion :**\n\nLe produit scalaire entre le gradient $\\nabla f(x)$ et tout vecteur tangent $\\gamma'(0)$ est nul. Le gradient est donc bien orthogonal à la ligne de niveau (ou surface de niveau) en $x$.\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "10189ead",
      "content": "#### Condition suffisante de minimum local (Critère de la Hessienne)\n\nSoit $a$ un point critique d'une fonction $f$ de classe $\\mathscr{C}^2$.\n\nProuver que si la matrice Hessienne $H_f(a)$ est définie positive, alors $a$ est un minimum local strict.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez la formule de Taylor à l'ordre 2 :\n\n$f(a+h) - f(a) \\approx \\frac{1}{2} \\langle H_f(a)h, h \\rangle$.\n\nUne matrice définie positive $H$ vérifie une propriété de coercivité : il existe $\\lambda > 0$ (la plus petite valeur propre) tel que $\\langle Hh, h \\rangle \\geq \\lambda \\|h\\|^2$.\n\nComparez le terme quadratique avec le reste $o(\\|h\\|^2)$. Le terme quadratique doit \"gagner\" proche de 0.\n\n</details>",
      "solution": "\n\nOn veut montrer que $f(a+h) > f(a)$ pour tout petit $h \\neq 0$.\n\n**Étape 1 : Développement de Taylor**\n\nComme $a$ est un point critique, $\\nabla f(a) = 0$. La formule de Taylor donne :\n\n$$ f(a+h) - f(a) = \\frac{1}{2} \\langle H_f(a)h, h \\rangle + \\|h\\|^2 \\varepsilon(h) $$\n\noù $\\varepsilon(h) \\to 0$ quand $h \\to 0$.\n\n**Étape 2 : Propriété de la Hessienne définie positive**\n\nSoit $\\lambda_{\\min}$ la plus petite valeur propre de $H_f(a)$. Puisque la matrice est définie positive, toutes ses valeurs propres sont strictement positives, donc $\\lambda_{\\min} > 0$.\n\nUne propriété d'algèbre linéaire pour les matrices symétriques donne :\n\n$$ \\forall h \\in \\mathbb{R}^n, \\quad \\langle H_f(a)h, h \\rangle \\geq \\lambda_{\\min} \\|h\\|^2 $$\n\n**Étape 3 : Inégalité**\n\nSubstituons cette inégalité dans Taylor :\n\n$$ f(a+h) - f(a) \\geq \\frac{1}{2} \\lambda_{\\min} \\|h\\|^2 + \\|h\\|^2 \\varepsilon(h) $$\n\n$$ f(a+h) - f(a) \\geq \\|h\\|^2 \\left( \\frac{\\lambda_{\\min}}{2} + \\varepsilon(h) \\right) $$\n\n**Étape 4 : Argument de domination**\n\nComme $\\varepsilon(h) \\to 0$ quand $h \\to 0$, il existe un voisinage de $0$ (une petite boule) dans lequel $|\\varepsilon(h)| < \\frac{\\lambda_{\\min}}{4}$.\n\nDans ce voisinage, le terme entre parenthèses est strictement positif (au moins $\\frac{\\lambda_{\\min}}{4}$).\n\nDonc, pour $h \\neq 0$ dans ce voisinage :\n\n$$ f(a+h) - f(a) > 0 \\implies f(a+h) > f(a) $$\n\n**Conclusion :**\n\n$f(a)$ est strictement inférieur aux valeurs voisines. $a$ est donc un minimum local strict.\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "10189ead",
      "content": "#### Nature indéterminée pour Hessienne avec valeurs propres de signes opposés (Point Selle)\n\nSoit $a$ un point critique tel que $H_f(a)$ admette une valeur propre strictement positive $\\lambda > 0$ et une valeur propre strictement négative $\\mu < 0$.\n\nProuver que $a$ n'est ni un maximum local, ni un minimum local.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nIl suffit de trouver deux directions d'approche différentes.\n\nSi on s'éloigne de $a$ dans la direction du vecteur propre associé à $\\lambda > 0$, la fonction croît (convexité).\n\nSi on s'éloigne de $a$ dans la direction du vecteur propre associé à $\\mu < 0$, la fonction décroît (concavité).\n\nUtilisez Taylor restreint à ces droites.\n\n</details>",
      "solution": "\n\nPour prouver que ce n'est pas un extremum, nous allons montrer que $f$ prend des valeurs supérieures à $f(a)$ et des valeurs inférieures à $f(a)$ dans tout voisinage de $a$.\n\n**Étape 1 : Direction de croissance**\n\nSoit $u$ un vecteur propre normé associé à la valeur propre $\\lambda > 0$. Donc $H_f(a)u = \\lambda u$.\n\nConsidérons $f$ sur la droite passant par $a$ dirigée par $u$ ($h = tu$).\n\n$$ f(a+tu) - f(a) = \\frac{1}{2} \\langle H_f(a)(tu), (tu) \\rangle + o(t^2) $$\n\n$$ f(a+tu) - f(a) = \\frac{t^2}{2} \\lambda \\langle u, u \\rangle + o(t^2) = \\frac{\\lambda}{2} t^2 + t^2 \\varepsilon(t) $$\n\nComme $\\lambda > 0$, pour $t$ assez petit, cette expression est positive. Donc il existe des points arbitrairement proches tels que $f(x) > f(a)$. Donc $a$ n'est pas un maximum.\n\n**Étape 2 : Direction de décroissance**\n\nSoit $v$ un vecteur propre normé associé à la valeur propre $\\mu < 0$.\n\nDe manière analogue :\n\n$$ f(a+tv) - f(a) = \\frac{t^2}{2} \\mu \\|v\\|^2 + o(t^2) = \\frac{\\mu}{2} t^2 + o(t^2) $$\n\nComme $\\mu < 0$, pour $t$ assez petit, cette expression est négative. Donc il existe des points arbitrairement proches tels que $f(x) < f(a)$. Donc $a$ n'est pas un minimum.\n\n**Conclusion :**\n\nPuisque dans tout voisinage de $a$, $f$ prend des valeurs supérieures et inférieures à $f(a)$, $a$ n'est ni un minimum ni un maximum. C'est un point selle (ou col).\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "10189ead",
      "content": "#### Lien entre Matrice Hessienne et Forme Quadratique\n\nProuver que l'expression matricielle $\\langle H_f(a)h, h \\rangle$ correspond bien à la somme pondérée des dérivées secondes.\n\nC'est-à-dire : $\\langle H_f(a)h, h \\rangle = \\sum_{i,j} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(a) h_i h_j$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nC'est un exercice d'algèbre linéaire et de notation.\n\nÉcrivez le vecteur $v = H_f(a)h$ composante par composante en utilisant la règle du produit matrice-vecteur.\n\nEnsuite, faites le produit scalaire de $v$ avec $h$.\n\n</details>",
      "solution": "\n\nSoit $H = H_f(a)$ pour simplifier les notations, et $h = (h_1, \\dots, h_n)^T$.\n\n**Étape 1 : Produit Matrice-Vecteur**\n\nCalculons le vecteur $v = Hh$. La $i$-ème composante de ce vecteur est le produit scalaire de la $i$-ème ligne de $H$ par le vecteur $h$.\n\nLes éléments de la matrice sont $H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$.\n\nDonc :\n\n$$ v_i = (Hh)_i = \\sum_{j=1}^n H_{ij} h_j = \\sum_{j=1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} h_j $$\n\n**Étape 2 : Produit Scalaire final**\n\nCalculons maintenant $\\langle v, h \\rangle = \\sum_{i=1}^n v_i h_i$.\n\n$$ \\langle Hh, h \\rangle = \\sum_{i=1}^n \\left( \\sum_{j=1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} h_j \\right) h_i $$\n\n**Conclusion :**\n\nEn regroupant les sommes, on obtient bien la forme quadratique associée aux dérivées secondes :\n\n$$ \\langle H_f(a)h, h \\rangle = \\sum_{i=1}^n \\sum_{j=1}^n \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(a) h_i h_j $$\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "10189ead",
      "content": "#### Unicité de l'extremum pour une fonction convexe\n\nSoit $f: \\mathbb{R}^n \\to \\mathbb{R}$ une fonction convexe et différentiable.\n\nProuver que si $a$ est un point critique de $f$, alors $a$ est un minimum global.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez la définition différentielle de la convexité (premier ordre).\n\nUne fonction différentiable est convexe si et seulement si son graphe est au-dessus de ses plans tangents :\n\n$$ \\forall x, y \\in \\mathbb{R}^n, \\quad f(y) \\geq f(x) + \\langle \\nabla f(x), y-x \\rangle $$\n\nAppliquez cette inégalité en prenant $x=a$ (le point critique).\n\n</details>",
      "solution": "\n\n**Étape 1 : Propriété de convexité**\n\nPuisque $f$ est convexe et différentiable, elle satisfait l'inégalité du gradient pour tous points $a$ et $y$ dans le domaine :\n\n$$ f(y) \\geq f(a) + \\langle \\nabla f(a), y-a \\rangle $$\n\n**Étape 2 : Utilisation du point critique**\n\nL'hypothèse est que $a$ est un point critique, donc $\\nabla f(a) = 0$.\n\nLe terme du produit scalaire s'annule :\n\n$$ \\langle \\nabla f(a), y-a \\rangle = \\langle 0, y-a \\rangle = 0 $$\n\n**Étape 3 : Conclusion**\n\nL'inégalité devient simplement :\n\n$$ \\forall y \\in \\mathbb{R}^n, \\quad f(y) \\geq f(a) $$\n\nCela correspond exactement à la définition d'un **minimum global**.\n\n*Note : Si la fonction est strictement convexe, ce minimum est de plus unique.*\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "10189ead",
      "content": "#### Le problème de la moindre distance (Application)\n\nSoit $K$ un fermé de $\\mathbb{R}^n$ et $p \\in \\mathbb{R}^n$ un point extérieur. On cherche $x \\in K$ minimisant la distance à $p$.\n\nProuver que minimiser la distance $d(x, p)$ revient à minimiser la distance au carré $f(x) = \\|x-p\\|^2$, et calculer le gradient de cette fonction.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\n1. La fonction racine carrée $t \\mapsto \\sqrt{t}$ est strictement croissante sur $\\mathbb{R}^+$.\n2. Pour le gradient, développez $\\|x-p\\|^2 = \\langle x-p, x-p \\rangle$ et utilisez les règles de dérivation du produit scalaire, ou les dérivées partielles.\n\n</details>",
      "solution": "\n\n**Étape 1 : Équivalence des problèmes**\n\nSoit $g(x) = \\|x-p\\|$. Comme $\\|x-p\\| \\geq 0$, minimiser $g(x)$ est équivalent à minimiser $(g(x))^2$. En effet, si $0 \\le A < B$, alors $A^2 < B^2$. L'ordre est préservé.\n\nIl est plus simple de travailler avec $f(x) = \\|x-p\\|^2$ car la racine carrée n'est pas dérivable en 0, alors que le carré de la norme est $\\mathscr{C}^\\infty$ partout.\n\n**Étape 2 : Calcul du gradient de $f$**\n\nOn peut écrire $f(x) = \\sum_{i=1}^n (x_i - p_i)^2$.\n\nCalculons la dérivée partielle par rapport à $x_k$ :\n\n$$ \\frac{\\partial f}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( (x_k - p_k)^2 + \\sum_{i \\neq k} (x_i - p_i)^2 \\right) $$\n\nLes termes où $i \\neq k$ sont constants par rapport à $x_k$.\n\n$$ \\frac{\\partial f}{\\partial x_k} = 2(x_k - p_k) $$\n\n**Conclusion :**\n\nLe vecteur gradient est composé des dérivées partielles :\n\n$$ \\nabla f(x) = (2(x_1 - p_1), \\dots, 2(x_n - p_n)) = 2(x - p) $$\n\nCela signifie que le gradient pointe dans la direction opposée à $p$.\n\n",
      "options": []
    }
  ]
}