{
  "info": {
    "id": "cf85e459",
    "title": "Réduction des endomorphismes auto-adjoints - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Réduction des endomorphismes auto-adjoints",
    "course": "Géométrie",
    "tags": [
      "endomorphismes",
      "auto-adjoints",
      "déterminants",
      "diagonalisation",
      "décomposition polaire"
    ],
    "count": 11
  },
  "cards": [
    {
      "id": "1",
      "stackId": "cf85e459",
      "content": "#### Caractérisation de l'inversibilité par le déterminant\n\nProuver qu'une matrice carrée $A \\in M_n(\\mathbb{K})$ est inversible si et seulement si son déterminant est non nul.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour le sens direct ($\\Rightarrow$), si $A$ est inversible, il existe $A^{-1}$ telle que $AA^{-1} = I_n$. Utilisez la propriété de multiplicativité du déterminant.\n\nPour le sens réciproque ($\\Leftarrow$), raisonnez par contraposée. Si $A$ n'est pas inversible, ses vecteurs colonnes sont linéairement dépendants. Que pouvez-vous en déduire sur le déterminant de $A$, sachant qu'il s'agit d'une forme n-linéaire alternée ?\n\n</details>",
      "solution": "\n\nSoit $A \\in M_n(\\mathbb{K})$.\n\n**Étape 1 : Sens direct ($A$ inversible $\\Rightarrow \\det(A) \\neq 0$)**\n\nSupposons que $A$ est inversible. Il existe alors une matrice $A^{-1}$ telle que $A A^{-1} = I_n$, où $I_n$ est la matrice identité d'ordre $n$.\n\nEn utilisant la propriété de multiplicativité du déterminant, nous avons :\n\n$\\det(A A^{-1}) = \\det(I_n)$.\n\nCeci se décompose en $\\det(A) \\det(A^{-1}) = 1$.\n\nLe produit de deux scalaires étant égal à 1, aucun des deux ne peut être nul. En particulier, $\\det(A) \\neq 0$.\n\n**Étape 2 : Sens réciproque ($\\det(A) \\neq 0 \\Rightarrow A$ inversible)**\n\nNous allons prouver la contraposée : si $A$ n'est pas inversible, alors $\\det(A) = 0$.\n\nSi $A$ n'est pas inversible, l'endomorphisme associé n'est pas bijectif. Son rang est strictement inférieur à $n$. Cela signifie que les vecteurs colonnes de $A$, notés $C_1, C_2, \\dots, C_n$, forment une famille linéairement dépendante.\n\nIl existe donc une colonne, disons $C_j$, qui est une combinaison linéaire des autres colonnes :\n\n$C_j = \\sum_{k \\neq j} \\alpha_k C_k$.\n\nLe déterminant est une forme n-linéaire par rapport à ses colonnes. Ainsi :\n\n$\\det(A) = \\det(C_1, \\dots, C_j, \\dots, C_n) = \\det(C_1, \\dots, \\sum_{k \\neq j} \\alpha_k C_k, \\dots, C_n)$.\n\nPar n-linéarité, on peut décomposer cette somme :\n\n$\\det(A) = \\sum_{k \\neq j} \\alpha_k \\det(C_1, \\dots, C_k, \\dots, C_n)$.\n\nDans cette expression, la colonne en position $j$ est $C_k$.\n\nPour chaque terme de la somme, la matrice dont on calcule le déterminant a deux colonnes identiques (la colonne $C_k$ en position $k$ et en position $j$). Puisque le déterminant est une forme alternée, son résultat est nul si deux de ses vecteurs d'entrée sont identiques.\n\nAinsi, chaque terme de la somme est nul, et donc $\\det(A) = 0$.\n\n**Conclusion**\n\nNous avons prouvé les deux implications, donc $A$ est inversible si et seulement si $\\det(A) \\neq 0$.\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "cf85e459",
      "content": "#### Déterminant d'une matrice triangulaire\n\nProuver que le déterminant d'une matrice triangulaire est égal au produit de ses coefficients diagonaux.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez la formule de Leibniz pour le déterminant :\n\n$$ \\det(A) = \\sum_{\\sigma \\in S_n} \\varepsilon(\\sigma) \\prod_{i=1}^{n} a_{\\sigma(i), i} $$\n\nConsidérez une matrice triangulaire supérieure $A=(a_{ij})$ où $a_{ij}=0$ si $i > j$. Analysez pour quelles permutations $\\sigma$ le produit $\\prod_{i=1}^{n} a_{\\sigma(i), i}$ peut être non nul.\n\n</details>",
      "solution": "\n\nSoit $A = (a_{ij})$ une matrice triangulaire supérieure de taille $n \\times n$. Cela signifie que $a_{ij} = 0$ pour tout $i > j$. Nous voulons prouver que $\\det(A) = \\prod_{i=1}^{n} a_{ii}$.\n\nLa démonstration pour une matrice triangulaire inférieure est analogue en utilisant la propriété $\\det(A) = \\det({}^tA)$.\n\n**Étape 1 : Analyse des termes de la formule de Leibniz**\n\nLa formule de Leibniz est $\\det(A) = \\sum_{\\sigma \\in S_n} \\varepsilon(\\sigma) \\prod_{i=1}^{n} a_{\\sigma(i), i}$.\n\nUn terme du produit est $a_{\\sigma(1), 1} a_{\\sigma(2), 2} \\cdots a_{\\sigma(n), n}$.\n\nPour que ce produit soit potentiellement non nul, il faut que chaque facteur $a_{\\sigma(i), i}$ soit non nul.\n\nÉtant donné que $A$ est triangulaire supérieure, nous devons avoir $\\sigma(i) \\le i$ pour tous les $i=1, \\dots, n$.\n\n**Étape 2 : Identification de la seule permutation possible**\n\nAnalysons la condition $\\sigma(i) \\le i$ pour tout $i$:\n\n- Pour $i=1$, nous devons avoir $\\sigma(1) \\le 1$, ce qui impose $\\sigma(1)=1$.\n- Pour $i=2$, nous devons avoir $\\sigma(2) \\le 2$. Comme $\\sigma$ est une permutation et que $\\sigma(1)=1$, $\\sigma(2)$ ne peut pas être 1. Donc $\\sigma(2)=2$.\n- En continuant par récurrence, supposons que $\\sigma(k)=k$ pour tout $k < i$. Pour $i$, on a $\\sigma(i) \\le i$. Comme les valeurs $1, 2, \\dots, i-1$ sont déjà prises par $\\sigma(1), \\dots, \\sigma(i-1)$, la seule valeur possible pour $\\sigma(i)$ est $i$.\n\nAinsi, la seule permutation $\\sigma \\in S_n$ pour laquelle le produit $\\prod_{i=1}^{n} a_{\\sigma(i), i}$ peut être non nul est la permutation identité, $\\sigma = \\text{Id}$.\n\n**Étape 3 : Calcul final du déterminant**\n\nLa somme dans la formule de Leibniz se réduit à un seul terme, celui où $\\sigma = \\text{Id}$.\n\nLa permutation identité est une permutation paire, donc sa signature $\\varepsilon(\\text{Id})$ est $+1$.\n\nLe déterminant est donc :\n\n$$ \\det(A) = \\varepsilon(\\text{Id}) \\prod_{i=1}^{n} a_{\\text{Id}(i), i} = 1 \\cdot \\prod_{i=1}^{n} a_{i, i} = a_{11} a_{22} \\cdots a_{nn} $$\n\n**Conclusion**\n\nLe déterminant d'une matrice triangulaire est bien le produit de ses coefficients diagonaux.\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "cf85e459",
      "content": "#### Somme directe des sous-espaces propres\n\nProuver que la somme de sous-espaces propres associés à des valeurs propres distinctes est une somme directe.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoient $\\lambda_1, \\dots, \\lambda_k$ des valeurs propres distinctes. Pour prouver que $E_{\\lambda_1} + \\dots + E_{\\lambda_k}$ est une somme directe, il faut montrer que si $x_1 + \\dots + x_k = 0$ avec $x_i \\in E_{\\lambda_i}$, alors tous les $x_i$ sont nuls.\n\nProcédez par récurrence sur le nombre $k$ de sous-espaces propres. Pour l'étape d'induction, appliquez l'endomorphisme $f$ à l'équation $x_1 + \\dots + x_k = 0$ et combinez le résultat avec l'équation originale pour éliminer l'un des vecteurs.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme de $E$, et soient $\\lambda_1, \\dots, \\lambda_k$ des valeurs propres distinctes de $f$. Notons $E_i = E_{\\lambda_i}(f)$ le sous-espace propre associé à $\\lambda_i$. Nous voulons montrer que la somme $E_1 + \\dots + E_k$ est directe, c'est-à-dire $E_1 \\oplus \\dots \\oplus E_k$.\n\nCela revient à montrer que si $x_1 \\in E_1, \\dots, x_k \\in E_k$ sont tels que\n\n$$ x_1 + x_2 + \\dots + x_k = 0 \\quad (*)$$\n\nalors $x_1 = x_2 = \\dots = x_k = 0$.\n\nNous procédons par récurrence sur $k$.\n\n**Cas de base (k=1)** : Si $x_1 = 0$, le résultat est trivial.\n\n**Cas de base (k=2)** : Soient $x_1 \\in E_1$ et $x_2 \\in E_2$ tels que $x_1 + x_2 = 0$.\n\nEn appliquant $f$, on obtient $f(x_1) + f(x_2) = f(0) = 0$.\n\nComme $x_1 \\in E_1$ et $x_2 \\in E_2$, on a $f(x_1) = \\lambda_1 x_1$ et $f(x_2) = \\lambda_2 x_2$. L'équation devient $\\lambda_1 x_1 + \\lambda_2 x_2 = 0$.\n\nDe l'équation initiale, $x_2 = -x_1$. En substituant : $\\lambda_1 x_1 - \\lambda_2 x_1 = 0$, soit $(\\lambda_1 - \\lambda_2)x_1 = 0$.\n\nPuisque les valeurs propres sont distinctes, $\\lambda_1 - \\lambda_2 \\neq 0$, donc $x_1 = 0$. Par suite, $x_2 = 0$. La somme est directe.\n\n**Étape d'induction** : Supposons que la propriété est vraie pour $k-1$ sous-espaces propres.\n\nPartons de l'équation $(*)$: $x_1 + x_2 + \\dots + x_k = 0$.\n\nAppliquons l'endomorphisme $f$ :\n\n$f(x_1) + f(x_2) + \\dots + f(x_k) = 0$\n\n$\\lambda_1 x_1 + \\lambda_2 x_2 + \\dots + \\lambda_k x_k = 0 \\quad (**)$\n\nMultiplions l'équation $(*)$ par $\\lambda_k$ :\n\n$\\lambda_k x_1 + \\lambda_k x_2 + \\dots + \\lambda_k x_k = 0 \\quad (***)$\n\nSoustrayons $(**)$ de $(***)$ :\n\n$(\\lambda_k - \\lambda_1) x_1 + (\\lambda_k - \\lambda_2) x_2 + \\dots + (\\lambda_k - \\lambda_{k-1}) x_{k-1} = 0$.\n\nC'est une somme de $k-1$ vecteurs, où chaque vecteur $(\\lambda_k - \\lambda_i)x_i$ appartient à $E_i$. Par hypothèse de récurrence, la somme des $k-1$ premiers sous-espaces est directe. Donc, tous les termes de cette somme sont nuls :\n\n$(\\lambda_k - \\lambda_i) x_i = 0$ pour $i = 1, \\dots, k-1$.\n\nComme les valeurs propres sont distinctes, $\\lambda_k - \\lambda_i \\neq 0$ pour $i < k$. Cela implique que $x_i = 0$ pour $i=1, \\dots, k-1$.\n\nEn reportant ces résultats dans l'équation initiale $(*)$, il reste $x_k = 0$.\n\n**Conclusion**\n\nNous avons montré que tous les $x_i$ sont nuls, ce qui prouve que la somme des sous-espaces propres est directe.\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "cf85e459",
      "content": "#### Équivalence entre valeurs propres et racines du polynôme caractéristique\n\nProuver qu'un scalaire $\\lambda \\in \\mathbb{K}$ est une valeur propre d'un endomorphisme $f$ si et seulement si $\\lambda$ est une racine de son polynôme caractéristique $\\chi_f$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSuivez la chaîne d'équivalences logiques.\n\n$\\lambda$ est une valeur propre de $f$\n\n$\\iff$ Il existe un vecteur $x \\neq 0$ tel que $f(x) = \\lambda x$.\n\n$\\iff$ ...\n\n$\\iff$ L'endomorphisme $(f - \\lambda \\text{Id})$ n'est pas ...\n\n$\\iff$ La matrice de $(f - \\lambda \\text{Id})$ n'est pas ...\n\n$\\iff \\det(f - \\lambda \\text{Id}) = \\dots$\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme sur un $\\mathbb{K}$-espace vectoriel $E$ de dimension finie, et $\\lambda \\in \\mathbb{K}$.\n\nPar définition, $\\lambda$ est une valeur propre de $f$ si et seulement s'il existe un vecteur non nul $x \\in E$ tel que $f(x) = \\lambda x$.\n\n**Étape 1 : Réécriture de la condition de valeur propre**\n\nL'équation $f(x) = \\lambda x$ peut être réécrite comme $f(x) - \\lambda x = 0$.\n\nEn utilisant l'endomorphisme identité $\\text{Id}_E$, on a $f(x) - \\lambda \\text{Id}_E(x) = 0$.\n\nPar linéarité, cela équivaut à $(f - \\lambda \\text{Id}_E)(x) = 0$.\n\nAinsi, $\\lambda$ est une valeur propre de $f$ si et seulement s'il existe un vecteur non nul $x$ dans le noyau de l'endomorphisme $(f - \\lambda \\text{Id}_E)$.\n\n**Étape 2 : Lien avec l'inversibilité**\n\nL'existence d'un vecteur non nul dans le noyau de $(f - \\lambda \\text{Id}_E)$ signifie que $\\ker(f - \\lambda \\text{Id}_E) \\neq \\{0\\}$.\n\nUn endomorphisme d'un espace de dimension finie est injectif si et seulement si son noyau est réduit au vecteur nul. Donc, $\\ker(f - \\lambda \\text{Id}_E) \\neq \\{0\\}$ est équivalent à dire que l'endomorphisme $(f - \\lambda \\text{Id}_E)$ n'est pas injectif.\n\nEn dimension finie, un endomorphisme est injectif si et seulement s'il est bijectif (c'est-à-dire inversible). Par conséquent, $\\lambda$ est une valeur propre si et seulement si $(f - \\lambda \\text{Id}_E)$ n'est pas inversible.\n\n**Étape 3 : Lien avec le déterminant**\n\nUn endomorphisme (ou sa matrice dans n'importe quelle base) est inversible si et seulement si son déterminant est non nul.\n\nDonc, l'endomorphisme $(f - \\lambda \\text{Id}_E)$ n'est pas inversible si et seulement si son déterminant est nul :\n\n$$ \\det(f - \\lambda \\text{Id}_E) = 0 $$\n\n**Conclusion**\n\nPar définition, le polynôme caractéristique de $f$ est $\\chi_f(\\lambda) = \\det(f - \\lambda \\text{Id}_E)$.\n\nNous avons donc montré l'équivalence :\n\n$\\lambda$ est une valeur propre de $f \\iff \\chi_f(\\lambda) = 0$.\n\nCeci signifie que les valeurs propres de $f$ sont exactement les racines de son polynôme caractéristique.\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "cf85e459",
      "content": "#### Diagonalisabilité avec $n$ valeurs propres distinctes\n\nProuver qu'un endomorphisme $f$ d'un espace vectoriel $E$ de dimension $n$ qui possède $n$ valeurs propres distinctes est diagonalisable.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nRappelez-vous qu'un endomorphisme est diagonalisable s'il existe une base de $E$ formée de vecteurs propres.\n\nUtilisez le fait que les sous-espaces propres associés à des valeurs propres distinctes sont en somme directe. Quelle est la dimension minimale de chaque sous-espace propre ? Concluez sur la dimension de la somme directe.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme sur un espace vectoriel $E$ de dimension $n$.\n\nSupposons que $f$ admette $n$ valeurs propres distinctes, que nous noterons $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$.\n\n**Étape 1 : Sous-espaces propres**\n\nPour chaque valeur propre $\\lambda_i$, le sous-espace propre associé $E_i = E_{\\lambda_i}(f)$ est non trivial, c'est-à-dire que sa dimension est au moins 1. En effet, par définition d'une valeur propre, il existe au moins un vecteur propre (non nul) qui lui est associé. Donc $\\dim(E_i) \\ge 1$ pour tout $i \\in \\{1, \\dots, n\\}$.\n\n**Étape 2 : Somme directe des sous-espaces propres**\n\nComme les $n$ valeurs propres $\\lambda_1, \\dots, \\lambda_n$ sont distinctes, nous savons que la somme de leurs sous-espaces propres est une somme directe. Notons $S = E_1 \\oplus E_2 \\oplus \\dots \\oplus E_n$.\n\nCe sous-espace $S$ est un sous-espace vectoriel de $E$.\n\n**Étape 3 : Dimension de la somme directe**\n\nLa dimension d'une somme directe de sous-espaces est la somme des dimensions de ces sous-espaces :\n\n$$ \\dim(S) = \\dim(E_1) + \\dim(E_2) + \\dots + \\dim(E_n) $$\n\nComme $\\dim(E_i) \\ge 1$ pour chaque $i$, on a :\n\n$$ \\dim(S) \\ge \\underbrace{1 + 1 + \\dots + 1}_{n \\text{ fois}} = n $$\n\nDonc, $\\dim(S) \\ge n$.\n\n**Étape 4 : Conclusion**\n\nLe sous-espace $S$ est inclus dans $E$, qui est de dimension $n$. Donc, $\\dim(S) \\le \\dim(E) = n$.\n\nEn combinant les deux inégalités, nous avons $\\dim(S) = n$.\n\nPuisque $S$ est un sous-espace de $E$ de même dimension que $E$, on a $S=E$.\n\nNous avons donc montré que $E = E_1 \\oplus E_2 \\oplus \\dots \\oplus E_n$.\n\nCela signifie que l'on peut former une base de $E$ en juxtaposant une base de chaque sous-espace propre $E_i$. Une telle base est entièrement constituée de vecteurs propres de $f$.\n\nPar définition, l'existence d'une base de vecteurs propres signifie que $f$ est diagonalisable.\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "cf85e459",
      "content": "#### Réalité des valeurs propres d'un endomorphisme auto-adjoint\n\nProuver que les valeurs propres d'un endomorphisme auto-adjoint sur un espace hermitien sont réelles.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoit $f$ un endomorphisme auto-adjoint sur un espace hermitien $E$, et soit $\\lambda \\in \\mathbb{C}$ une valeur propre associée au vecteur propre $x \\neq 0$.\n\nCalculez le produit scalaire $\\langle f(x), x \\rangle$.\n\nD'une part, utilisez $f(x) = \\lambda x$. D'autre part, utilisez la propriété d'auto-adjontion $\\langle f(x), x \\rangle = \\langle x, f(x) \\rangle$. Comparez les deux expressions obtenues.\n\n</details>",
      "solution": "\n\nSoit $E$ un espace vectoriel hermitien (sur $\\mathbb{C}$) et $f$ un endomorphisme auto-adjoint sur $E$.\n\nSoit $\\lambda \\in \\mathbb{C}$ une valeur propre de $f$, et $x \\in E$ un vecteur propre associé, avec $x \\neq 0$.\n\nPar définition, on a $f(x) = \\lambda x$.\n\n**Étape 1 : Calcul de $\\langle f(x), x \\rangle$ en utilisant la définition du vecteur propre**\n\nEn utilisant $f(x)=\\lambda x$ dans le produit scalaire $\\langle f(x), x \\rangle$, on obtient :\n\n$$ \\langle f(x), x \\rangle = \\langle \\lambda x, x \\rangle $$\n\nPar sesquilinéarité du produit scalaire hermitien, le scalaire sort de la première composante :\n\n$$ \\langle f(x), x \\rangle = \\lambda \\langle x, x \\rangle = \\lambda \\|x\\|^2 $$\n\n**Étape 2 : Calcul de $\\langle f(x), x \\rangle$ en utilisant la propriété d'auto-adjontion**\n\nPuisque $f$ est auto-adjoint, $f=f^*$, on a $\\langle f(x), y \\rangle = \\langle x, f(y) \\rangle$ pour tous $x, y \\in E$. En particulier pour $y=x$ :\n\n$$ \\langle f(x), x \\rangle = \\langle x, f(x) \\rangle $$\n\nRemplaçons $f(x)$ par $\\lambda x$ dans le membre de droite :\n\n$$ \\langle f(x), x \\rangle = \\langle x, \\lambda x \\rangle $$\n\nPar sesquilinéarité, le scalaire sort de la seconde composante en prenant son conjugué :\n\n$$ \\langle f(x), x \\rangle = \\bar{\\lambda} \\langle x, x \\rangle = \\bar{\\lambda} \\|x\\|^2 $$\n\n**Étape 3 : Comparaison et conclusion**\n\nEn égalant les deux expressions obtenues pour $\\langle f(x), x \\rangle$, on a :\n\n$$ \\lambda \\|x\\|^2 = \\bar{\\lambda} \\|x\\|^2 $$\n\n$$ (\\lambda - \\bar{\\lambda}) \\|x\\|^2 = 0 $$\n\nComme $x$ est un vecteur propre, il est non nul, donc $\\|x\\|^2 \\neq 0$.\n\nOn en déduit que $\\lambda - \\bar{\\lambda} = 0$, ce qui signifie que $\\lambda = \\bar{\\lambda}$.\n\nUn nombre complexe égal à son conjugué est un nombre réel.\n\n**Conclusion**\n\nToute valeur propre d'un endomorphisme auto-adjoint est réelle.\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "cf85e459",
      "content": "#### Orthogonalité des sous-espaces propres d'un endomorphisme auto-adjoint\n\nProuver que les sous-espaces propres d'un endomorphisme auto-adjoint associés à des valeurs propres distinctes sont orthogonaux.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoit $f$ un endomorphisme auto-adjoint. Soient $\\lambda$ et $\\mu$ deux valeurs propres distinctes, avec $x \\in E_\\lambda$ et $y \\in E_\\mu$.\n\nLe but est de montrer que $\\langle x, y \\rangle = 0$.\n\nConsidérez la quantité $\\langle f(x), y \\rangle$. Calculez-la de deux manières différentes :\n\n1. En utilisant $f(x) = \\lambda x$.\n2. En utilisant la propriété d'auto-adjontion pour \"déplacer\" $f$ sur $y$, puis en utilisant $f(y) = \\mu y$.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme auto-adjoint sur un espace euclidien ou hermitien $E$.\n\nSoient $\\lambda$ et $\\mu$ deux valeurs propres distinctes de $f$. Soit $x$ un vecteur propre associé à $\\lambda$ ($x \\in E_\\lambda$) et $y$ un vecteur propre associé à $\\mu$ ($y \\in E_\\mu$).\n\nOn a donc $f(x) = \\lambda x$ et $f(y) = \\mu y$.\n\n**Étape 1 : Premier calcul de $\\langle f(x), y \\rangle$**\n\nEn utilisant $f(x)=\\lambda x$, on a :\n\n$$ \\langle f(x), y \\rangle = \\langle \\lambda x, y \\rangle = \\lambda \\langle x, y \\rangle $$\n\n(Note : dans le cas hermitien, le scalaire sort tel quel de la première composante).\n\n**Étape 2 : Second calcul de $\\langle f(x), y \\rangle$**\n\nEn utilisant la propriété d'auto-adjontion de $f$, on peut écrire :\n\n$$ \\langle f(x), y \\rangle = \\langle x, f(y) \\rangle $$\n\nMaintenant, utilisons $f(y)=\\mu y$ :\n\n$$ \\langle x, f(y) \\rangle = \\langle x, \\mu y \\rangle $$\n\nDans un espace euclidien, $\\langle x, \\mu y \\rangle = \\mu \\langle x, y \\rangle$.\n\nDans un espace hermitien, $\\langle x, \\mu y \\rangle = \\bar{\\mu} \\langle x, y \\rangle$.\n\nCependant, nous savons que les valeurs propres d'un endomorphisme auto-adjoint sont réelles, donc $\\mu = \\bar{\\mu}$. L'expression est donc $\\mu \\langle x, y \\rangle$ dans les deux cas.\n\nOn a donc :\n\n$$ \\langle f(x), y \\rangle = \\mu \\langle x, y \\rangle $$\n\n**Étape 3 : Comparaison et conclusion**\n\nEn égalant les deux expressions obtenues :\n\n$$ \\lambda \\langle x, y \\rangle = \\mu \\langle x, y \\rangle $$\n\n$$ (\\lambda - \\mu) \\langle x, y \\rangle = 0 $$\n\nPar hypothèse, les valeurs propres $\\lambda$ et $\\mu$ sont distinctes, donc $\\lambda - \\mu \\neq 0$.\n\nPour que le produit soit nul, il faut nécessairement que $\\langle x, y \\rangle = 0$.\n\n**Conclusion**\n\nTout vecteur $x \\in E_\\lambda$ est orthogonal à tout vecteur $y \\in E_\\mu$ si $\\lambda \\neq \\mu$. Les sous-espaces propres $E_\\lambda$ et $E_\\mu$ sont donc orthogonaux.\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "cf85e459",
      "content": "#### Stabilité de l'orthogonal pour un endomorphisme auto-adjoint\n\nProuver que si $F$ est un sous-espace vectoriel stable par un endomorphisme auto-adjoint $f$, alors son complément orthogonal $F^\\perp$ est également stable par $f$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoit $f$ auto-adjoint et $F$ stable par $f$, c'est-à-dire $f(F) \\subseteq F$.\n\nPour montrer que $F^\\perp$ est stable par $f$, on doit prouver que pour tout $y \\in F^\\perp$, on a $f(y) \\in F^\\perp$.\n\nLa condition $f(y) \\in F^\\perp$ signifie que pour tout $x \\in F$, le produit scalaire $\\langle f(y), x \\rangle$ est nul.\n\nUtilisez la propriété d'auto-adjontion pour transformer cette expression.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme auto-adjoint sur un espace euclidien ou hermitien $E$.\n\nSoit $F$ un sous-espace vectoriel de $E$ stable par $f$, ce qui signifie que pour tout $z \\in F$, $f(z) \\in F$.\n\nNous voulons montrer que $F^\\perp$ est aussi stable par $f$, c'est-à-dire que pour tout $y \\in F^\\perp$, on a $f(y) \\in F^\\perp$.\n\n**Étape 1 : Traduction de la condition de stabilité**\n\nUn vecteur $v$ appartient à $F^\\perp$ si et seulement si il est orthogonal à tous les vecteurs de $F$, c'est-à-dire $\\langle v, x \\rangle = 0$ pour tout $x \\in F$.\n\nPour montrer que $f(y) \\in F^\\perp$, nous devons donc montrer que pour un $y \\in F^\\perp$ quelconque, on a $\\langle f(y), x \\rangle = 0$ pour tout $x \\in F$.\n\n**Étape 2 : Utilisation de l'auto-adjontion**\n\nSoit $y \\in F^\\perp$ et $x \\in F$. Calculons $\\langle f(y), x \\rangle$.\n\nPuisque $f$ est auto-adjoint, on a :\n\n$$ \\langle f(y), x \\rangle = \\langle y, f(x) \\rangle $$\n\n**Étape 3 : Utilisation de la stabilité de F**\n\nPar hypothèse, $F$ est stable par $f$. Puisque $x \\in F$, on a $f(x) \\in F$.\n\nNous savons aussi que $y \\in F^\\perp$. Par définition de $F^\\perp$, $y$ est orthogonal à tout vecteur de $F$. En particulier, $y$ est orthogonal au vecteur $f(x) \\in F$.\n\nDonc :\n\n$$ \\langle y, f(x) \\rangle = 0 $$\n\n**Conclusion**\n\nEn combinant les étapes, nous avons montré que pour tout $y \\in F^\\perp$ et pour tout $x \\in F$ :\n\n$$ \\langle f(y), x \\rangle = 0 $$\n\nCeci est la définition de $f(y) \\in F^\\perp$.\n\nPuisque cela est vrai pour tout $y \\in F^\\perp$, le sous-espace $F^\\perp$ est stable par $f$.\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "cf85e459",
      "content": "#### Caractérisation des endomorphismes positifs par leurs valeurs propres\n\nProuver qu'un endomorphisme auto-adjoint $f$ est positif si et seulement si toutes ses valeurs propres sont positives ou nulles.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour le sens direct ($\\Rightarrow$), supposez que $f$ est positif. Soit $\\lambda$ une valeur propre et $v$ un vecteur propre associé. Calculez $\\langle f(v), v \\rangle$ et utilisez la définition d'un endomorphisme positif pour en déduire le signe de $\\lambda$.\n\nPour le sens réciproque ($\\Leftarrow$), supposez que toutes les valeurs propres sont $\\ge 0$. Utilisez le théorème spectral pour affirmer qu'il existe une base orthonormée de vecteurs propres. Décomposez un vecteur quelconque $x$ dans cette base et calculez $\\langle f(x), x \\rangle$.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme auto-adjoint sur un espace euclidien ou hermitien $E$.\n\n**Étape 1 : Sens direct ($f$ positif $\\Rightarrow$ les valeurs propres sont $\\ge 0$)**\n\nSupposons que $f$ est positif. Par définition, cela signifie que $\\langle f(x), x \\rangle \\ge 0$ pour tout $x \\in E$.\n\nSoit $\\lambda$ une valeur propre de $f$ et $v$ un vecteur propre associé non nul. On a $f(v) = \\lambda v$.\n\nCalculons $\\langle f(v), v \\rangle$ :\n\n$$ \\langle f(v), v \\rangle = \\langle \\lambda v, v \\rangle = \\lambda \\langle v, v \\rangle = \\lambda \\|v\\|^2 $$\n\nPuisque $f$ est positif, on doit avoir $\\langle f(v), v \\rangle \\ge 0$. Donc :\n\n$$ \\lambda \\|v\\|^2 \\ge 0 $$\n\nComme $v$ est un vecteur propre, il est non nul, donc $\\|v\\|^2 > 0$.\n\nOn en conclut que $\\lambda \\ge 0$.\n\n**Étape 2 : Sens réciproque (les valeurs propres sont $\\ge 0 \\Rightarrow f$ positif)**\n\nSupposons que toutes les valeurs propres de $f$ sont positives ou nulles.\n\nPuisque $f$ est auto-adjoint, le théorème spectral s'applique : il existe une base orthonormée $\\mathcal{B} = (e_1, \\dots, e_n)$ de $E$ constituée de vecteurs propres de $f$.\n\nSoient $\\lambda_1, \\dots, \\lambda_n$ les valeurs propres correspondantes (non nécessairement distinctes), avec $\\lambda_i \\ge 0$ pour tout $i$. On a $f(e_i) = \\lambda_i e_i$.\n\nSoit $x$ un vecteur quelconque de $E$. On peut le décomposer dans la base $\\mathcal{B}$ :\n\n$$ x = \\sum_{i=1}^n x_i e_i, \\quad \\text{où } x_i = \\langle x, e_i \\rangle $$\n\nCalculons $f(x)$ :\n\n$$ f(x) = f\\left(\\sum_{i=1}^n x_i e_i\\right) = \\sum_{i=1}^n x_i f(e_i) = \\sum_{i=1}^n x_i (\\lambda_i e_i) = \\sum_{i=1}^n \\lambda_i x_i e_i $$\n\nMaintenant, calculons le produit scalaire $\\langle f(x), x \\rangle$ :\n\n$$ \\langle f(x), x \\rangle = \\left\\langle \\sum_{i=1}^n \\lambda_i x_i e_i, \\sum_{j=1}^n x_j e_j \\right\\rangle $$\n\nPar bilinéarité (ou sesquilinéarité), et comme la base est orthonormée ($\\langle e_i, e_j \\rangle = \\delta_{ij}$), on obtient :\n\n$$ \\langle f(x), x \\rangle = \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_i x_i \\bar{x_j} \\langle e_i, e_j \\rangle = \\sum_{i=1}^n \\lambda_i x_i \\bar{x_i} = \\sum_{i=1}^n \\lambda_i |x_i|^2 $$\n\n(Dans le cas réel, $\\bar{x_j}=x_j$ et $|x_i|^2=x_i^2$).\n\nPar hypothèse, chaque $\\lambda_i \\ge 0$. De plus, $|x_i|^2 \\ge 0$ pour tout $i$.\n\nLa somme de termes positifs ou nuls est elle-même positive ou nulle.\n\n$$ \\langle f(x), x \\rangle = \\sum_{i=1}^n \\underbrace{\\lambda_i}_{\\ge 0} \\underbrace{|x_i|^2}_{\\ge 0} \\ge 0 $$\n\n**Conclusion**\n\nNous avons prouvé les deux implications. Un endomorphisme auto-adjoint est positif si et seulement si toutes ses valeurs propres sont positives ou nulles.\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "cf85e459",
      "content": "#### Construction d'une matrice symétrique définie positive\n\nProuver que pour toute matrice réelle inversible $M \\in GL_n(\\mathbb{R})$, la matrice $S = {}^tMM$ est symétrique et définie positive.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour prouver la symétrie, calculez ${}^tS$ et utilisez les propriétés de la transposition d'un produit.\n\nPour prouver qu'elle est définie positive, vous devez montrer que pour tout vecteur colonne non nul $X \\in \\mathbb{R}^n$, on a ${}^tXSX > 0$. Remplacez $S$ par ${}^tMM$ et réarrangez l'expression pour faire apparaître la norme d'un vecteur. Utilisez ensuite le fait que $M$ est inversible.\n\n</details>",
      "solution": "\n\nSoit $M \\in GL_n(\\mathbb{R})$ une matrice réelle inversible et soit $S = {}^tMM$.\n\n**Étape 1 : Preuve de la symétrie**\n\nCalculons la transposée de $S$ :\n\n$$ {}^tS = {}^t({}^tMM) $$\n\nEn utilisant la propriété ${}^t(AB) = {}^tB {}^tA$, on a :\n\n$$ {}^tS = {}^tM {}^t({}^tM) $$\n\nLa double transposition est l'identité, donc ${}^t({}^tM) = M$.\n\n$$ {}^tS = {}^tMM = S $$\n\nPuisque ${}^tS = S$, la matrice $S$ est symétrique.\n\n**Étape 2 : Preuve du caractère défini positif**\n\nPour montrer que $S$ est définie positive, nous devons montrer que pour tout vecteur colonne non nul $X \\in \\mathbb{R}^n \\setminus \\{0\\}$, la forme quadratique associée est strictement positive : ${}^tXSX > 0$.\n\nSoit $X \\in \\mathbb{R}^n$ avec $X \\neq 0$.\n\n$$ {}^tXSX = {}^tX({}^tMM)X $$\n\nPar associativité du produit matriciel, on peut regrouper les termes :\n\n$$ {}^tXSX = ({}^tX{}^tM)(MX) = {}^t(MX)(MX) $$\n\nSoit $Y = MX$. L'expression devient ${}^tYY$.\n\nLe produit ${}^tYY$ est le produit scalaire canonique de $Y$ avec lui-même, ce qui correspond au carré de sa norme euclidienne :\n\n$$ {}^tYY = \\|Y\\|^2 $$\n\nOn a donc ${}^tXSX = \\|MX\\|^2$.\n\nLa norme d'un vecteur est toujours positive ou nulle. Nous devons montrer qu'elle est strictement positive.\n\n$\\|MX\\|^2 \\ge 0$.\n\nSupposons que $\\|MX\\|^2 = 0$. Cela implique que $MX = 0$.\n\nPuisque la matrice $M$ est inversible, elle définit une application linéaire injective. Son noyau est donc réduit au vecteur nul : $\\ker(M) = \\{0\\}$.\n\nAinsi, $MX=0$ implique que $X=0$.\n\nOr, nous avons pris un vecteur $X$ non nul. Par conséquent, $MX$ ne peut pas être le vecteur nul, et donc $\\|MX\\|^2 > 0$.\n\n**Conclusion**\n\nPour tout $X \\neq 0$, on a ${}^tXSX > 0$. La matrice $S={}^tMM$ est donc symétrique et définie positive.\n\n",
      "options": []
    },
    {
      "id": "11",
      "stackId": "cf85e459",
      "content": "#### Unicité de la décomposition polaire\n\nProuver l'unicité de la décomposition polaire $M=SO$ pour une matrice inversible $M$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSupposez qu'il existe deux décompositions : $M = S_1O_1 = S_2O_2$, où $S_1, S_2$ sont symétriques définies positives et $O_1, O_2$ sont orthogonales.\n\nLe but est de montrer que $S_1=S_2$ et $O_1=O_2$.\n\nCommencez par exprimer $M{}^tM$ en utilisant les deux décompositions. Que pouvez-vous en déduire sur $S_1^2$ et $S_2^2$ ? Utilisez ensuite l'unicité de la racine carrée d'une matrice symétrique définie positive.\n\n</details>",
      "solution": "\n\nSoit $M \\in GL_n(\\mathbb{R})$. Supposons qu'il existe deux décompositions polaires de $M$ :\n\n$$ M = S_1 O_1 \\quad \\text{et} \\quad M = S_2 O_2 $$\n\noù $S_1, S_2$ sont des matrices symétriques définies positives et $O_1, O_2$ sont des matrices orthogonales.\n\n**Étape 1 : Montrer que $S_1 = S_2$**\n\nConsidérons le produit $M{}^tM$. En utilisant la première décomposition :\n\n$$ M{}^tM = (S_1 O_1) {}^t(S_1 O_1) = S_1 O_1 {}^tO_1 {}^tS_1 $$\n\nPuisque $O_1$ est orthogonale, ${}^tO_1 = O_1^{-1}$, donc $O_1 {}^tO_1 = I$.\n\nPuisque $S_1$ est symétrique, ${}^tS_1 = S_1$.\n\nL'expression devient :\n\n$$ M{}^tM = S_1 I S_1 = S_1^2 $$\n\nEn faisant le même calcul avec la seconde décomposition, on obtient :\n\n$$ M{}^tM = S_2^2 $$\n\nOn a donc $S_1^2 = S_2^2 = M{}^tM$.\n\nLa matrice $A = M{}^tM$ est une matrice symétrique définie positive (car $M$ est inversible). Or, une matrice symétrique définie positive admet une unique racine carrée symétrique définie positive.\n\nPuisque $S_1$ et $S_2$ sont toutes deux des racines carrées symétriques définies positives de $A$, elles doivent être égales :\n\n$$ S_1 = S_2 $$\n\n**Étape 2 : Montrer que $O_1 = O_2$**\n\nMaintenant que nous savons que $S_1=S_2$, notons cette matrice commune $S$.\n\nL'hypothèse de départ devient $M = S O_1 = S O_2$.\n\n$$ S O_1 = S O_2 $$\n\nPuisque $S$ est une matrice symétrique définie positive, toutes ses valeurs propres sont strictement positives. Son déterminant (le produit de ses valeurs propres) est donc non nul, ce qui signifie que $S$ est inversible.\n\nOn peut donc multiplier à gauche par $S^{-1}$ :\n\n$$ S^{-1}(S O_1) = S^{-1}(S O_2) $$\n\n$$ (S^{-1}S) O_1 = (S^{-1}S) O_2 $$\n\n$$ I O_1 = I O_2 $$\n\n$$ O_1 = O_2 $$\n\n**Conclusion**\n\nNous avons montré que si deux décompositions existent, leurs composantes respectives doivent être identiques. La décomposition polaire $M=SO$ d'une matrice inversible est donc unique.\n\n",
      "options": []
    }
  ]
}