{
  "info": {
    "id": "e86f3c89",
    "title": "Algèbre bilinéaire - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Algèbre bilinéaire",
    "course": "Algèbre",
    "tags": [
      "Algèbre bilinéaire",
      "Dualité",
      "Formes bilinéaires",
      "Formes quadratiques",
      "Espaces euclidiens",
      "Diagonalisation",
      "Gram-Schmidt"
    ],
    "count": 13
  },
  "cards": [
    {
      "id": "1",
      "stackId": "e86f3c89",
      "content": "#### Formule de Polarisation\n\nProuver que pour toute forme quadratique $q$ associée à une forme bilinéaire symétrique $\\varphi$ sur un espace vectoriel $V$ (où le corps $K$ est de caractéristique différente de 2), on peut retrouver $\\varphi$ par la formule :\n\n$$\\varphi(u, v) = \\frac{1}{2} (q(u + v) - q(u) - q(v))$$\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nRappelez-vous la définition d'une forme quadratique : $q(x) = \\varphi(x, x)$.\n\nDéveloppez l'expression $q(u + v) = \\varphi(u + v, u + v)$ en utilisant la bilinéarité (linéarité par rapport aux deux variables) et la symétrie de $\\varphi$.\n\n</details>",
      "solution": "\n\nSoit $\\varphi$ une forme bilinéaire symétrique et $q$ la forme quadratique associée, définie par $q(x) = \\varphi(x, x)$.\n\n**Étape 1 : Développement de $q(u+v)$**\n\nCalculons $q(u + v)$ en utilisant la définition :\n\n$$q(u + v) = \\varphi(u + v, u + v)$$\n\nPar la propriété de bilinéarité (distributivité), nous développons l'expression :\n\n$$\\varphi(u + v, u + v) = \\varphi(u, u) + \\varphi(u, v) + \\varphi(v, u) + \\varphi(v, v)$$\n\n**Étape 2 : Utilisation de la symétrie**\n\nComme $\\varphi$ est symétrique, on a $\\varphi(v, u) = \\varphi(u, v)$. De plus, par définition, $\\varphi(u, u) = q(u)$ et $\\varphi(v, v) = q(v)$.\n\nL'expression devient :\n\n$$q(u + v) = q(u) + 2\\varphi(u, v) + q(v)$$\n\n**Conclusion :**\n\nEn isolant le terme $\\varphi(u, v)$, on obtient :\n\n$$2\\varphi(u, v) = q(u + v) - q(u) - q(v)$$\n\n$$\\varphi(u, v) = \\frac{1}{2} (q(u + v) - q(u) - q(v))$$\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "e86f3c89",
      "content": "#### Changement de base pour une forme bilinéaire\n\nSoit $\\varphi$ une forme bilinéaire sur $V$. Soient $\\mathfrak{B}$ et $\\mathcal{C}$ deux bases de $V$, et $P$ la matrice de passage de $\\mathfrak{B}$ à $\\mathcal{C}$.\n\nProuver que si $A$ est la matrice de $\\varphi$ dans la base $\\mathfrak{B}$ et $A'$ est la matrice de $\\varphi$ dans la base $\\mathcal{C}$, alors :\n\n$$A' = {}^t P A P$$\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez l'expression matricielle de la forme bilinéaire. Si $X$ et $Y$ sont les vecteurs colonnes des coordonnées dans $\\mathfrak{B}$, et $X', Y'$ ceux dans $\\mathcal{C}$, rappelez-vous la relation de changement de base : $X = PX'$.\n\nSubstituez cette relation dans l'expression $\\varphi(u, v) = {}^t X A Y$.\n\n</details>",
      "solution": "\n\nSoient $u, v$ deux vecteurs de $V$.\n\nNotons $X, Y$ leurs matrices colonnes de coordonnées dans la base $\\mathfrak{B}$, et $X', Y'$ leurs coordonnées dans la base $\\mathcal{C}$.\n\n**Étape 1 : Expression matricielle et changement de coordonnées**\n\nLa valeur de la forme bilinéaire peut être calculée dans la base $\\mathfrak{B}$ par :\n\n$$\\varphi(u, v) = {}^t X A Y$$\n\nLa formule de changement de base pour les vecteurs est $X = P X'$ et $Y = P Y'$.\n\n**Étape 2 : Substitution**\n\nRemplaçons $X$ et $Y$ dans l'expression de $\\varphi$ :\n\n$$\\varphi(u, v) = {}^t (P X') A (P Y')$$\n\nUtilisons la propriété de la transposition $^t(MN) = {}^tN {}^tM$ :\n\n$$\\varphi(u, v) = ({}^t X' {}^t P) A (P Y') = {}^t X' ({}^t P A P) Y'$$\n\n**Conclusion :**\n\nPar définition de la matrice $A'$ dans la base $\\mathcal{C}$, on doit avoir $\\varphi(u, v) = {}^t X' A' Y'$ pour tous vecteurs $u, v$ (donc pour tous $X', Y'$).\n\nEn identifiant les termes, on obtient :\n\n$$A' = {}^t P A P$$\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "e86f3c89",
      "content": "#### Inégalité de Cauchy-Schwarz\n\nSoit $E$ un espace euclidien muni d'un produit scalaire noté $(x | y)$ et de la norme associée $\\|x\\| = \\sqrt{(x | x)}$.\n\nProuver que pour tous $x, y \\in E$ :\n\n$$|(x | y)| \\le \\|x\\| \\cdot \\|y\\|$$\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nConsidérez la fonction réelle $P(t) = \\|x + ty\\|^2$ pour un réel $t$.\n\nCette fonction est toujours positive ou nulle. Développez-la pour obtenir un polynôme du second degré en $t$. Que pouvez-vous dire de son discriminant ?\n\n</details>",
      "solution": "\n\nSoient $x, y \\in E$.\n\n**Étape 1 : Cas trivial**\n\nSi $y = 0$, alors $(x | 0) = 0$ et $\\|x\\| \\cdot \\|0\\| = 0$. L'égalité $0 \\le 0$ est vérifiée. Supposons donc $y \\ne 0$.\n\n**Étape 2 : Étude du polynôme quadratique**\n\nPour tout réel $t \\in \\mathbb{R}$, considérons le vecteur $x + ty$. Par la propriété de positivité du produit scalaire :\n\n$$\\|x + ty\\|^2 = (x + ty | x + ty) \\ge 0$$\n\nDéveloppons cette expression grâce à la bilinéarité et la symétrie :\n\n$$(x | x) + 2t(x | y) + t^2(y | y) \\ge 0$$\n\n$$\\|x\\|^2 + 2t(x | y) + t^2\\|y\\|^2 \\ge 0$$\n\n**Étape 3 : Discriminant**\n\nIl s'agit d'un polynôme du second degré en $t$ de la forme $At^2 + Bt + C$ avec $A = \\|y\\|^2$, $B = 2(x | y)$ et $C = \\|x\\|^2$.\n\nPuisque ce polynôme est toujours positif ou nul pour tout $t \\in \\mathbb{R}$, il ne peut pas avoir deux racines réelles distinctes. Son discriminant $\\Delta$ doit donc être négatif ou nul ($\\Delta \\le 0$).\n\n$$\\Delta = B^2 - 4AC = (2(x | y))^2 - 4\\|y\\|^2 \\|x\\|^2 \\le 0$$\n\n$$4(x | y)^2 - 4\\|x\\|^2 \\|y\\|^2 \\le 0$$\n\n**Conclusion :**\n\nEn divisant par 4 et en réarrangeant :\n\n$$(x | y)^2 \\le \\|x\\|^2 \\|y\\|^2$$\n\nEn prenant la racine carrée (croissante sur $\\mathbb{R}^+$) :\n\n$$|(x | y)| \\le \\|x\\| \\cdot \\|y\\|$$\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "e86f3c89",
      "content": "#### Coordonnées dans une base orthonormée\n\nSoit $(e_1, \\dots, e_n)$ une base orthonormée d'un espace euclidien $E$.\n\nProuver que pour tout vecteur $x \\in E$, ses coordonnées sont données par les produits scalaires avec les vecteurs de la base, c'est-à-dire :\n\n$$x = \\sum_{i=1}^n (x | e_i) e_i$$\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nÉcrivez $x$ comme une combinaison linéaire quelconque $x = \\sum_{j=1}^n \\lambda_j e_j$.\n\nCalculez le produit scalaire $(x | e_i)$ en utilisant cette expression et la propriété d'orthonormalité $(e_j | e_i) = \\delta_{ji}$.\n\n</details>",
      "solution": "\n\nSoit $x \\in E$. Puisque $(e_1, \\dots, e_n)$ est une base, il existe des scalaires uniques $\\lambda_1, \\dots, \\lambda_n$ tels que :\n\n$$x = \\sum_{j=1}^n \\lambda_j e_j$$\n\n**Étape 1 : Calcul du produit scalaire**\n\nFixons un indice $i \\in \\{1, \\dots, n\\}$ et calculons le produit scalaire $(x | e_i)$ :\n\n$$(x | e_i) = \\left( \\sum_{j=1}^n \\lambda_j e_j \\;\\Bigg|\\; e_i \\right)$$\n\n**Étape 2 : Utilisation de la linéarité**\n\nPar linéarité à gauche du produit scalaire :\n\n$$(x | e_i) = \\sum_{j=1}^n \\lambda_j (e_j | e_i)$$\n\n**Étape 3 : Utilisation de l'orthonormalité**\n\nLa famille est orthonormée, donc $(e_j | e_i) = \\delta_{ji}$ (vaut 1 si $j=i$, 0 sinon).\n\nDans la somme, tous les termes sont nuls sauf celui où $j=i$ :\n\n$$(x | e_i) = \\lambda_i (e_i | e_i) = \\lambda_i \\cdot 1 = \\lambda_i$$\n\n**Conclusion :**\n\nNous avons montré que le coefficient $\\lambda_i$ est exactement $(x | e_i)$. Ainsi :\n\n$$x = \\sum_{i=1}^n (x | e_i) e_i$$\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "e86f3c89",
      "content": "#### Orthogonalité des sous-espaces propres (Matrices symétriques)\n\nSoit $A$ une matrice symétrique réelle (${}^tA = A$) représentant un endomorphisme $u$ dans une base orthonormée.\n\nProuver que si $v_1$ et $v_2$ sont deux vecteurs propres de $A$ associés à des valeurs propres distinctes $\\lambda_1$ et $\\lambda_2$, alors $v_1$ et $v_2$ sont orthogonaux.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nConsidérez le produit scalaire $(Av_1 | v_2)$ (qui matriciellement est ${}^t(Av_1)v_2$).\n\nCalculez ce produit de deux façons :\n\n1. En utilisant $Av_1 = \\lambda_1 v_1$.\n2. En utilisant la propriété de symétrie $(Av_1 | v_2) = (v_1 | Av_2)$ et $Av_2 = \\lambda_2 v_2$.\n\nComparez les résultats.\n\n</details>",
      "solution": "\n\nSoient $v_1, v_2$ tels que $Av_1 = \\lambda_1 v_1$ et $Av_2 = \\lambda_2 v_2$ avec $\\lambda_1 \\ne \\lambda_2$.\n\nDans $\\mathbb{R}^n$, le produit scalaire canonique est $(x|y) = {}^tx y$.\n\n**Étape 1 : Calcul du produit scalaire $(Av_1 | v_2)$**\n\nEn remplaçant $Av_1$ :\n\n$$(Av_1 | v_2) = (\\lambda_1 v_1 | v_2) = \\lambda_1 (v_1 | v_2)$$\n\n**Étape 2 : Utilisation de la symétrie de la matrice**\n\nPour une matrice symétrique réelle $A$, on a l'identité $(Ax | y) = (x | Ay)$ pour tout $x, y$.\n\nEn effet : ${}^t(Ax)y = {}^tx {}^tA y = {}^tx A y = (x | Ay)$.\n\nDonc :\n\n$$(Av_1 | v_2) = (v_1 | Av_2)$$\n\nEn remplaçant $Av_2$ :\n\n$$(v_1 | Av_2) = (v_1 | \\lambda_2 v_2) = \\lambda_2 (v_1 | v_2)$$\n\n**Étape 3 : Comparaison**\n\nNous avons obtenu :\n\n$$\\lambda_1 (v_1 | v_2) = \\lambda_2 (v_1 | v_2)$$\n\nCe qui équivaut à :\n\n$$(\\lambda_1 - \\lambda_2) (v_1 | v_2) = 0$$\n\n**Conclusion :**\n\nPuisque $\\lambda_1 \\ne \\lambda_2$, le terme $(\\lambda_1 - \\lambda_2)$ n'est pas nul. On peut diviser par ce terme, ce qui impose :\n\n$$(v_1 | v_2) = 0$$\n\nLes vecteurs propres sont donc orthogonaux.\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "e86f3c89",
      "content": "#### Minimisation de la distance (Projection Orthogonale)\n\nSoit $F$ un sous-espace vectoriel d'un espace euclidien $E$. Soit $x \\in E$ et $\\pi_F(x)$ sa projection orthogonale sur $F$.\n\nProuver que $\\pi_F(x)$ est la meilleure approximation de $x$ dans $F$, c'est-à-dire :\n\n$$\\forall y \\in F, \\quad \\|x - \\pi_F(x)\\| \\le \\|x - y\\|$$\n\net que l'égalité n'a lieu que si $y = \\pi_F(x)$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nÉcrivez le vecteur différence $x - y$ comme $(x - \\pi_F(x)) + (\\pi_F(x) - y)$.\n\nRemarquez que le premier terme est dans $F^\\perp$ et le second est dans $F$.\n\nAppliquez le théorème de Pythagore.\n\n</details>",
      "solution": "\n\nSoit $y$ un vecteur quelconque de $F$.\n\n**Étape 1 : Décomposition du vecteur**\n\nIntroduisons $\\pi_F(x)$ dans la norme que nous voulons minimiser :\n\n$$x - y = (x - \\pi_F(x)) + (\\pi_F(x) - y)$$\n\nPosons $u = x - \\pi_F(x)$ et $v = \\pi_F(x) - y$.\n\n**Étape 2 : Vérification de l'orthogonalité**\n\n*   Le vecteur $u = x - \\pi_F(x)$ appartient à $F^\\perp$ par définition de la projection orthogonale.\n*   Le vecteur $v = \\pi_F(x) - y$ appartient à $F$ car $\\pi_F(x) \\in F$ et $y \\in F$, et $F$ est un sous-espace vectoriel.\n\nDonc $u$ et $v$ sont orthogonaux : $(u | v) = 0$.\n\n**Étape 3 : Théorème de Pythagore**\n\nPuisque $u \\perp v$, on a :\n\n$$\\|x - y\\|^2 = \\|u + v\\|^2 = \\|u\\|^2 + \\|v\\|^2$$\n\n$$\\|x - y\\|^2 = \\|x - \\pi_F(x)\\|^2 + \\|\\pi_F(x) - y\\|^2$$\n\n**Conclusion :**\n\nComme $\\|\\pi_F(x) - y\\|^2 \\ge 0$, on obtient directement :\n\n$$\\|x - y\\|^2 \\ge \\|x - \\pi_F(x)\\|^2$$\n\nDonc $\\|x - y\\| \\ge \\|x - \\pi_F(x)\\|$.\n\nL'égalité a lieu si et seulement si le terme positif ajouté est nul, c'est-à-dire $\\|\\pi_F(x) - y\\|^2 = 0$, soit $y = \\pi_F(x)$.\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "e86f3c89",
      "content": "#### Caractérisation des isométries et produit scalaire\n\nSoit $u$ un endomorphisme d'un espace euclidien $E$.\n\nProuver que $u$ conserve la norme ($\\forall x, \\|u(x)\\| = \\|x\\|$) si et seulement si $u$ conserve le produit scalaire ($\\forall x, y, (u(x) | u(y)) = (x | y)$).\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\n*   Pour le sens \"conserve le produit scalaire $\\implies$ conserve la norme\", c'est immédiat par définition de la norme.\n*   Pour le sens inverse, utilisez l'identité de polarisation qui exprime le produit scalaire en fonction de la norme : $(x|y) = \\frac{1}{2}(\\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2)$.\n\n</details>",
      "solution": "\n\n**Sens direct ($\\Longleftarrow$) :**\n\nSupposons que $u$ conserve le produit scalaire : $\\forall x, y \\in E, (u(x) | u(y)) = (x | y)$.\n\nEn prenant $y = x$, on obtient :\n\n$$(u(x) | u(x)) = (x | x) \\implies \\|u(x)\\|^2 = \\|x\\|^2$$\n\nComme la norme est positive, $\\|u(x)\\| = \\|x\\|$.\n\n**Sens réciproque ($\\Longrightarrow$) :**\n\nSupposons que $u$ conserve la norme : $\\forall v \\in E, \\|u(v)\\| = \\|v\\|$.\n\nUtilisons l'identité de polarisation pour le produit scalaire réel :\n\n$$(x | y) = \\frac{1}{2} \\left( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\right)$$\n\nCalculons $(u(x) | u(y))$ :\n\n$$(u(x) | u(y)) = \\frac{1}{2} \\left( \\|u(x)+u(y)\\|^2 - \\|u(x)\\|^2 - \\|u(y)\\|^2 \\right)$$\n\nPar linéarité de $u$, $u(x)+u(y) = u(x+y)$, donc :\n\n$$(u(x) | u(y)) = \\frac{1}{2} \\left( \\|u(x+y)\\|^2 - \\|u(x)\\|^2 - \\|u(y)\\|^2 \\right)$$\n\nPar l'hypothèse de conservation de la norme, $\\|u(v)\\| = \\|v\\|$ pour tout $v$, donc :\n\n$$(u(x) | u(y)) = \\frac{1}{2} \\left( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\right)$$\n\nLe membre de droite est exactement la définition de $(x | y)$.\n\nDonc $(u(x) | u(y)) = (x | y)$.\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "e86f3c89",
      "content": "#### Caractérisation des matrices orthogonales\n\nProuver qu'une matrice carrée réelle $M$ est orthogonale (c'est-à-dire que ses colonnes forment une base orthonormée pour le produit scalaire canonique) si et seulement si :\n\n$${}^t M M = I_n$$\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nNotez $C_1, \\dots, C_n$ les colonnes de $M$.\n\nExprimez l'élément $(i, j)$ de la matrice produit ${}^t M M$ en fonction des colonnes de $M$. Rappelez-vous que le produit scalaire canonique de deux vecteurs colonnes $U$ et $V$ est ${}^t U V$.\n\n</details>",
      "solution": "\n\nSoit $M \\in M_n(\\mathbb{R})$. Notons $C_1, \\dots, C_n$ les vecteurs colonnes de $M$.\n\nCalculons le produit $P = {}^t M M$.\n\n**Étape 1 : Expression des coefficients du produit**\n\nL'élément à la ligne $i$ et colonne $j$ de la matrice $P$, noté $P_{ij}$, est le produit de la ligne $i$ de ${}^t M$ par la colonne $j$ de $M$.\n\nLa ligne $i$ de ${}^t M$ est la transposée de la colonne $i$ de $M$, soit ${}^t C_i$.\n\nDonc :\n\n$$P_{ij} = {}^t C_i C_j$$\n\nOr, pour le produit scalaire canonique sur $\\mathbb{R}^n$, on a $(C_i | C_j) = {}^t C_i C_j$.\n\nAinsi, $P_{ij} = (C_i | C_j)$.\n\n**Étape 2 : Condition d'orthonormalité**\n\nLa famille des colonnes $(C_1, \\dots, C_n)$ est une base orthonormée si et seulement si :\n\n$$(C_i | C_j) = \\delta_{ij} = \\begin{cases} 1 & \\text{si } i=j \\\\ 0 & \\text{si } i \\ne j \\end{cases}$$\n\n**Conclusion :**\n\nCette condition est équivalente à dire que $P_{ij} = \\delta_{ij}$ pour tout $i, j$, ce qui signifie exactement que la matrice $P$ est la matrice identité $I_n$.\n\nD'où l'équivalence :\n\n$$(C_1, \\dots, C_n) \\text{ est orthonormée} \\iff {}^t M M = I_n$$\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "e86f3c89",
      "content": "#### Dimension de l'orthogonal\n\nSoit $F$ un sous-espace vectoriel d'un espace euclidien $E$ de dimension finie $n$.\n\nProuver que :\n\n$$\\dim(F^\\perp) = n - \\dim(F)$$\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nConsidérez une base orthonormée $(e_1, \\dots, e_p)$ de $F$ (que l'on peut obtenir par Gram-Schmidt).\n\nComplétez cette base en une base orthonormée $(e_1, \\dots, e_n)$ de $E$ entier.\n\nMontrez que $(e_{p+1}, \\dots, e_n)$ est une base de $F^\\perp$.\n\n</details>",
      "solution": "\n\nSoit $p = \\dim(F)$.\n\n**Étape 1 : Base adaptée**\n\nIl existe une base orthonormée de $F$, notons-la $(e_1, \\dots, e_p)$.\n\nD'après le théorème de la base incomplète (version orthonormée), on peut compléter cette famille en une base orthonormée $\\mathfrak{B} = (e_1, \\dots, e_p, e_{p+1}, \\dots, e_n)$ de l'espace entier $E$.\n\n**Étape 2 : Identification de $F^\\perp$**\n\nMontrons que $G = \\text{Vect}(e_{p+1}, \\dots, e_n)$ est égal à $F^\\perp$.\n\n*   **Inclusion $G \\subset F^\\perp$ :**\n\n    Soit $v \\in G$. $v$ est combinaison linéaire de $e_{p+1}, \\dots, e_n$.\n\n    Pour tout $u \\in F$, $u$ est combinaison linéaire de $e_1, \\dots, e_p$.\n\n    Comme la base $\\mathfrak{B}$ est orthonormée, tout vecteur de $\\{e_{p+1}, \\dots, e_n\\}$ est orthogonal à tout vecteur de $\\{e_1, \\dots, e_p\\}$. Par bilinéarité, $v \\perp u$. Donc $v \\in F^\\perp$.\n\n*   **Dimension :**\n\n    La famille $(e_{p+1}, \\dots, e_n)$ est libre (sous-famille d'une base) et engendre $G$. Donc $\\dim(G) = n - p$.\n\n**Étape 3 : Argument de somme directe**\n\nOn sait que $F \\cap F^\\perp = \\{0\\}$ (si un vecteur est dans $F$ et orthogonal à $F$, il est orthogonal à lui-même, donc nul par définition du produit scalaire défini positif).\n\nDonc $\\dim(F \\oplus F^\\perp) = \\dim(F) + \\dim(F^\\perp) \\le n$.\n\nComme $G \\subset F^\\perp$, on a $\\dim(F^\\perp) \\ge n - p$.\n\nEn réalité, dans un espace euclidien, $E = F \\oplus F^\\perp$.\n\nTout $x$ s'écrit $x = \\sum_{i=1}^n (x|e_i)e_i = \\underbrace{\\sum_{i=1}^p (x|e_i)e_i}_{\\in F} + \\underbrace{\\sum_{i=p+1}^n (x|e_i)e_i}_{\\in G}$.\n\nComme $x$ quelconque se décompose, $F^\\perp$ est exactement $G$.\n\n**Conclusion :**\n\n$$\\dim(F^\\perp) = \\dim(G) = n - p = n - \\dim(F)$$\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "e86f3c89",
      "content": "#### Base Duale\n\nSoit $V$ un espace vectoriel de dimension $n$ et $\\mathfrak{B} = (e_1, \\dots, e_n)$ une base de $V$.\n\nProuver qu'il existe une unique famille de formes linéaires $(\\varphi_1, \\dots, \\varphi_n)$ telle que $\\varphi_i(e_j) = \\delta_{i,j}$, et que cette famille forme une base de $V^*$ (l'espace dual).\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour l'existence et l'unicité, rappelez-vous qu'une application linéaire est entièrement déterminée par ses valeurs sur une base.\n\nPour montrer que c'est une base, prouvez que la famille est libre, puis utilisez un argument de dimension ($\\dim V^* = \\dim V = n$).\n\n</details>",
      "solution": "\n\n**Étape 1 : Existence et Unicité des $\\varphi_i$**\n\nUne forme linéaire est une application linéaire de $V$ dans $K$. Une application linéaire est définie de manière unique par les images des vecteurs d'une base.\n\nPour chaque $i$ fixé, définissons $\\varphi_i$ par ses valeurs sur la base $\\mathfrak{B}$ :\n\n$$\\varphi_i(e_j) = \\delta_{i,j} = \\begin{cases} 1 & \\text{si } i=j \\\\ 0 & \\text{si } i \\ne j \\end{cases}$$\n\nCette définition assure l'existence et l'unicité de chaque $\\varphi_i$.\n\n**Étape 2 : Liberté de la famille**\n\nSoit $\\lambda_1, \\dots, \\lambda_n$ des scalaires tels que la combinaison linéaire soit nulle :\n\n$$\\sum_{i=1}^n \\lambda_i \\varphi_i = 0_{V^*}$$\n\nCela signifie que pour tout vecteur $v \\in V$, $\\sum \\lambda_i \\varphi_i(v) = 0$.\n\nAppliquons cette égalité au vecteur de base $e_k$ :\n\n$$\\sum_{i=1}^n \\lambda_i \\varphi_i(e_k) = 0$$\n\nPar définition, $\\varphi_i(e_k) = \\delta_{i,k}$. Seul le terme $i=k$ survit :\n\n$$\\lambda_k \\cdot 1 = 0 \\implies \\lambda_k = 0$$\n\nCeci étant vrai pour tout $k \\in \\{1, \\dots, n\\}$, la famille est libre.\n\n**Étape 3 : Conclusion (Base)**\n\nLa famille $(\\varphi_1, \\dots, \\varphi_n)$ est une famille libre de $n$ vecteurs dans $V^*$.\n\nComme $\\dim(V^*) = \\dim(V) = n$, toute famille libre de $n$ éléments est une base.\n\nCette base est appelée la **base duale** de $\\mathfrak{B}$.\n\n",
      "options": []
    },
    {
      "id": "11",
      "stackId": "e86f3c89",
      "content": "#### Matrice définie positive $\\implies$ Inversible\n\nProuver qu'une matrice définie positive est inversible.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\n$A$ est définie positive si pour tout vecteur $x \\neq 0$, le produit scalaire $\\langle Ax, x \\rangle > 0$.\n\nPour montrer que $A$ est inversible, il suffit de montrer que son noyau est réduit à $\\{0\\}$.\n\nPrenez un vecteur $x \\in \\text{Ker}(A)$ et calculez $\\langle Ax, x \\rangle$.\n\n</details>",
      "solution": "\n\nNous allons prouver que si une matrice symétrique $A$ est définie positive, alors son noyau est réduit au vecteur nul.\n\n**Preuve par le Noyau (Algébrique)**\n\nRappelons la définition : $A$ est définie positive si pour tout vecteur $x \\neq 0$, le produit scalaire $\\langle Ax, x \\rangle > 0$.\n\nSupposons que $x$ soit un vecteur du noyau de $A$, c'est-à-dire $Ax = 0$.\n\nCalculons la quantité $\\langle Ax, x \\rangle$.\nPuisque $Ax = 0$, alors $\\langle 0, x \\rangle = 0$.\n\nOr, l'hypothèse \"$A$ est définie positive\" impose que si $x \\neq 0$, alors $\\langle Ax, x \\rangle$ doit être strictement supérieur à 0.\n\nLe seul moyen d'avoir $\\langle Ax, x \\rangle = 0$ sans contredire la définition est que $x$ soit le vecteur nul.\n\n**Conclusion :**\n$\\text{Ker}(A) = \\{0\\}$, donc $A$ est inversible.\n\n*(Autre approche possible : via le théorème spectral, toutes les valeurs propres sont strictement positives, donc leur produit - le déterminant - est non nul)*.\n\n",
      "options": []
    },
    {
      "id": "12",
      "stackId": "e86f3c89",
      "content": "#### Théorème Spectral (Matrices symétriques réelles)\n\nProuver que tout endomorphisme symétrique (ou matrice symétrique réelle) est diagonalisable dans une base orthonormée.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nProcédez par récurrence sur la dimension de l'espace $n$.\n\nUtilisez deux résultats intermédiaires (à admettre ou à prouver rapidement) :\n1. Les valeurs propres d'une matrice symétrique réelle sont réelles.\n2. Si un sous-espace est stable par un endomorphisme symétrique, alors son orthogonal l'est aussi.\n\n</details>",
      "solution": "\n\nNous allons prouver la propriété $P(n)$ : \"Toute matrice symétrique de taille $n$ est diagonalisable dans une base orthonormée\" par récurrence sur $n$.\n\n**Préliminaires**\n\n*   **Lemme 1 :** Les valeurs propres d'une matrice symétrique réelle sont réelles.\n    *Preuve :* Soit $AX = \\lambda X$. On a ${}^t\\bar{X} A X = \\lambda \\|X\\|^2$. Par symétrie et réalité de $A$, ${}^t\\bar{X} A X = {}^t(A\\bar{X}) X = \\bar{\\lambda} \\|X\\|^2$. Donc $\\lambda = \\bar{\\lambda}$.\n*   **Lemme 2 :** Si un sous-espace $F$ est stable par $A$, alors $F^\\perp$ l'est aussi.\n    *Preuve :* Soit $y \\in F^\\perp$. Pour tout $x \\in F$, $\\langle Ay, x \\rangle = \\langle y, Ax \\rangle = 0$ car $Ax \\in F$. Donc $Ay \\in F^\\perp$.\n\n**Initialisation ($n=1$)**\n\nUne matrice $1 \\times 1$ est déjà diagonale. La propriété est vraie.\n\n**Hérédité**\n\nSupposons que $P(n-1)$ soit vraie. Soit $A$ une matrice symétrique de taille $n$.\n\n1.  **Trouver un premier vecteur :** D'après le Lemme 1, le polynôme caractéristique de $A$ a des racines réelles. Il existe au moins une valeur propre réelle $\\lambda_1$ et un vecteur propre associé $e_1$ que l'on normalise ($\\|e_1\\|=1$).\n2.  **Créer les espaces :** Soit $D = \\text{Vect}(e_1)$. $D$ est stable par $A$.\n3.  **Utiliser l'orthogonal :** D'après le Lemme 2, l'hyperplan $H = D^\\perp$ (de dimension $n-1$) est aussi stable par $A$.\n4.  **Restriction :** On considère la restriction $A_H$ de $A$ à $H$. C'est un endomorphisme symétrique de $H$.\n5.  **Hypothèse de récurrence :** On applique $P(n-1)$ à $A_H$. Il existe une base orthonormée $(e_2, \\dots, e_n)$ de $H$ constituée de vecteurs propres de $A$.\n\n**Conclusion**\n\nLa famille $\\mathcal{B} = (e_1, e_2, \\dots, e_n)$ est constituée de vecteurs propres. Elle est orthonormée car $e_1 \\perp H$ et $(e_2, \\dots, e_n)$ est orthonormée.\n$A$ est donc diagonalisable dans une base orthonormée.\n\n",
      "options": []
    },
    {
      "id": "13",
      "stackId": "e86f3c89",
      "content": "#### Racine carrée d'une matrice symétrique définie positive\n\nProuver qu'il existe une unique matrice symétrique définie positive $R$ telle que $R^2 = A$, pour toute matrice symétrique définie positive $A$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour l'existence, diagonalisez $A$ grâce au théorème spectral ($A = P D P^{-1}$) et construisez $R$ en prenant la racine carrée des éléments diagonaux.\n\nPour l'unicité, montrez que si $R$ est une racine carrée symétrique définie positive, elle commute avec $A$ et donc stabilise les sous-espaces propres de $A$, sur lesquels elle agit comme une homothétie positive.\n\n</details>",
      "solution": "\n\nSoit $A$ une matrice symétrique réelle définie positive.\n\n**Partie 1 : Existence**\n\nD'après le théorème spectral, il existe une matrice orthogonale $P$ et une matrice diagonale $D = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$ telles que $A = P D P^{-1}$.\nComme $A$ est définie positive, toutes ses valeurs propres $\\lambda_i$ sont strictement positives.\n\nPosons $\\mu_i = \\sqrt{\\lambda_i}$ pour tout $i$. Soit $\\Delta = \\text{diag}(\\mu_1, \\dots, \\mu_n)$.\nDéfinissons la matrice $R = P \\Delta P^{-1}$.\n\n*   **$R^2 = A$ :** $R^2 = (P \\Delta P^{-1})(P \\Delta P^{-1}) = P \\Delta^2 P^{-1} = P D P^{-1} = A$.\n*   **$R$ est symétrique :** ${}^t R = {}^t (P \\Delta {}^t P) = P {}^t \\Delta {}^t P = P \\Delta {}^t P = R$.\n*   **$R$ est définie positive :** Les valeurs propres de $R$ sont les $\\mu_i > 0$.\n\nLa matrice $R$ convient.\n\n**Partie 2 : Unicité**\n\nSoit $M$ une matrice symétrique définie positive telle que $M^2 = A$.\nComme $M$ commute avec $M^2$, $M$ commute avec $A$ ($MA = M^3 = AM$).\nDonc $M$ laisse stable les sous-espaces propres de $A$.\n\nSoit $E_\\lambda(A)$ un sous-espace propre de $A$ associé à la valeur propre $\\lambda$.\nLa restriction de $M$ à $E_\\lambda(A)$ est un endomorphisme symétrique $M_\\lambda$ tel que $(M_\\lambda)^2 = \\lambda \\text{Id}_{E_\\lambda}$.\nPuisque $M$ est définie positive, ses valeurs propres sont positives. Or les valeurs propres de $M_\\lambda$ doivent vérifier $x^2 = \\lambda$, donc $x = \\sqrt{\\lambda}$ (car $x>0$).\nL'unique endomorphisme diagonalisable à valeurs propres positives vérifiant ceci est l'homothétie de rapport $\\sqrt{\\lambda}$.\n\nAinsi, sur chaque sous-espace propre de $A$, $M$ est uniquement déterminé (c'est l'homothétie de rapport $\\sqrt{\\lambda}$). Comme $E$ est la somme directe orthogonale de ces sous-espaces, $M$ est unique sur tout $E$.\n\n**Conclusion :**\nIl existe une unique racine carrée symétrique définie positive, notée $\\sqrt{A}$.\n\n",
      "options": []
    }
  ]
}