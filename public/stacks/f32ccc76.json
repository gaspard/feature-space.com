{
  "info": {
    "id": "f32ccc76",
    "title": "Groupes d'isométries - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Groupes d'isométries",
    "course": "Géométrie",
    "tags": [
      "isométrie",
      "espace euclidien",
      "espace hermitien",
      "groupe orthogonal",
      "groupe unitaire",
      "produit vectoriel",
      "rotation"
    ],
    "count": 10
  },
  "cards": [
    {
      "id": "1",
      "stackId": "f32ccc76",
      "content": "#### Équivalence entre la préservation de la norme et du produit scalaire\n\nProuver que pour un endomorphisme $f$ d'un espace euclidien $E$, les deux affirmations suivantes sont équivalentes :\n\n1. $f$ préserve la norme : $\\forall x \\in E, \\|f(x)\\| = \\|x\\|$.\n2. $f$ préserve le produit scalaire : $\\forall x, y \\in E, \\varphi(f(x), f(y)) = \\varphi(x, y)$.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nPour prouver une équivalence ($P \\iff Q$), il faut prouver les deux implications ($P \\implies Q$ et $Q \\implies P$).\n\nPour l'implication $2 \\implies 1$, il suffit de considérer le cas particulier où $y=x$ dans la définition de la préservation du produit scalaire et d'utiliser le lien entre la norme et le produit scalaire : $\\|v\\|^2 = \\varphi(v,v)$.\n\nPour l'implication $1 \\implies 2$, l'astuce consiste à utiliser une **identité de polarisation** qui exprime le produit scalaire en termes de normes. Pour un espace euclidien (sur $\\mathbb{R}$), l'identité la plus courante est :\n\n$$ \\varphi(x, y) = \\frac{1}{2} \\left( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\right) $$\n\nAppliquez cette identité à $\\varphi(f(x), f(y))$ et utilisez l'hypothèse que $f$ préserve la norme.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme de l'espace euclidien $E$.\n\n**Étape 1 : Preuve de l'implication ($f$ préserve le produit scalaire $\\implies$ $f$ préserve la norme)**\n\nSupposons que $f$ préserve le produit scalaire, c'est-à-dire que pour tous $x, y \\in E$, on a $\\varphi(f(x), f(y)) = \\varphi(x, y)$.\n\nNous voulons montrer que $\\|f(u)\\| = \\|u\\|$ pour tout $u \\in E$.\n\nLa norme d'un vecteur est définie par $\\|u\\| = \\sqrt{\\varphi(u, u)}$. Il est équivalent de montrer que $\\|f(u)\\|^2 = \\|u\\|^2$.\n\nEn utilisant la définition de la norme au carré, on a :\n\n$$ \\|f(u)\\|^2 = \\varphi(f(u), f(u)) $$\n\nPar notre hypothèse (en prenant $x=u$ et $y=u$), nous avons :\n\n$$ \\varphi(f(u), f(u)) = \\varphi(u, u) $$\n\nOr, $\\varphi(u, u) = \\|u\\|^2$.\n\nDonc, $\\|f(u)\\|^2 = \\|u\\|^2$. Comme les normes sont des réels positifs, on peut prendre la racine carrée des deux côtés pour obtenir $\\|f(u)\\| = \\|u\\|$.\n\n**Étape 2 : Preuve de l'implication ($f$ préserve la norme $\\implies$ $f$ préserve le produit scalaire)**\n\nSupposons que $f$ préserve la norme, c'est-à-dire que pour tout $v \\in E$, on a $\\|f(v)\\| = \\|v\\|$.\n\nNous utilisons l'identité de polarisation pour un espace euclidien :\n\n$$ \\varphi(x, y) = \\frac{1}{2} \\left( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\right) $$\n\nAppliquons cette identité à l'expression $\\varphi(f(x), f(y))$ :\n\n$$ \\varphi(f(x), f(y)) = \\frac{1}{2} \\left( \\|f(x)+f(y)\\|^2 - \\|f(x)\\|^2 - \\|f(y)\\|^2 \\right) $$\n\nPuisque $f$ est un endomorphisme (application linéaire), on a $f(x)+f(y) = f(x+y)$. Donc :\n\n$$ \\varphi(f(x), f(y)) = \\frac{1}{2} \\left( \\|f(x+y)\\|^2 - \\|f(x)\\|^2 - \\|f(y)\\|^2 \\right) $$\n\nMaintenant, nous utilisons notre hypothèse que $f$ préserve la norme. Cela signifie que :\n\n- $\\|f(x+y)\\| = \\|x+y\\|$\n- $\\|f(x)\\| = \\|x\\|$\n- $\\|f(y)\\| = \\|y\\|$\n\nEn substituant ces égalités dans l'expression de $\\varphi(f(x), f(y))$, on obtient :\n\n$$ \\varphi(f(x), f(y)) = \\frac{1}{2} \\left( \\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2 \\right) $$\n\nLe membre de droite est exactement la définition de $\\varphi(x, y)$ par l'identité de polarisation.\n\n**Conclusion**\n\nNous avons donc montré que $\\varphi(f(x), f(y)) = \\varphi(x, y)$ pour tous $x, y \\in E$.\n\nLes deux affirmations sont bien équivalentes.\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "f32ccc76",
      "content": "#### Caractérisation des isométries par l'adjoint\n\nProuver que pour un endomorphisme $f$ d'un espace euclidien ou hermitien $E$, $f$ est une isométrie si et seulement si $f^* \\circ f = \\text{Id}_E$, où $f^*$ est l'application adjointe de $f$.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nOn sait qu'un endomorphisme est une isométrie si et seulement si il préserve le produit scalaire. La preuve consistera donc à montrer l'équivalence entre $\\varphi(f(x), f(y)) = \\varphi(x, y)$ et $f^* \\circ f = \\text{Id}_E$.\n\nLa clé est la définition de l'application adjointe $f^*$, qui est l'unique endomorphisme vérifiant :\n\n$$ \\forall x, y \\in E, \\quad \\varphi(f(x), y) = \\varphi(x, f^*(y)) $$\n\nPour l'implication $(\\implies)$, partez de $\\varphi(f(x), f(y)) = \\varphi(x, y)$ et utilisez la définition de l'adjoint pour transformer le membre de gauche. Vous obtiendrez une égalité de la forme $\\varphi(x, v) = \\varphi(x, w)$ pour tout $x$, ce qui implique $v=w$.\n\nPour l'implication $(\\impliedby)$, partez de $f^* \\circ f = \\text{Id}_E$ et calculez $\\varphi(f(x), f(y))$ en utilisant la définition de l'adjoint pour \"déplacer\" le premier $f$ de l'autre côté du produit scalaire.\n\n</details>",
      "solution": "\n\nNous allons prouver l'équivalence : $f$ préserve le produit scalaire $\\iff f^* \\circ f = \\text{Id}_E$.\n\n**Étape 1 : Implication ($\\implies$)**\n\nSupposons que $f$ préserve le produit scalaire, c'est-à-dire $\\forall x, y \\in E, \\varphi(f(x), f(y)) = \\varphi(x, y)$.\n\nPar définition de l'application adjointe $f^*$, nous avons aussi :\n\n$$ \\forall x, y \\in E, \\quad \\varphi(f(x), f(y)) = \\varphi(x, f^*(f(y))) $$\n\nEn combinant ces deux égalités, nous obtenons :\n\n$$ \\forall x, y \\in E, \\quad \\varphi(x, y) = \\varphi(x, f^*(f(y))) $$\n\nOn peut réarranger cette équation :\n\n$$ \\forall x, y \\in E, \\quad \\varphi(x, y) - \\varphi(x, f^*(f(y))) = 0 $$\n\nPar linéarité du produit scalaire par rapport à la deuxième variable :\n\n$$ \\forall x, y \\in E, \\quad \\varphi(x, y - f^*(f(y))) = 0 $$\n\nCette égalité est vraie pour tout $x \\in E$. En particulier, si nous choisissons $x = y - f^*(f(y))$, nous avons :\n\n$$ \\varphi(y - f^*(f(y)), y - f^*(f(y))) = 0 $$\n\nCeci est la norme au carré du vecteur $y - f^*(f(y))$. Or, la norme d'un vecteur est nulle si et seulement si le vecteur lui-même est nul. Donc :\n\n$$ y - f^*(f(y)) = 0 \\quad \\implies \\quad y = f^*(f(y)) $$\n\nCette égalité étant vraie pour tout $y \\in E$, cela signifie que l'application $f^* \\circ f$ est l'application identité $\\text{Id}_E$.\n\n**Étape 2 : Implication ($\\impliedby$)**\n\nSupposons que $f^* \\circ f = \\text{Id}_E$.\n\nNous voulons montrer que $f$ préserve le produit scalaire. Calculons $\\varphi(f(x), f(y))$ pour des vecteurs $x, y$ quelconques.\n\nPar la définition de l'adjoint $f^*$, on peut écrire :\n\n$$ \\varphi(f(x), f(y)) = \\varphi(x, f^*(f(y))) $$\n\nMaintenant, nous utilisons notre hypothèse $f^* \\circ f = \\text{Id}_E$, ce qui signifie $f^*(f(y)) = y$.\n\nEn substituant, on obtient :\n\n$$ \\varphi(f(x), f(y)) = \\varphi(x, y) $$\n\nCeci est vrai pour tous $x, y \\in E$. Donc, $f$ préserve le produit scalaire.\n\n**Conclusion**\n\nLes deux affirmations sont équivalentes. Comme la préservation du produit scalaire est équivalente à la préservation de la norme, nous avons bien prouvé qu'un endomorphisme $f$ est une isométrie si et seulement si $f^* \\circ f = \\text{Id}_E$.\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "f32ccc76",
      "content": "#### Image d'une base orthonormée par une isométrie\n\nProuver qu'un endomorphisme $f$ d'un espace euclidien $E$ de dimension finie $n$ est une isométrie si et seulement si l'image par $f$ d'une base orthonormée est une base orthonormée.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nSoit $\\mathcal{B} = (e_1, \\dots, e_n)$ une base orthonormée (BON) de $E$. Son image par $f$ est la famille de vecteurs $\\mathcal{B}' = (f(e_1), \\dots, f(e_n))$.\n\nPour l'implication $(\\implies)$ : Supposons que $f$ est une isométrie. Elle préserve donc le produit scalaire. Pour montrer que $\\mathcal{B}'$ est une BON, il faut montrer que $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$ (symbole de Kronecker). Utilisez l'hypothèse et le fait que $\\mathcal{B}$ est une BON.\n\nPour l'implication $(\\impliedby)$ : Supposons que $\\mathcal{B}'$ est une BON. On veut montrer que $f$ est une isométrie, c'est-à-dire que $\\|f(x)\\| = \\|x\\|$ pour tout $x \\in E$. Exprimez un vecteur $x$ quelconque dans la base $\\mathcal{B}$ : $x = \\sum_{i=1}^n x_i e_i$. Calculez $\\|x\\|^2$ en utilisant les propriétés d'une BON. Appliquez ensuite $f$ à $x$ et calculez $\\|f(x)\\|^2$ en utilisant le fait que $\\mathcal{B}'$ est une BON. Comparez les deux résultats.\n\n</details>",
      "solution": "\n\nSoit $\\mathcal{B} = (e_1, \\dots, e_n)$ une base orthonormée (BON) de $E$.\n\n**Étape 1 : Implication ($\\implies$)**\n\nSupposons que $f$ est une isométrie. Par équivalence, $f$ préserve le produit scalaire.\n\nMontrons que la famille $\\mathcal{B}' = (f(e_1), \\dots, f(e_n))$ est une base orthonormée. Pour cela, nous devons vérifier la condition d'orthonormalité : $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$.\n\nCalculons ce produit scalaire :\n\n$$ \\varphi(f(e_i), f(e_j)) $$\n\nPuisque $f$ préserve le produit scalaire, nous avons :\n\n$$ \\varphi(f(e_i), f(e_j)) = \\varphi(e_i, e_j) $$\n\nOr, $\\mathcal{B}$ est une base orthonormée, donc par définition, $\\varphi(e_i, e_j) = \\delta_{ij}$.\n\nAinsi, $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$. Ceci montre que la famille $\\mathcal{B}'$ est orthonormée. Une famille orthonormée de $n$ vecteurs dans un espace de dimension $n$ est toujours une base. Donc, $\\mathcal{B}'$ est une base orthonormée.\n\n**Étape 2 : Implication ($\\impliedby$)**\n\nSupposons que pour une base orthonormée $\\mathcal{B}=(e_1, \\dots, e_n)$, son image $\\mathcal{B}' = (f(e_1), \\dots, f(e_n))$ est aussi une base orthonormée.\n\nMontrons que $f$ est une isométrie. Pour cela, nous allons montrer qu'elle préserve la norme pour tout vecteur $x \\in E$.\n\nSoit $x$ un vecteur quelconque de $E$. On peut le décomposer dans la base $\\mathcal{B}$ :\n\n$$ x = \\sum_{i=1}^n x_i e_i $$\n\noù $x_i = \\varphi(x, e_i)$ sont les coordonnées de $x$.\n\nCalculons la norme au carré de $x$. Puisque $\\mathcal{B}$ est une BON :\n\n$$ \\|x\\|^2 = \\varphi\\left(\\sum_i x_i e_i, \\sum_j x_j e_j\\right) = \\sum_{i,j} x_i x_j \\varphi(e_i, e_j) = \\sum_{i,j} x_i x_j \\delta_{ij} = \\sum_{i=1}^n x_i^2 $$\n\nMaintenant, calculons l'image de $x$ par $f$. Par linéarité de $f$ :\n\n$$ f(x) = f\\left(\\sum_{i=1}^n x_i e_i\\right) = \\sum_{i=1}^n x_i f(e_i) $$\n\nCalculons la norme au carré de $f(x)$. Par hypothèse, la base $(f(e_1), \\dots, f(e_n))$ est orthonormée, donc $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$.\n\n$$ \\|f(x)\\|^2 = \\varphi\\left(\\sum_i x_i f(e_i), \\sum_j x_j f(e_j)\\right) = \\sum_{i,j} x_i x_j \\varphi(f(e_i), f(e_j)) = \\sum_{i,j} x_i x_j \\delta_{ij} = \\sum_{i=1}^n x_i^2 $$\n\nEn comparant les deux résultats, nous voyons que $\\|x\\|^2 = \\|f(x)\\|^2$.\n\nDonc, $\\|x\\| = \\|f(x)\\|$ pour tout $x \\in E$. L'application $f$ est bien une isométrie.\n\n**Conclusion**\n\nLes deux affirmations sont équivalentes.\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "f32ccc76",
      "content": "#### Structure de groupe de $O_n(\\mathbb{R})$\n\nProuver que l'ensemble des matrices orthogonales d'ordre $n$, noté $O_n(\\mathbb{R}) = \\{ M \\in M_n(\\mathbb{R}) \\mid {}^tMM = I_n \\}$, forme un groupe pour la multiplication matricielle.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nPour démontrer qu'un ensemble muni d'une loi de composition est un groupe, il faut vérifier trois axiomes :\n\n1.  **Stabilité** (ou loi de composition interne) : Le produit de deux matrices de $O_n(\\mathbb{R})$ est-il encore dans $O_n(\\mathbb{R})$ ? Pour le vérifier, prenez $A, B \\in O_n(\\mathbb{R})$ et calculez ${}^t(AB)(AB)$. Utilisez la propriété ${}^t(AB) = {}^tB {}^tA$.\n2.  **Existence d'un élément neutre** : La matrice identité $I_n$ est-elle dans $O_n(\\mathbb{R})$ ? Vérifiez si ${}^tI_n I_n = I_n$.\n3.  **Existence d'un inverse** : Pour toute matrice $A \\in O_n(\\mathbb{R})$, son inverse $A^{-1}$ est-il aussi dans $O_n(\\mathbb{R})$ ? De la définition ${}^tAA = I_n$, on peut déduire que $A^{-1} = {}^tA$. Il faut donc montrer que si $A$ est orthogonale, alors ${}^tA$ est aussi orthogonale. Pour cela, calculez ${}^t({}^tA)({}^tA)$.\n\nLa multiplication matricielle est associative, il n'est donc pas nécessaire de vérifier cette propriété.\n\n</details>",
      "solution": "\n\nNous devons vérifier les trois axiomes qui définissent un groupe.\n\n**1. Stabilité de la multiplication**\n\nSoient $A$ et $B$ deux matrices appartenant à $O_n(\\mathbb{R})$. Par définition, nous avons ${}^tAA = I_n$ et ${}^tBB = I_n$.\n\nNous devons montrer que le produit $AB$ appartient aussi à $O_n(\\mathbb{R})$, c'est-à-dire que ${}^t(AB)(AB) = I_n$.\n\nCalculons ce produit :\n\n$$ {}^t(AB)(AB) = ({}^tB {}^tA)(AB) $$\n\nEn utilisant l'associativité de la multiplication matricielle :\n\n$$ {}^t(AB)(AB) = {}^tB ({}^tA A) B $$\n\nPuisque $A \\in O_n(\\mathbb{R})$, on a ${}^tAA = I_n$. On remplace :\n\n$$ {}^t(AB)(AB) = {}^tB (I_n) B = {}^tB B $$\n\nEt comme $B \\in O_n(\\mathbb{R})$, on a ${}^tBB = I_n$.\n\nFinalement, ${}^t(AB)(AB) = I_n$, ce qui prouve que $AB \\in O_n(\\mathbb{R})$. La loi est donc interne.\n\n**2. Existence de l'élément neutre**\n\nL'élément neutre pour la multiplication matricielle est la matrice identité $I_n$. Nous devons vérifier si $I_n \\in O_n(\\mathbb{R})$.\n\nOn a ${}^tI_n = I_n$. Calculons ${}^tI_n I_n$ :\n\n$$ {}^tI_n I_n = I_n I_n = I_n $$\n\nLa condition est satisfaite, donc $I_n \\in O_n(\\mathbb{R})$.\n\n**3. Existence de l'inverse**\n\nSoit $A \\in O_n(\\mathbb{R})$. Par définition, ${}^tAA = I_n$. Ceci montre que $A$ est inversible et que son inverse est $A^{-1} = {}^tA$.\n\nNous devons maintenant montrer que cet inverse $A^{-1}$ appartient lui-même à $O_n(\\mathbb{R})$. Autrement dit, nous devons prouver que ${}^tA$ est une matrice orthogonale.\n\nVérifions la condition pour ${}^tA$ :\n\n$$ {}^t({}^tA)({}^tA) = A({}^tA) $$\n\nNous savons que ${}^tAA = I_n$. En multipliant à gauche par $A$ et à droite par $A^{-1}$, on obtient $A({}^tA)A A^{-1} = A I_n A^{-1}$, ce qui donne $A({}^tA)=I_n$.\n\nDonc, ${}^t({}^tA)({}^tA) = I_n$.\n\nCeci prouve que ${}^tA \\in O_n(\\mathbb{R})$, et donc que l'inverse de tout élément de $O_n(\\mathbb{R})$ est aussi dans $O_n(\\mathbb{R})$.\n\n**Conclusion**\n\nL'ensemble $O_n(\\mathbb{R})$ est stable pour la multiplication, contient l'élément neutre, et chaque élément a un inverse dans l'ensemble. Comme la multiplication matricielle est associative, $O_n(\\mathbb{R})$ est bien un groupe.\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "f32ccc76",
      "content": "#### Déterminant d'une matrice orthogonale\n\nProuver que si $M \\in O_n(\\mathbb{R})$ est une matrice orthogonale, alors son déterminant vaut soit $1$, soit $-1$.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nLa preuve est très directe et repose sur les propriétés fondamentales du déterminant.\n\n1.  Partez de la définition d'une matrice orthogonale : ${}^tMM = I_n$.\n2.  Appliquez la fonction déterminant des deux côtés de cette égalité.\n3.  Utilisez deux propriétés clés du déterminant :\n    -   $\\det(AB) = \\det(A)\\det(B)$\n    -   $\\det({}^tM) = \\det(M)$\n4.  Résolvez l'équation simple que vous obtiendrez pour $\\det(M)$.\n\n</details>",
      "solution": "\n\n**Étape 1 : Utiliser la définition de la matrice orthogonale**\n\nSoit $M$ une matrice de $O_n(\\mathbb{R})$. Par définition, elle satisfait la relation :\n\n$$ {}^tMM = I_n $$\n\noù $I_n$ est la matrice identité d'ordre $n$.\n\n**Étape 2 : Appliquer le déterminant**\n\nAppliquons la fonction déterminant à chaque membre de l'égalité :\n\n$$ \\det({}^tMM) = \\det(I_n) $$\n\n**Étape 3 : Utiliser les propriétés du déterminant**\n\nNous utilisons la propriété de multiplicativité du déterminant, $\\det(AB) = \\det(A)\\det(B)$:\n\n$$ \\det({}^tM) \\det(M) = \\det(I_n) $$\n\nNous savons que le déterminant de la matrice identité est 1, donc $\\det(I_n) = 1$.\n\nNous utilisons aussi la propriété que le déterminant d'une matrice est égal à celui de sa transposée, $\\det({}^tM) = \\det(M)$.\n\nL'équation devient :\n\n$$ \\det(M) \\det(M) = 1 $$\n\n$$ (\\det(M))^2 = 1 $$\n\n**Étape 4 : Résoudre l'équation**\n\nNous avons l'équation $(\\det(M))^2 = 1$. Puisque $M$ est une matrice à coefficients réels, son déterminant $\\det(M)$ est un nombre réel. Les seules solutions réelles de l'équation $x^2 = 1$ sont $x=1$ et $x=-1$.\n\n**Conclusion**\n\nPar conséquent, si $M$ est une matrice orthogonale, son déterminant doit être égal à 1 ou -1.\n\n$$ \\det(M) \\in \\{-1, 1\\} $$\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "f32ccc76",
      "content": "#### Caractérisation des matrices orthogonales par leurs colonnes\n\nProuver qu'une matrice carrée $M$ d'ordre $n$ à coefficients réels est orthogonale si et seulement si ses vecteurs colonnes forment une base orthonormée de $\\mathbb{R}^n$ pour le produit scalaire canonique.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nSoit $M$ une matrice carrée d'ordre $n$. Notons ses vecteurs colonnes $C_1, C_2, \\dots, C_n$.\n\nLa preuve consiste à calculer explicitement le produit matriciel ${}^tMM$ et à interpréter ses coefficients.\n\n1.  Écrivez la matrice $M$ en fonction de ses colonnes : $M = \\begin{pmatrix} C_1 & C_2 & \\dots & C_n \\end{pmatrix}$.\n2.  Déduisez-en la forme de la matrice transposée ${}^tM$. Ses lignes seront les transposées des colonnes de $M$.\n3.  Calculez le produit $P = {}^tMM$. Le coefficient $p_{ij}$ (à la ligne $i$ et colonne $j$) de $P$ est le produit de la $i$-ème ligne de ${}^tM$ par la $j$-ème colonne de $M$.\n4.  Reconnaissez que ce produit est exactement le produit scalaire canonique $\\langle C_i, C_j \\rangle$.\n5.  $M$ est orthogonale si et seulement si ${}^tMM = I_n$. Traduisez cette condition matricielle en conditions sur les coefficients $p_{ij}$, et donc sur les produits scalaires $\\langle C_i, C_j \\rangle$. Concluez en utilisant la définition d'une base orthonormée.\n\n</details>",
      "solution": "\n\nSoit $M \\in M_n(\\mathbb{R})$. Notons $C_1, C_2, \\dots, C_n$ les $n$ vecteurs colonnes de $M$. Chaque $C_j$ est un vecteur de $\\mathbb{R}^n$.\n\n$$ M = \\begin{pmatrix} C_1 & C_2 & \\dots & C_n \\end{pmatrix} $$\n\nLa matrice transposée ${}^tM$ a pour lignes les transposées des colonnes de $M$:\n\n$$ {}^tM = \\begin{pmatrix} {}^tC_1 \\\\ {}^tC_2 \\\\ \\vdots \\\\ {}^tC_n \\end{pmatrix} $$\n\nCalculons le produit matriciel $P = {}^tMM$. Le coefficient $p_{ij}$ de $P$, situé à la $i$-ème ligne et $j$-ème colonne, est obtenu en multipliant la $i$-ème ligne de ${}^tM$ par la $j$-ème colonne de $M$.\n\n$$ p_{ij} = {}^tC_i C_j $$\n\nLe produit d'une matrice ligne ${}^tC_i$ par une matrice colonne $C_j$ est un scalaire. Si $C_i = (c_{1i}, c_{2i}, \\dots, c_{ni})$ et $C_j = (c_{1j}, c_{2j}, \\dots, c_{nj})$, alors :\n\n$$ p_{ij} = {}^tC_i C_j = \\sum_{k=1}^n c_{ki} c_{kj} $$\n\nCette expression est précisément la définition du produit scalaire canonique de $\\mathbb{R}^n$ entre les vecteurs $C_i$ et $C_j$.\n\n$$ p_{ij} = \\langle C_i, C_j \\rangle $$\n\nDonc, la matrice produit est la matrice des produits scalaires des vecteurs colonnes :\n\n$$ {}^tMM = \\begin{pmatrix} \\langle C_1, C_1 \\rangle & \\langle C_1, C_2 \\rangle & \\dots & \\langle C_1, C_n \\rangle \\\\ \\langle C_2, C_1 \\rangle & \\langle C_2, C_2 \\rangle & \\dots & \\langle C_2, C_n \\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\langle C_n, C_1 \\rangle & \\langle C_n, C_2 \\rangle & \\dots & \\langle C_n, C_n \\rangle \\end{pmatrix} $$\n\nMaintenant, utilisons cette formulation pour prouver l'équivalence.\n\nPar définition, la matrice $M$ est orthogonale si et seulement si ${}^tMM = I_n$.\n\nLa matrice identité $I_n$ est la matrice dont les coefficients sont donnés par le symbole de Kronecker $\\delta_{ij}$ (1 si $i=j$, 0 sinon).\n\nDonc, $M$ est orthogonale si et seulement si $p_{ij} = \\delta_{ij}$ pour tous $i, j \\in \\{1, \\dots, n\\}$.\n\nEn utilisant notre expression pour $p_{ij}$, cela est équivalent à :\n\n$$ \\langle C_i, C_j \\rangle = \\delta_{ij} \\quad \\text{pour tous } i, j $$\n\nCette condition signifie :\n\n- Si $i \\neq j$, $\\langle C_i, C_j \\rangle = 0$. Les vecteurs colonnes sont deux à deux orthogonaux.\n- Si $i = j$, $\\langle C_i, C_i \\rangle = 1$, ce qui signifie $\\|C_i\\|^2 = 1$, ou $\\|C_i\\| = 1$. Chaque vecteur colonne est de norme 1 (unitaire).\n\nL'ensemble de ces conditions est exactement la définition d'une famille orthonormée de vecteurs. Une famille orthonormée de $n$ vecteurs dans un espace de dimension $n$ comme $\\mathbb{R}^n$ forme une base.\n\n**Conclusion**\n\nUne matrice $M$ est orthogonale si et seulement si ses vecteurs colonnes $(C_1, \\dots, C_n)$ vérifient $\\langle C_i, C_j \\rangle = \\delta_{ij}$, ce qui est la définition d'une base orthonormée de $\\mathbb{R}^n$.\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "f32ccc76",
      "content": "#### Préservation de l'angle non-orienté par une isométrie\n\nProuver qu'une isométrie $f$ d'un espace euclidien $E$ préserve l'angle non-orienté entre deux vecteurs non nuls.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nL'angle non-orienté $\\theta(x, y)$ entre deux vecteurs non nuls $x$ et $y$ est défini par la formule :\n\n$$ \\theta(x,y) = \\arccos\\left(\\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|}\\right) $$\n\nPour prouver que $f$ préserve cet angle, vous devez montrer que $\\theta(f(x), f(y)) = \\theta(x, y)$.\n\nLa fonction $\\arccos$ est injective sur son domaine de définition $[-1, 1]$. Il suffit donc de prouver que l'argument de l'arccos est le même, c'est-à-dire :\n\n$$ \\frac{\\varphi(f(x),f(y))}{\\|f(x)\\|\\|f(y)\\|} = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|} $$\n\nUtilisez les propriétés d'une isométrie : elle préserve à la fois la norme et le produit scalaire.\n\n</details>",
      "solution": "\n\n**Étape 1 : Définition de l'angle non-orienté**\n\nSoient $x$ et $y$ deux vecteurs non nuls de l'espace euclidien $E$. L'angle non-orienté $\\theta(x,y)$ entre eux est l'unique réel dans $[0, \\pi]$ tel que :\n\n$$ \\cos(\\theta(x,y)) = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|} $$\n\nDe même, l'angle non-orienté $\\theta(f(x), f(y))$ entre leurs images par $f$ (qui sont aussi non nulles car $f$ est une isométrie) est défini par :\n\n$$ \\cos(\\theta(f(x), f(y))) = \\frac{\\varphi(f(x), f(y))}{\\|f(x)\\|\\|f(y)\\|} $$\n\n**Étape 2 : Utiliser les propriétés de l'isométrie**\n\nSoit $f$ une isométrie. Par définition, $f$ préserve la norme. C'est-à-dire que pour tout vecteur $v \\in E$, $\\|f(v)\\| = \\|v\\|$.\n\nEn particulier, pour nos vecteurs $x$ et $y$ :\n\n- $\\|f(x)\\| = \\|x\\|$\n- $\\|f(y)\\| = \\|y\\|$\n\nDe plus, nous avons montré qu'une transformation qui préserve la norme préserve aussi le produit scalaire. Donc, pour tous vecteurs $u, v \\in E$, $\\varphi(f(u), f(v)) = \\varphi(u, v)$.\n\nEn particulier, pour nos vecteurs $x$ et $y$ :\n\n- $\\varphi(f(x), f(y)) = \\varphi(x, y)$\n\n**Étape 3 : Comparer les cosinus des angles**\n\nSubstituons ces propriétés dans l'expression de $\\cos(\\theta(f(x), f(y)))$ :\n\n$$ \\cos(\\theta(f(x), f(y))) = \\frac{\\varphi(f(x), f(y))}{\\|f(x)\\|\\|f(y)\\|} = \\frac{\\varphi(x, y)}{\\|x\\|\\|y\\|} $$\n\nNous reconnaissons le membre de droite comme étant l'expression de $\\cos(\\theta(x, y))$. Nous avons donc montré que :\n\n$$ \\cos(\\theta(f(x), f(y))) = \\cos(\\theta(x, y)) $$\n\n**Étape 4 : Conclure sur l'égalité des angles**\n\nLes angles $\\theta(x, y)$ et $\\theta(f(x), f(y))$ sont tous les deux, par définition, des valeurs dans l'intervalle $[0, \\pi]$. La fonction cosinus est strictement décroissante sur cet intervalle, et donc injective.\n\nSi deux angles de $[0, \\pi]$ ont le même cosinus, ils doivent être égaux.\n\nPar conséquent, $\\theta(f(x), f(y)) = \\theta(x, y)$.\n\n**Conclusion**\n\nUne isométrie préserve bien l'angle non-orienté entre deux vecteurs.\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "f32ccc76",
      "content": "#### Relation d'équivalence pour l'orientation des bases\n\nDans un $\\mathbb{R}$-espace vectoriel $E$ de dimension finie, on dit que deux bases $\\mathcal{B}$ et $\\mathcal{B}'$ ont la même orientation (noté $\\mathcal{B} \\sim \\mathcal{B}'$) si le déterminant de la matrice de passage de $\\mathcal{B}$ à $\\mathcal{B}'$, noté $P_{\\mathcal{B} \\to \\mathcal{B}'}$, est strictement positif.\n\nProuver que $\\sim$ est une relation d'équivalence sur l'ensemble des bases de $E$.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nPour prouver qu'une relation est d'équivalence, il faut vérifier trois propriétés : la réflexivité, la symétrie et la transitivité.\n\nUtilisez les propriétés fondamentales des matrices de passage :\n\n1.  **Réflexivité** ($\\mathcal{B} \\sim \\mathcal{B}$) : Quelle est la matrice de passage $P_{\\mathcal{B} \\to \\mathcal{B}}$ ? Quel est son déterminant ?\n2.  **Symétrie** (si $\\mathcal{B} \\sim \\mathcal{B}'$, alors $\\mathcal{B}' \\sim \\mathcal{B}$) : Quelle est la relation entre $P_{\\mathcal{B} \\to \\mathcal{B}'}$ et $P_{\\mathcal{B}' \\to \\mathcal{B}}$ ? Comment leurs déterminants sont-ils liés ?\n3.  **Transitivité** (si $\\mathcal{B} \\sim \\mathcal{B}'$ et $\\mathcal{B}' \\sim \\mathcal{B}''$, alors $\\mathcal{B} \\sim \\mathcal{B}''$) : Quelle est la relation entre $P_{\\mathcal{B} \\to \\mathcal{B}''}$, $P_{\\mathcal{B} \\to \\mathcal{B}'}$ et $P_{\\mathcal{B}' \\to \\mathcal{B}''}$ (formule de Chasles) ? Utilisez la propriété de multiplicativité du déterminant.\n\n</details>",
      "solution": "\n\nSoit $\\mathcal{E}$ l'ensemble de toutes les bases de l'espace vectoriel $E$. Nous allons vérifier les trois propriétés de la relation d'équivalence pour la relation $\\sim$ sur $\\mathcal{E}$.\n\n**1. Réflexivité**\n\nNous devons montrer que pour toute base $\\mathcal{B} \\in \\mathcal{E}$, on a $\\mathcal{B} \\sim \\mathcal{B}$.\n\nLa matrice de passage de la base $\\mathcal{B}$ à elle-même est la matrice identité $I$.\n\n$$ P_{\\mathcal{B} \\to \\mathcal{B}} = I $$\n\nLe déterminant de la matrice identité est 1.\n\n$$ \\det(P_{\\mathcal{B} \\to \\mathcal{B}}) = \\det(I) = 1 $$\n\nPuisque $1 > 0$, la condition est remplie. La relation est réflexive.\n\n**2. Symétrie**\n\nNous devons montrer que si $\\mathcal{B} \\sim \\mathcal{B}'$, alors $\\mathcal{B}' \\sim \\mathcal{B}$.\n\nL'hypothèse $\\mathcal{B} \\sim \\mathcal{B}'$ signifie que $\\det(P_{\\mathcal{B} \\to \\mathcal{B}'}) > 0$.\n\nLa matrice de passage de $\\mathcal{B}'$ à $\\mathcal{B}$ est l'inverse de la matrice de passage de $\\mathcal{B}$ à $\\mathcal{B}'$:\n\n$$ P_{\\mathcal{B}' \\to \\mathcal{B}} = (P_{\\mathcal{B} \\to \\mathcal{B}'})^{-1} $$\n\nLe déterminant de l'inverse d'une matrice est l'inverse de son déterminant :\n\n$$ \\det(P_{\\mathcal{B}' \\to \\mathcal{B}}) = \\det((P_{\\mathcal{B} \\to \\mathcal{B}'})^{-1}) = \\frac{1}{\\det(P_{\\mathcal{B} \\to \\mathcal{B}'})} $$\n\nPar hypothèse, $\\det(P_{\\mathcal{B} \\to \\mathcal{B}'})$ est un nombre réel strictement positif. L'inverse d'un nombre réel strictement positif est aussi strictement positif.\n\nDonc, $\\det(P_{\\mathcal{B}' \\to \\mathcal{B}}) > 0$, ce qui signifie que $\\mathcal{B}' \\sim \\mathcal{B}$. La relation est symétrique.\n\n**3. Transitivité**\n\nNous devons montrer que si $\\mathcal{B} \\sim \\mathcal{B}'$ et $\\mathcal{B}' \\sim \\mathcal{B}''$, alors $\\mathcal{B} \\sim \\mathcal{B}''$.\n\nLes hypothèses sont :\n\n- $\\mathcal{B} \\sim \\mathcal{B}' \\implies \\det(P_{\\mathcal{B} \\to \\mathcal{B}'}) > 0$\n- $\\mathcal{B}' \\sim \\mathcal{B}'' \\implies \\det(P_{\\mathcal{B}' \\to \\mathcal{B}''}) > 0$\n\nLa formule de Chasles pour les matrices de passage nous dit que :\n\n$$ P_{\\mathcal{B} \\to \\mathcal{B}''} = P_{\\mathcal{B} \\to \\mathcal{B}'} \\cdot P_{\\mathcal{B}' \\to \\mathcal{B}''} $$\n\nEn utilisant la propriété de multiplicativité du déterminant :\n\n$$ \\det(P_{\\mathcal{B} \\to \\mathcal{B}''}) = \\det(P_{\\mathcal{B} \\to \\mathcal{B}'}) \\cdot \\det(P_{\\mathcal{B}' \\to \\mathcal{B}''}) $$\n\nPar hypothèse, nous multiplions deux nombres réels strictement positifs. Le produit de deux nombres réels strictement positifs est un nombre réel strictement positif.\n\nDonc, $\\det(P_{\\mathcal{B} \\to \\mathcal{B}''}) > 0$, ce qui signifie que $\\mathcal{B} \\sim \\mathcal{B}''$. La relation est transitive.\n\n**Conclusion**\n\nLa relation $\\sim$ est réflexive, symétrique et transitive. C'est donc bien une relation d'équivalence.\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "f32ccc76",
      "content": "#### L'inverse d'une isométrie est une isométrie\n\nSoit $f$ une isométrie d'un espace euclidien $E$ de dimension finie. Prouver que $f$ est une application bijective et que son application inverse $f^{-1}$ est aussi une isométrie.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nLa preuve se fait en deux temps.\n\n1.  **Prouver la bijectivité** : Dans un espace de dimension finie, un endomorphisme est bijectif si et seulement s'il est injectif. Pour prouver l'injectivité, montrez que le noyau de $f$ est réduit au vecteur nul, i.e., $\\ker(f) = \\{0\\}$. Pour cela, prenez un vecteur $x \\in \\ker(f)$, ce qui signifie $f(x)=0$. Utilisez la propriété de l'isométrie $\\|f(x)\\|=\\|x\\|$ pour en déduire une information sur $\\|x\\|$, et donc sur $x$.\n\n2.  **Prouver que $f^{-1}$ est une isométrie** : On veut montrer que $\\|f^{-1}(y)\\| = \\|y\\|$ pour tout vecteur $y \\in E$. L'astuce est de poser $x = f^{-1}(y)$, ce qui est équivalent à $y = f(x)$. Maintenant, exprimez $\\|y\\|$ en fonction de $x$, utilisez le fait que $f$ est une isométrie sur $x$, puis ré-exprimez le résultat en fonction de $y$.\n\n</details>",
      "solution": "\n\n**Étape 1 : Preuve de la bijectivité de $f$**\n\nSoit $f$ une isométrie sur un espace $E$ de dimension finie. Pour montrer que $f$ est bijective, il suffit de montrer qu'elle est injective, c'est-à-dire que son noyau ne contient que le vecteur nul.\n\nSoit $x \\in \\ker(f)$. Par définition du noyau, cela signifie que $f(x) = 0$.\n\nCalculons la norme de $f(x)$ : $\\|f(x)\\| = \\|0\\| = 0$.\n\nPar définition d'une isométrie, $f$ préserve la norme, donc $\\|f(x)\\| = \\|x\\|$.\n\nEn combinant les deux égalités, nous obtenons $\\|x\\| = 0$.\n\nLa seule propriété de la norme nous dit que $\\|x\\| = 0$ si et seulement si $x = 0$.\n\nDonc, le seul vecteur dans le noyau de $f$ est le vecteur nul : $\\ker(f) = \\{0\\}$.\n\nUn endomorphisme en dimension finie est injectif si et seulement s'il est bijectif. Par conséquent, $f$ est une application bijective. Elle admet donc une application inverse $f^{-1}$.\n\n**Étape 2 : Preuve que $f^{-1}$ est une isométrie**\n\nNous voulons maintenant prouver que $f^{-1}: E \\to E$ est une isométrie. Nous devons montrer que pour tout vecteur $y \\in E$, on a $\\|f^{-1}(y)\\| = \\|y\\|$.\n\nSoit $y \\in E$. Puisque $f$ est bijective, il existe un unique vecteur $x \\in E$ tel que $y = f(x)$. Cet unique $x$ est par définition $f^{-1}(y)$.\n\n$$ x = f^{-1}(y) \\iff y = f(x) $$\n\nPartons de la propriété que $f$ est une isométrie, appliquée au vecteur $x$ :\n\n$$ \\|f(x)\\| = \\|x\\| $$\n\nEn utilisant les relations ci-dessus, nous pouvons remplacer $f(x)$ par $y$ et $x$ par $f^{-1}(y)$:\n\n$$ \\|y\\| = \\|f^{-1}(y)\\| $$\n\nCette égalité est vraie pour tout $y \\in E$.\n\n**Conclusion**\n\nNous avons prouvé que $f$ est bijective et que son inverse $f^{-1}$ préserve la norme. Par conséquent, $f^{-1}$ est également une isométrie. Ceci confirme que l'ensemble des isométries de $E$, noté $O(E)$, forme bien un groupe pour la composition.\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "f32ccc76",
      "content": "#### $SO_n(\\mathbb{R})$ est un sous-groupe de $O_n(\\mathbb{R})$\n\nProuver que le groupe spécial orthogonal $SO_n(\\mathbb{R})$, défini comme l'ensemble des matrices orthogonales de déterminant 1, est un sous-groupe du groupe orthogonal $O_n(\\mathbb{R})$.\n\n<details class=\"hint\">\n\n<summary>Indices et stratégie globale</summary>\n\nPour prouver qu'un sous-ensemble $H$ d'un groupe $G$ est un sous-groupe, il faut vérifier trois points :\n\n1.  $H$ n'est pas vide (il contient l'élément neutre de $G$).\n2.  $H$ est stable par la loi de composition de $G$ (si $a, b \\in H$, alors $ab \\in H$).\n3.  $H$ est stable par passage à l'inverse (si $a \\in H$, alors $a^{-1} \\in H$).\n\nAppliquez ces trois points à $H = SO_n(\\mathbb{R})$ et $G = O_n(\\mathbb{R})$. Vous aurez besoin des propriétés du déterminant : $\\det(I_n)=1$, $\\det(AB)=\\det(A)\\det(B)$ et $\\det(A^{-1}) = 1/\\det(A)$.\n\n</details>",
      "solution": "\n\nSoit $O_n(\\mathbb{R})$ le groupe des matrices orthogonales et $SO_n(\\mathbb{R}) = \\{ M \\in O_n(\\mathbb{R}) \\mid \\det(M)=1 \\}$. Nous vérifions les trois conditions pour être un sous-groupe.\n\n**1. $SO_n(\\mathbb{R})$ est non vide**\n\nL'élément neutre du groupe $O_n(\\mathbb{R})$ est la matrice identité $I_n$.\n\nNous savons que $I_n$ est orthogonale car ${}^tI_n I_n = I_n I_n = I_n$.\n\nDe plus, son déterminant est $\\det(I_n) = 1$.\n\nPuisque $I_n \\in O_n(\\mathbb{R})$ et $\\det(I_n) = 1$, la matrice identité $I_n$ appartient à $SO_n(\\mathbb{R})$. L'ensemble n'est donc pas vide.\n\n**2. Stabilité par multiplication**\n\nSoient $A$ et $B$ deux matrices appartenant à $SO_n(\\mathbb{R})$.\n\nPar définition, cela signifie que $A, B \\in O_n(\\mathbb{R})$ et $\\det(A)=1$, $\\det(B)=1$.\n\nNous devons montrer que le produit $AB$ est aussi dans $SO_n(\\mathbb{R})$.\n\n*   Premièrement, comme $O_n(\\mathbb{R})$ est un groupe, le produit $AB$ est encore une matrice orthogonale, donc $AB \\in O_n(\\mathbb{R})$.\n*   Deuxièmement, calculons le déterminant du produit. En utilisant la propriété de multiplicativité du déterminant :\n\n    $$ \\det(AB) = \\det(A) \\det(B) $$\n\n    Puisque $\\det(A)=1$ et $\\det(B)=1$, on a :\n\n    $$ \\det(AB) = 1 \\cdot 1 = 1 $$\n\nLes deux conditions sont remplies, donc $AB \\in SO_n(\\mathbb{R})$. L'ensemble est stable par multiplication.\n\n**3. Stabilité par passage à l'inverse**\n\nSoit $A \\in SO_n(\\mathbb{R})$.\n\nCela signifie que $A \\in O_n(\\mathbb{R})$ et $\\det(A)=1$.\n\nNous devons montrer que son inverse, $A^{-1}$, est aussi dans $SO_n(\\mathbb{R})$.\n\n*   Premièrement, comme $A \\in O_n(\\mathbb{R})$ et que $O_n(\\mathbb{R})$ est un groupe, son inverse $A^{-1}$ est aussi dans $O_n(\\mathbb{R})$.\n*   Deuxièmement, calculons le déterminant de l'inverse :\n\n    $$ \\det(A^{-1}) = \\frac{1}{\\det(A)} $$\n\n    Puisque $\\det(A)=1$, on a :\n\n    $$ \\det(A^{-1}) = \\frac{1}{1} = 1 $$\n\nLes deux conditions sont remplies, donc $A^{-1} \\in SO_n(\\mathbb{R})$. L'ensemble est stable par passage à l'inverse.\n\n**Conclusion**\n\n$SO_n(\\mathbb{R})$ est un sous-ensemble non vide de $O_n(\\mathbb{R})$, stable par multiplication et par passage à l'inverse. C'est donc un sous-groupe de $O_n(\\mathbb{R})$.\n\n",
      "options": []
    }
  ]
}