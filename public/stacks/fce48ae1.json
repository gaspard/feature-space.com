{
  "info": {
    "id": "fce48ae1",
    "title": "Rappels d’algèbre linéaire - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Rappels d’algèbre linéaire",
    "course": "Algèbre",
    "tags": [
      "Algèbre linéaire",
      "Espaces vectoriels",
      "Déterminant",
      "Endomorphismes",
      "Diagonalisation"
    ],
    "count": 10
  },
  "cards": [
    {
      "id": "1",
      "stackId": "fce48ae1",
      "content": "#### Intersection de sous-espaces vectoriels\n\nProuvez que l'intersection de deux sous-espaces vectoriels $W_1$ et $W_2$ d'un $K$-espace vectoriel $V$ est un sous-espace vectoriel de $V$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour montrer qu'un ensemble $W$ est un sous-espace vectoriel, vous devez vérifier deux choses principales :\n\n1. L'ensemble est non vide (il contient au moins le vecteur nul).\n2. L'ensemble est stable par combinaison linéaire (si $u, v \\in W$ et $\\lambda \\in K$, alors $\\lambda u + v \\in W$).\n\nAppliquez ces critères à l'ensemble $W_1 \\cap W_2$.\n\n</details>",
      "solution": "\n\nSoit $W = W_1 \\cap W_2$. Nous devons montrer que $W$ est un sous-espace vectoriel de $V$.\n\n**Étape 1 : Le vecteur nul appartient à l'intersection**\n\nPuisque $W_1$ et $W_2$ sont des sous-espaces vectoriels, ils contiennent tous deux le vecteur nul $0_V$.\n\nPar conséquent, $0_V \\in W_1$ et $0_V \\in W_2$, ce qui implique $0_V \\in W_1 \\cap W_2$. L'intersection n'est donc pas vide.\n\n**Étape 2 : Stabilité par combinaison linéaire**\n\nSoient $u, v \\in W$ et $\\lambda \\in K$.\n\nPuisque $u, v \\in W_1 \\cap W_2$, alors $u \\in W_1$ et $v \\in W_1$. Comme $W_1$ est un sous-espace vectoriel, il est stable par combinaison linéaire, donc $\\lambda u + v \\in W_1$.\n\nDe même, $u \\in W_2$ et $v \\in W_2$. Comme $W_2$ est un sous-espace vectoriel, on a $\\lambda u + v \\in W_2$.\n\nPuisque $\\lambda u + v$ appartient à la fois à $W_1$ et à $W_2$, il appartient à leur intersection.\n\n$$ \\lambda u + v \\in W_1 \\cap W_2 $$\n\n**Conclusion :**\n\n$W_1 \\cap W_2$ contient $0_V$ et est stable par combinaison linéaire. C'est donc un sous-espace vectoriel de $V$.\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "fce48ae1",
      "content": "#### Unicité des coordonnées dans une base\n\nSoit $V$ un espace vectoriel et $\\mathfrak{B} = \\{e_1, \\dots, e_n\\}$ une base de $V$. Prouvez que tout vecteur $v \\in V$ s'écrit de manière unique comme combinaison linéaire des vecteurs de la base.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nL'existence de la décomposition vient du fait que la famille est génératrice.\n\nPour l'unicité, supposez qu'il existe deux écritures différentes pour un même vecteur $v$. Soustrayez les deux égalités et utilisez le fait que la famille $\\mathfrak{B}$ est libre (indépendante).\n\n</details>",
      "solution": "\n\n**Étape 1 : Existence**\n\nPar définition d'une base, la famille $\\mathfrak{B}$ est génératrice. Donc pour tout $v \\in V$, il existe des scalaires $\\lambda_1, \\dots, \\lambda_n$ tels que :\n\n$$ v = \\sum_{i=1}^n \\lambda_i e_i $$\n\n**Étape 2 : Unicité**\n\nSupposons que $v$ admette deux décompositions :\n\n$$ v = \\sum_{i=1}^n \\lambda_i e_i \\quad \\text{et} \\quad v = \\sum_{i=1}^n \\mu_i e_i $$\n\nEn soustrayant la deuxième égalité à la première, on obtient :\n\n$$ 0_V = v - v = \\left(\\sum_{i=1}^n \\lambda_i e_i\\right) - \\left(\\sum_{i=1}^n \\mu_i e_i\\right) = \\sum_{i=1}^n (\\lambda_i - \\mu_i) e_i $$\n\n**Étape 3 : Utilisation de la liberté**\n\nPuisque $\\mathfrak{B}$ est une base, c'est une famille libre. Par définition d'une famille libre, la seule combinaison linéaire nulle est celle dont tous les coefficients sont nuls.\n\nAinsi, pour tout $i \\in \\{1, \\dots, n\\}$ :\n\n$$ \\lambda_i - \\mu_i = 0 \\implies \\lambda_i = \\mu_i $$\n\n**Conclusion :**\n\nLes coefficients sont identiques, la décomposition est donc unique.\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "fce48ae1",
      "content": "#### Le noyau est un sous-espace vectoriel\n\nSoit $f : V \\to W$ une application linéaire. Prouvez que le noyau de $f$, noté $\\text{Ker}(f)$, est un sous-espace vectoriel de $V$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nRappelez-vous la définition du noyau : $\\text{Ker}(f) = \\{v \\in V \\mid f(v) = 0_W\\}$.\n\nUtilisez la linéarité de $f$ (notamment $f(\\lambda u + v) = \\lambda f(u) + f(v)$) pour vérifier les propriétés de stabilité d'un sous-espace vectoriel.\n\n</details>",
      "solution": "\n\nNous devons montrer que $\\text{Ker}(f)$ contient $0_V$ et est stable par combinaison linéaire.\n\n**Étape 1 : Le vecteur nul**\n\nComme $f$ est linéaire, on sait que l'image du vecteur nul est le vecteur nul : $f(0_V) = 0_W$.\n\nDonc $0_V \\in \\text{Ker}(f)$.\n\n**Étape 2 : Stabilité**\n\nSoient $u, v \\in \\text{Ker}(f)$ et $\\lambda \\in K$.\n\nPar définition du noyau, cela signifie que $f(u) = 0_W$ et $f(v) = 0_W$.\n\nCalculons l'image de la combinaison linéaire $\\lambda u + v$ par $f$ :\n\n$$ f(\\lambda u + v) = \\lambda f(u) + f(v) \\quad (\\text{car } f \\text{ est linéaire}) $$\n\nEn remplaçant $f(u)$ et $f(v)$ par $0_W$ :\n\n$$ f(\\lambda u + v) = \\lambda \\cdot 0_W + 0_W = 0_W $$\n\n**Conclusion :**\n\nPuisque $f(\\lambda u + v) = 0_W$, le vecteur $\\lambda u + v$ appartient à $\\text{Ker}(f)$. $\\text{Ker}(f)$ est bien un sous-espace vectoriel de $V$.\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "fce48ae1",
      "content": "#### Injectivité et Noyau\n\nSoit $f : V \\to W$ une application linéaire. Prouvez que $f$ est injective si et seulement si $\\text{Ker}(f) = \\{0_V\\}$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nCette preuve est une double implication ($\\iff$).\n\n1. ($\\Rightarrow$) Si $f$ est injective, montrez que le seul vecteur qui s'envoie sur $0_W$ est $0_V$.\n2. ($\\Leftarrow$) Si le noyau est trivial, supposez $f(x) = f(y)$ et utilisez la linéarité pour montrer que $f(x-y) = 0$, puis concluez que $x=y$.\n\n</details>",
      "solution": "\n\n**Sens direct ($\\Rightarrow$) :** Supposons $f$ injective.\n\nSoit $x \\in \\text{Ker}(f)$. Par définition, $f(x) = 0_W$.\n\nOr, nous savons que pour toute application linéaire, $f(0_V) = 0_W$.\n\nOn a donc $f(x) = f(0_V)$.\n\nPuisque $f$ est injective, $f(x) = f(0_V)$ implique $x = 0_V$.\n\nDonc $\\text{Ker}(f) = \\{0_V\\}$.\n\n**Sens réciproque ($\\Leftarrow$) :** Supposons $\\text{Ker}(f) = \\{0_V\\}$.\n\nSoient $x, y \\in V$ tels que $f(x) = f(y)$.\n\nPar linéarité, cela implique :\n\n$$ f(x) - f(y) = 0_W \\implies f(x - y) = 0_W $$\n\nCela signifie que le vecteur $x - y$ appartient au noyau de $f$.\n\nOr, par hypothèse, $\\text{Ker}(f) = \\{0_V\\}$, donc :\n\n$$ x - y = 0_V \\implies x = y $$\n\nNous avons montré que $f(x) = f(y) \\implies x = y$, donc $f$ est injective.\n\n**Conclusion :**\n\n$f$ est injective $\\iff \\text{Ker}(f) = \\{0_V\\}$.\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "fce48ae1",
      "content": "#### Changement de coordonnées\n\nSoit $V$ un espace vectoriel muni d'une base $\\mathfrak{B} = \\{e_1, \\dots, e_n\\}$. Soit $\\mathcal{C} = \\{v_1, \\dots, v_n\\}$ une autre base de $V$. Soit $P$ la matrice de passage de $\\mathfrak{B}$ à $\\mathcal{C}$ (ses colonnes sont les composantes des $v_j$ dans $\\mathfrak{B}$).\n\nProuvez que si $X$ est la matrice colonne des coordonnées d'un vecteur $u$ dans $\\mathfrak{B}$, et $X'$ celles dans $\\mathcal{C}$, alors $X = P X'$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nÉcrivez le vecteur $u$ comme combinaison linéaire des éléments de la base $\\mathcal{C}$ en utilisant les coordonnées $X'$.\n\nEnsuite, remplacez chaque vecteur de base $v_j$ de $\\mathcal{C}$ par son expression en fonction de la base $\\mathfrak{B}$ (qui utilise les coefficients de la matrice $P$).\n\nIdentifiez le résultat avec l'expression de $u$ dans la base $\\mathfrak{B}$.\n\n</details>",
      "solution": "\n\nSoit $u \\in V$.\n\nNotons $X = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}$ les coordonnées dans $\\mathfrak{B}$, donc $u = \\sum_{i=1}^n x_i e_i$.\n\nNotons $X' = \\begin{pmatrix} x'_1 \\\\ \\vdots \\\\ x'_n \\end{pmatrix}$ les coordonnées dans $\\mathcal{C}$, donc $u = \\sum_{j=1}^n x'_j v_j$.\n\n**Étape 1 : Expression des vecteurs de la nouvelle base**\n\nLa matrice de passage $P = (p_{ij})$ contient en $j$-ème colonne les coordonnées de $v_j$ dans la base $\\mathfrak{B}$ :\n\n$$ v_j = \\sum_{i=1}^n p_{ij} e_i $$\n\n**Étape 2 : Substitution**\n\nRemplaçons $v_j$ dans l'expression de $u$ :\n\n$$ u = \\sum_{j=1}^n x'_j \\left( \\sum_{i=1}^n p_{ij} e_i \\right) $$\n\n**Étape 3 : Réarrangement (interversion des sommes)**\n\n$$ u = \\sum_{i=1}^n \\left( \\sum_{j=1}^n p_{ij} x'_j \\right) e_i $$\n\n**Étape 4 : Identification**\n\nPar l'unicité des coordonnées dans la base $\\mathfrak{B}$, le coefficient devant $e_i$ doit être $x_i$.\n\n$$ x_i = \\sum_{j=1}^n p_{ij} x'_j $$\n\nCette égalité pour tout $i$ correspond exactement au produit matriciel de la ligne $i$ de $P$ par la colonne $X'$.\n\n**Conclusion :**\n\nSous forme matricielle, cela s'écrit $X = P X'$.\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "fce48ae1",
      "content": "#### Déterminant de l'inverse\n\nProuvez que si une matrice $A \\in M_n(K)$ est inversible, alors $\\det(A^{-1}) = \\frac{1}{\\det(A)}$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez la propriété multiplicative du déterminant : $\\det(AB) = \\det(A)\\det(B)$.\n\nAppliquez cette propriété à l'identité définissant l'inverse : $A A^{-1} = I_n$.\n\nRappelez-vous que $\\det(I_n) = 1$.\n\n</details>",
      "solution": "\n\n**Étape 1 : Définition de l'inverse**\n\nSi $A$ est inversible, il existe $A^{-1}$ telle que :\n\n$$ A A^{-1} = I_n $$\n\n**Étape 2 : Application du déterminant**\n\nAppliquons la fonction déterminant de chaque côté de l'égalité :\n\n$$ \\det(A A^{-1}) = \\det(I_n) $$\n\n**Étape 3 : Propriétés du déterminant**\n\nNous savons que $\\det(I_n) = 1$ (normalisation).\n\nNous savons aussi que le déterminant est multiplicatif, c'est-à-dire $\\det(XY) = \\det(X)\\det(Y)$.\n\nDonc :\n\n$$ \\det(A) \\det(A^{-1}) = 1 $$\n\n**Étape 4 : Conclusion**\n\nPuisque le produit est 1, cela implique que $\\det(A) \\neq 0$ (ce qui est cohérent avec l'inversibilité) et que nous pouvons diviser par $\\det(A)$.\n\n$$ \\det(A^{-1}) = \\frac{1}{\\det(A)} = (\\det(A))^{-1} $$\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "fce48ae1",
      "content": "#### Liberté des vecteurs propres\n\nSoit $u$ un endomorphisme de $V$. Prouvez que si $v_1, \\dots, v_k$ sont des vecteurs propres associés à des valeurs propres distinctes $\\lambda_1, \\dots, \\lambda_k$ (avec $\\lambda_i \\neq \\lambda_j$ pour $i \\neq j$), alors la famille $\\{v_1, \\dots, v_k\\}$ est libre.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nProcédez par récurrence sur le nombre $k$ de vecteurs.\n\nPour l'hérédité (passage de $k-1$ à $k$), supposez une combinaison linéaire nulle $\\sum_{i=1}^k \\alpha_i v_i = 0$.\n\nAppliquez l'endomorphisme $f$ à cette équation, puis multipliez l'équation originale par une valeur propre (par exemple $\\lambda_k$). En soustrayant les deux résultats, vous éliminerez un terme et pourrez utiliser l'hypothèse de récurrence.\n\n</details>",
      "solution": "\n\nNous procédons par récurrence sur $k$.\n\n**Initialisation ($k=1$) :**\n\nSi $v_1$ est un vecteur propre, il est non nul par définition. La famille $\\{v_1\\}$ est donc libre.\n\n**Hérédité :**\n\nSupposons la propriété vraie pour toute famille de $k-1$ vecteurs propres associés à des valeurs propres distinctes.\n\nSoient $v_1, \\dots, v_k$ des vecteurs propres associés à des valeurs propres distinctes $\\lambda_1, \\dots, \\lambda_k$.\n\nConsidérons une combinaison linéaire nulle :\n\n(1) $$ \\alpha_1 v_1 + \\alpha_2 v_2 + \\dots + \\alpha_k v_k = 0_V $$\n\nAppliquons l'endomorphisme $u$ à cette égalité. Par linéarité et définition des vecteurs propres ($u(v_i) = \\lambda_i v_i$) :\n\n(2) $$ \\alpha_1 \\lambda_1 v_1 + \\alpha_2 \\lambda_2 v_2 + \\dots + \\alpha_k \\lambda_k v_k = 0_V $$\n\nMultiplions l'équation (1) par le scalaire $\\lambda_k$ :\n\n(3) $$ \\alpha_1 \\lambda_k v_1 + \\alpha_2 \\lambda_k v_2 + \\dots + \\alpha_k \\lambda_k v_k = 0_V $$\n\nSoustrayons l'équation (3) de l'équation (2) :\n\n$$ \\alpha_1 (\\lambda_1 - \\lambda_k) v_1 + \\dots + \\alpha_{k-1} (\\lambda_{k-1} - \\lambda_k) v_k + \\alpha_k (\\lambda_k - \\lambda_k) v_k = 0_V $$\n\nLe dernier terme s'annule. Il reste :\n\n$$ \\sum_{i=1}^{k-1} \\alpha_i (\\lambda_i - \\lambda_k) v_i = 0_V $$\n\nCeci est une combinaison linéaire de $k-1$ vecteurs propres. Par hypothèse de récurrence, cette famille est libre, donc tous les coefficients sont nuls :\n\n$$ \\forall i \\in \\{1, \\dots, k-1\\}, \\quad \\alpha_i (\\lambda_i - \\lambda_k) = 0 $$\n\nComme les valeurs propres sont distinctes, $\\lambda_i - \\lambda_k \\neq 0$ pour tout $i < k$. Donc nécessairement $\\alpha_i = 0$ pour $i=1, \\dots, k-1$.\n\nEn reportant dans (1), il reste $\\alpha_k v_k = 0$. Comme $v_k \\neq 0$, alors $\\alpha_k = 0$.\n\n**Conclusion :**\n\nTous les $\\alpha_i$ sont nuls, la famille est libre.\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "fce48ae1",
      "content": "#### Caractérisation de la somme directe (intersection)\n\nSoient $E$ et $F$ deux sous-espaces vectoriels de $V$. Prouvez que la somme $E + F$ est directe (notée $E \\oplus F$) si et seulement si $E \\cap F = \\{0_V\\}$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nRappelez-vous la définition de la somme directe : pour tout $w \\in E+F$, il existe une **unique** décomposition $w = u + v$ avec $u \\in E, v \\in F$.\n\n($\\Rightarrow$) Si $x$ est dans l'intersection, considérez sa décomposition comme $x + 0$ et $0 + x$.\n\n($\\Leftarrow$) Si l'intersection est nulle, supposez deux décompositions $u+v = u'+v'$ et montrez que $u-u'$ appartient à l'intersection.\n\n</details>",
      "solution": "\n\n**Sens direct ($\\Rightarrow$) :** Supposons la somme directe.\n\nSoit $x \\in E \\cap F$.\n\nOn peut écrire $x$ comme élément de la somme $E+F$ de deux manières :\n\n1. $x = x + 0_V$ (avec $x \\in E$ et $0_V \\in F$).\n2. $x = 0_V + x$ (avec $0_V \\in E$ et $x \\in F$).\n\nPuisque la somme est directe, la décomposition est unique. Donc le premier terme de la somme doit être le même : $x = 0_V$.\n\nAinsi, $E \\cap F = \\{0_V\\}$.\n\n**Sens réciproque ($\\Leftarrow$) :** Supposons $E \\cap F = \\{0_V\\}$.\n\nSoit $w \\in E + F$. Supposons qu'il existe deux décompositions :\n\n$$ w = u + v = u' + v' \\quad \\text{avec } u, u' \\in E \\text{ et } v, v' \\in F $$\n\nOn peut réarranger l'égalité :\n\n$$ u - u' = v' - v $$\n\nLe terme de gauche ($u - u'$) appartient à $E$ (car $E$ est un sous-espace).\n\nLe terme de droite ($v' - v$) appartient à $F$ (car $F$ est un sous-espace).\n\nDonc, ce vecteur commun appartient à $E \\cap F$.\n\nPar hypothèse, $E \\cap F = \\{0_V\\}$, donc :\n\n$$ u - u' = 0_V \\implies u = u' $$\n\n$$ v' - v = 0_V \\implies v = v' $$\n\nL'unicité de la décomposition est prouvée.\n\n**Conclusion :**\n\nLa somme est directe si et seulement si l'intersection est réduite au vecteur nul.\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "fce48ae1",
      "content": "#### Matrice d'un endomorphisme diagonalisable\n\nProuvez que si $\\mathfrak{B} = \\{v_1, \\dots, v_n\\}$ est une base de $V$ constituée de vecteurs propres d'un endomorphisme $f$, alors la matrice de $f$ dans cette base est diagonale.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nLa $j$-ème colonne de la matrice de $f$ dans la base $\\mathfrak{B}$ contient les coordonnées du vecteur $f(v_j)$ exprimé dans la base $\\mathfrak{B}$.\n\nUtilisez la définition d'un vecteur propre : $f(v_j) = \\lambda_j v_j$.\n\n</details>",
      "solution": "\n\nSoit $A = \\text{Mat}_{\\mathfrak{B}}(f)$. Par définition, la colonne $j$ de $A$ correspond aux coordonnées du vecteur $f(v_j)$ dans la base $\\mathfrak{B} = \\{v_1, \\dots, v_n\\}$.\n\n**Étape 1 : Image des vecteurs de base**\n\nPuisque les $v_j$ sont des vecteurs propres, il existe des scalaires $\\lambda_j$ (les valeurs propres) tels que :\n\n$$ f(v_j) = \\lambda_j v_j $$\n\n**Étape 2 : Coordonnées dans la base**\n\nLe vecteur $\\lambda_j v_j$ s'écrit comme combinaison linéaire des éléments de la base :\n\n$$ f(v_j) = 0 \\cdot v_1 + \\dots + \\lambda_j \\cdot v_j + \\dots + 0 \\cdot v_n $$\n\nLes coordonnées sont donc nulles partout sauf à la $j$-ème position, où la coordonnée est $\\lambda_j$.\n\n**Étape 3 : Construction de la matrice**\n\nLa matrice $A$ a donc la forme :\n\n$$ A = \\begin{pmatrix}\n\n\\lambda_1 & 0 & \\dots & 0 \\\\\n\n0 & \\lambda_2 & \\dots & 0 \\\\\n\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\n0 & 0 & \\dots & \\lambda_n\n\n\\end{pmatrix} $$\n\nLes termes hors de la diagonale sont nuls ($a_{ij} = 0$ pour $i \\neq j$) et les termes diagonaux sont les valeurs propres ($a_{jj} = \\lambda_j$).\n\n**Conclusion :**\n\nLa matrice est diagonale.\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "fce48ae1",
      "content": "#### Dimension d'une somme directe\n\nProuvez que si $E$ et $F$ sont deux sous-espaces de dimension finie en somme directe, alors $\\dim(E \\oplus F) = \\dim(E) + \\dim(F)$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nConsidérez une base $\\mathfrak{B}_E = \\{e_1, \\dots, e_p\\}$ de $E$ et une base $\\mathfrak{B}_F = \\{f_1, \\dots, f_q\\}$ de $F$.\n\nMontrez que la réunion de ces deux familles forme une base de $E \\oplus F$. Vous devrez prouver que cette réunion est génératrice et libre.\n\n</details>",
      "solution": "\n\nSoit $\\mathfrak{B}_E = \\{u_1, \\dots, u_p\\}$ une base de $E$ ($\\dim E = p$) et $\\mathfrak{B}_F = \\{v_1, \\dots, v_q\\}$ une base de $F$ ($\\dim F = q$).\n\nPosons $\\mathcal{B} = \\{u_1, \\dots, u_p, v_1, \\dots, v_q\\}$.\n\n**Étape 1 : Famille génératrice**\n\nSoit $w \\in E \\oplus F$. Par définition de la somme, $w = u + v$ avec $u \\in E$ et $v \\in F$.\n\n$u$ est combinaison linéaire de $\\mathfrak{B}_E$ et $v$ est combinaison linéaire de $\\mathfrak{B}_F$.\n\nDonc $w$ est combinaison linéaire de $\\mathcal{B}$. La famille engendre $E \\oplus F$.\n\n**Étape 2 : Famille libre**\n\nSupposons une combinaison linéaire nulle :\n\n$$ \\sum_{i=1}^p \\alpha_i u_i + \\sum_{j=1}^q \\beta_j v_j = 0_V $$\n\nPosons $x = \\sum \\alpha_i u_i \\in E$ et $y = \\sum \\beta_j v_j \\in F$.\n\nL'équation devient $x + y = 0_V$, soit $x = -y$.\n\nComme $x \\in E$ et $-y \\in F$ (car $F$ est un sous-espace), on a $x \\in E \\cap F$.\n\nLa somme étant directe, $E \\cap F = \\{0_V\\}$, donc $x = 0_V$ et $y = 0_V$.\n\nPuisque $x = \\sum \\alpha_i u_i = 0_V$ et que $\\mathfrak{B}_E$ est une base (donc libre), tous les $\\alpha_i = 0$.\n\nPuisque $y = \\sum \\beta_j v_j = 0_V$ et que $\\mathfrak{B}_F$ est une base, tous les $\\beta_j = 0$.\n\n**Conclusion :**\n\nLa famille $\\mathcal{B}$ est une base de $E \\oplus F$. Elle contient $p+q$ éléments.\n\nDonc $\\dim(E \\oplus F) = p + q = \\dim(E) + \\dim(F)$.\n\n",
      "options": []
    }
  ]
}