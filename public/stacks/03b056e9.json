{
  "info": {
    "id": "03b056e9",
    "title": "Groupes d'isométries - preuves (A)",
    "type": "proofs",
    "level": "regular",
    "chapter": "Groupes d'isométries",
    "course": "Géométrie",
    "tags": [
      "isométrie",
      "espace euclidien",
      "espace hermitien",
      "groupe orthogonal",
      "groupe unitaire",
      "produit vectoriel",
      "rotation"
    ],
    "count": 10
  },
  "cards": [
    {
      "id": "1",
      "stackId": "03b056e9",
      "content": "#### Équivalence entre la préservation de la norme et du produit scalaire\n\nProuver que pour un endomorphisme $f$ d'un espace euclidien $E$, $f$ est une isométrie (préserve la norme) si et seulement si $f$ préserve le produit scalaire.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour montrer que la préservation de la norme implique la préservation du produit scalaire, utilisez une des identités de polarisation, qui exprime le produit scalaire en termes de la norme. Par exemple, $\\varphi(x, y) = \\frac{1}{2}(\\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2)$.\n\nL'implication inverse est plus directe.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme de l'espace euclidien $(E, \\varphi)$. Nous devons prouver l'équivalence :\n\n$$ (\\forall x \\in E, \\|f(x)\\| = \\|x\\|) \\iff (\\forall x, y \\in E, \\varphi(f(x), f(y)) = \\varphi(x, y)) $$\n\n**Étape 1 : (Préservation du produit scalaire $\\implies$ Préservation de la norme)**\n\nSupposons que $f$ préserve le produit scalaire. Pour tout $x, y \\in E$, on a $\\varphi(f(x), f(y)) = \\varphi(x, y)$.\n\nPrenons $y=x$. On a alors $\\varphi(f(x), f(x)) = \\varphi(x, x)$.\n\nPar définition de la norme associée au produit scalaire, $\\|v\\|^2 = \\varphi(v,v)$.\n\nDonc, l'équation devient $\\|f(x)\\|^2 = \\|x\\|^2$.\n\nPuisque les normes sont des réels positifs, on peut prendre la racine carrée des deux côtés, ce qui donne $\\|f(x)\\| = \\|x\\|$.\n\nAinsi, $f$ préserve la norme.\n\n**Étape 2 : (Préservation de la norme $\\implies$ Préservation du produit scalaire)**\n\nSupposons que $f$ préserve la norme. Pour tout $v \\in E$, on a $\\|f(v)\\| = \\|v\\|$.\n\nNous utilisons l'identité de polarisation : $\\varphi(x, y) = \\frac{1}{2}(\\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2)$.\n\nAppliquons cette identité aux vecteurs images $f(x)$ et $f(y)$ :\n\n$$ \\varphi(f(x), f(y)) = \\frac{1}{2}(\\|f(x)+f(y)\\|^2 - \\|f(x)\\|^2 - \\|f(y)\\|^2) $$\n\nPuisque $f$ est un endomorphisme (linéaire), $f(x)+f(y) = f(x+y)$.\n\n$$ \\varphi(f(x), f(y)) = \\frac{1}{2}(\\|f(x+y)\\|^2 - \\|f(x)\\|^2 - \\|f(y)\\|^2) $$\n\nNous savons par hypothèse que $f$ préserve la norme, donc $\\|f(v)\\| = \\|v\\|$ pour tout $v \\in E$. Appliquons ceci pour $v=x+y$, $v=x$, et $v=y$ :\n\n$$ \\varphi(f(x), f(y)) = \\frac{1}{2}(\\|x+y\\|^2 - \\|x\\|^2 - \\|y\\|^2) $$\n\nOn reconnaît dans le membre de droite l'identité de polarisation pour $\\varphi(x, y)$.\n\n$$ \\varphi(f(x), f(y)) = \\varphi(x, y) $$\n\nAinsi, $f$ préserve le produit scalaire.\n\n**Conclusion :** Les deux propriétés sont bien équivalentes.\n\n",
      "options": []
    },
    {
      "id": "2",
      "stackId": "03b056e9",
      "content": "#### Caractérisation d'une isométrie par son adjoint\n\nProuver qu'un endomorphisme $f$ d'un espace euclidien ou hermitien $E$ est une isométrie si et seulement si $f^* \\circ f = \\text{Id}_E$, où $f^*$ est l'endomorphisme adjoint de $f$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nUtilisez la définition de l'application adjointe $f^*$, qui est : $\\forall x, y \\in E, \\varphi(f(x), y) = \\varphi(x, f^*(y))$.\n\nMontrez l'équivalence entre la condition $f^* \\circ f = \\text{Id}_E$ et la condition de préservation du produit scalaire $\\varphi(f(x), f(y)) = \\varphi(x, y)$.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme de $E$. Nous allons prouver l'équivalence en montrant les deux implications.\n\n**Étape 1 : ($f^* \\circ f = \\text{Id}_E \\implies f$ est une isométrie)**\n\nSupposons que $f^* \\circ f = \\text{Id}_E$. Cela signifie que pour tout $x \\in E$, $f^*(f(x)) = x$.\n\nConsidérons le produit scalaire $\\varphi(f(x), f(y))$ pour $x, y \\in E$ quelconques. Par définition de l'adjoint $f^*$, on peut écrire :\n\n$$ \\varphi(f(x), f(y)) = \\varphi(x, f^*(f(y))) $$\n\nEn utilisant notre hypothèse $f^*(f(y)) = y$, on obtient :\n\n$$ \\varphi(f(x), f(y)) = \\varphi(x, y) $$\n\nCeci montre que $f$ préserve le produit scalaire. D'après la propriété démontrée précédemment, un endomorphisme qui préserve le produit scalaire préserve aussi la norme, donc $f$ est une isométrie.\n\n**Étape 2 : ($f$ est une isométrie $\\implies f^* \\circ f = \\text{Id}_E$)**\n\nSupposons que $f$ est une isométrie. Cela signifie que $f$ préserve le produit scalaire, donc $\\forall x, y \\in E, \\varphi(f(x), f(y)) = \\varphi(x, y)$.\n\nPar définition de l'adjoint, on a aussi $\\varphi(f(x), f(y)) = \\varphi(x, f^*(f(y)))$.\n\nEn combinant ces deux égalités, on obtient :\n\n$$ \\varphi(x, y) = \\varphi(x, f^*(f(y))) $$\n\n$$ \\varphi(x, y) - \\varphi(x, f^*(f(y))) = 0 $$\n\nPar linéarité du produit scalaire par rapport à la deuxième variable (ou sesquilinéarité dans le cas hermitien) :\n\n$$ \\varphi(x, y - f^*(f(y))) = 0 $$\n\nCette égalité est vraie pour tout $x \\in E$. Le seul vecteur de $E$ qui est orthogonal à tous les vecteurs de $E$ est le vecteur nul (car le produit scalaire est non dégénéré). Donc, pour un $y$ fixé, on doit avoir :\n\n$$ y - f^*(f(y)) = 0 $$\n\n$$ y = f^*(f(y)) $$\n\nCeci étant vrai pour tout $y \\in E$, on conclut que l'application $f^* \\circ f$ est l'application identité $\\text{Id}_E$.\n\n**Conclusion :** Un endomorphisme $f$ est une isométrie si et seulement si $f^* \\circ f = \\text{Id}_E$. Cela implique aussi que toute isométrie en dimension finie est un isomorphisme (car elle est injective), et que son inverse est son adjoint : $f^{-1} = f^*$.\n\n",
      "options": []
    },
    {
      "id": "3",
      "stackId": "03b056e9",
      "content": "#### Image d'une base orthonormée par une isométrie\n\nProuver qu'un endomorphisme $f$ est une isométrie si et seulement si l'image par $f$ d'une base orthonormée est une base orthonormée.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoit $\\mathcal{B} = (e_1, \\dots, e_n)$ une base orthonormée. Pour prouver que son image $\\mathcal{B}' = (f(e_1), \\dots, f(e_n))$ est orthonormée, il faut montrer que $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$ (symbole de Kronecker).\n\nPour la réciproque, exprimez un vecteur $x$ quelconque dans la base $\\mathcal{B}$, $x = \\sum_{i=1}^n x_i e_i$. Calculez $\\|x\\|^2$ puis $\\|f(x)\\|^2$ et montrez qu'ils sont égaux.\n\n</details>",
      "solution": "\n\nSoit $f$ un endomorphisme de $E$, et $\\mathcal{B} = (e_1, \\dots, e_n)$ une base orthonormée de $E$.\n\n**Étape 1 : (Si $f$ est une isométrie, alors l'image de $\\mathcal{B}$ est une base orthonormée)**\n\nSupposons que $f$ est une isométrie. Alors $f$ préserve le produit scalaire.\n\nSoit $\\mathcal{B}' = (f(e_1), \\dots, f(e_n))$ l'image de la base $\\mathcal{B}$ par $f$. Pour montrer que $\\mathcal{B}'$ est une base orthonormée, nous devons vérifier que $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$ pour tous $i, j \\in \\{1, \\dots, n\\}$.\n\nPuisque $f$ préserve le produit scalaire, on a :\n\n$$ \\varphi(f(e_i), f(e_j)) = \\varphi(e_i, e_j) $$\n\nComme $\\mathcal{B}$ est une base orthonormée, on sait que $\\varphi(e_i, e_j) = \\delta_{ij}$.\n\nDonc, $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$.\n\nCeci prouve que la famille $\\mathcal{B}'$ est une famille orthonormée. Une famille orthonormée de $n$ vecteurs dans un espace de dimension $n$ est toujours une base. Donc $\\mathcal{B}'$ est une base orthonormée.\n\n**Étape 2 : (Si l'image d'une base orthonormée est une base orthonormée, alors $f$ est une isométrie)**\n\nSupposons qu'il existe une base orthonormée $\\mathcal{B}=(e_1, \\dots, e_n)$ telle que son image $\\mathcal{B}' = (f(e_1), \\dots, f(e_n))$ soit aussi une base orthonormée.\n\nOn a donc $\\varphi(f(e_i), f(e_j)) = \\delta_{ij}$.\n\nSoit $x$ un vecteur quelconque de $E$. On peut le décomposer dans la base $\\mathcal{B}$ : $x = \\sum_{i=1}^n x_i e_i$.\n\nLa norme au carré de $x$ est donnée par (puisque $\\mathcal{B}$ est orthonormée) :\n\n$$ \\|x\\|^2 = \\varphi\\left(\\sum_{i=1}^n x_i e_i, \\sum_{j=1}^n x_j e_j\\right) = \\sum_{i,j} x_i x_j \\varphi(e_i, e_j) = \\sum_{i,j} x_i x_j \\delta_{ij} = \\sum_{i=1}^n x_i^2 $$\n\nMaintenant, calculons l'image de $x$ par $f$. Par linéarité de $f$ :\n\n$$ f(x) = f\\left(\\sum_{i=1}^n x_i e_i\\right) = \\sum_{i=1}^n x_i f(e_i) $$\n\nCalculons la norme au carré de $f(x)$. Comme la base $(f(e_1), \\dots, f(e_n))$ est orthonormée par hypothèse, le calcul est similaire :\n\n$$ \\|f(x)\\|^2 = \\varphi\\left(\\sum_{i=1}^n x_i f(e_i), \\sum_{j=1}^n x_j f(e_j)\\right) = \\sum_{i,j} x_i x_j \\varphi(f(e_i), f(e_j)) = \\sum_{i,j} x_i x_j \\delta_{ij} = \\sum_{i=1}^n x_i^2 $$\n\nNous avons donc $\\|x\\|^2 = \\sum_{i=1}^n x_i^2$ et $\\|f(x)\\|^2 = \\sum_{i=1}^n x_i^2$.\n\nIl s'ensuit que $\\|f(x)\\|^2 = \\|x\\|^2$, et donc $\\|f(x)\\| = \\|x\\|$.\n\nCeci étant vrai pour tout $x \\in E$, $f$ est une isométrie.\n\n",
      "options": []
    },
    {
      "id": "4",
      "stackId": "03b056e9",
      "content": "#### Le groupe des isométries $O(E)$\n\nProuver que l'ensemble des isométries d'un espace euclidien $E$, noté $O(E)$, muni de la composition d'applications $(\\circ)$, forme un groupe.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour prouver qu'un ensemble muni d'une loi de composition est un groupe, il faut vérifier trois axiomes :\n\n1.  **Stabilité :** Si $f, g \\in O(E)$, est-ce que $f \\circ g \\in O(E)$ ?\n2.  **Élément neutre :** L'application identité est-elle une isométrie ?\n3.  **Inverse :** Si $f \\in O(E)$, son inverse $f^{-1}$ existe-t-il et est-il aussi une isométrie ? (Rappel : en dimension finie, une application linéaire injective est bijective).\n\n</details>",
      "solution": "\n\nPour montrer que $(O(E), \\circ)$ est un groupe, nous devons vérifier les trois axiomes de la structure de groupe.\n\n**1. Stabilité (ou loi de composition interne)**\n\nSoient $f$ et $g$ deux isométries de $E$. Nous devons montrer que leur composée $f \\circ g$ est aussi une isométrie.\n\nPour tout $x \\in E$, nous devons vérifier que $\\|(f \\circ g)(x)\\| = \\|x\\|$.\n\n$$ \\|(f \\circ g)(x)\\| = \\|f(g(x))\\| $$\n\nPuisque $f$ est une isométrie, $\\|f(y)\\| = \\|y\\|$ pour tout $y \\in E$. Appliquons ceci avec $y=g(x)$:\n\n$$ \\|f(g(x))\\| = \\|g(x)\\| $$\n\nPuisque $g$ est une isométrie, $\\|g(x)\\| = \\|x\\|$.\n\nEn combinant les deux, on obtient $\\|(f \\circ g)(x)\\| = \\|x\\|$.\n\nLa composition est donc bien une loi interne sur $O(E)$.\n\n**2. Élément neutre**\n\nL'élément neutre pour la composition d'applications est l'application identité, $\\text{Id}_E$, définie par $\\text{Id}_E(x) = x$ pour tout $x \\in E$.\n\nVérifions si $\\text{Id}_E$ est une isométrie :\n\n$$ \\|\\text{Id}_E(x)\\| = \\|x\\| $$\n\nCeci est vrai par définition. Donc, $\\text{Id}_E \\in O(E)$.\n\n**3. Existence de l'inverse**\n\nSoit $f \\in O(E)$. Nous devons montrer que $f$ est bijective et que son inverse $f^{-1}$ est aussi une isométrie.\n\n*   **Bijectivité :** Pour montrer que $f$ est bijective, il suffit de montrer qu'elle est injective, car $f$ est un endomorphisme en dimension finie. Pour montrer l'injectivité, on montre que son noyau est réduit au vecteur nul. Soit $x \\in \\ker(f)$, alors $f(x) = 0$. On a $\\|f(x)\\| = \\|0\\| = 0$. Comme $f$ est une isométrie, $\\|f(x)\\| = \\|x\\|$. Donc $\\|x\\|=0$, ce qui implique $x=0$. Le noyau est donc $\\ker(f) = \\{0\\}$, et $f$ est injective, donc bijective. L'inverse $f^{-1}$ existe.\n*   **$f^{-1}$ est une isométrie :** Nous devons montrer que $\\|f^{-1}(y)\\| = \\|y\\|$ pour tout $y \\in E$.\n\nSoit $y \\in E$. Puisque $f$ est surjective, il existe un $x \\in E$ tel que $y = f(x)$. Alors, par définition de l'inverse, $x = f^{-1}(y)$.\n\nOn a $\\|y\\| = \\|f(x)\\|$.\n\nPuisque $f$ est une isométrie, $\\|f(x)\\| = \\|x\\|$.\n\nDonc $\\|y\\| = \\|x\\|$.\n\nEn substituant $x = f^{-1}(y)$, on obtient $\\|y\\| = \\|f^{-1}(y)\\|$.\n\nCeci montre que $f^{-1}$ est une isométrie. Donc $f^{-1} \\in O(E)$.\n\n**Conclusion :** Les trois axiomes étant vérifiés, $(O(E), \\circ)$ est un groupe.\n\n",
      "options": []
    },
    {
      "id": "5",
      "stackId": "03b056e9",
      "content": "#### Structure de groupe de $O_n(\\mathbb{R})$\n\nProuver que l'ensemble des matrices orthogonales $O_n(\\mathbb{R}) = \\{ M \\in M_n(\\mathbb{R}) \\mid {}^tMM = I_n \\}$ est un groupe pour la multiplication matricielle.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nVérifiez les trois axiomes de groupe :\n\n1.  **Stabilité :** Si $A, B \\in O_n(\\mathbb{R})$, est-ce que $AB \\in O_n(\\mathbb{R})$ ? Calculez ${}^t(AB)(AB)$ et utilisez la propriété ${}^t(AB) = {}^tB{}^tA$.\n2.  **Élément neutre :** La matrice identité $I_n$ est-elle dans $O_n(\\mathbb{R})$ ?\n3.  **Inverse :** Si $A \\in O_n(\\mathbb{R})$, son inverse $A^{-1}$ existe-t-il et est-il dans $O_n(\\mathbb{R})$ ? La définition ${}^tAA=I_n$ donne une expression simple pour $A^{-1}$.\n\n</details>",
      "solution": "\n\nPour montrer que $(O_n(\\mathbb{R}), \\times)$ est un groupe, nous vérifions les trois axiomes.\n\n**1. Stabilité par multiplication**\n\nSoient $A, B \\in O_n(\\mathbb{R})$. Par définition, nous avons ${}^tAA = I_n$ et ${}^tBB = I_n$.\n\nNous devons montrer que le produit $AB$ est aussi dans $O_n(\\mathbb{R})$, c'est-à-dire que ${}^t(AB)(AB) = I_n$.\n\n$$ {}^t(AB)(AB) = ({}^tB{}^tA)(AB) $$\n\nPar associativité de la multiplication matricielle :\n\n$$ {}^tB({}^tA A)B $$\n\nPuisque $A \\in O_n(\\mathbb{R})$, on a ${}^tAA = I_n$.\n\n$$ {}^tB(I_n)B = {}^tBB $$\n\nEt puisque $B \\in O_n(\\mathbb{R})$, on a ${}^tBB = I_n$.\n\nDonc, ${}^t(AB)(AB) = I_n$, ce qui prouve que $AB \\in O_n(\\mathbb{R})$.\n\n**2. Élément neutre**\n\nL'élément neutre pour la multiplication matricielle est la matrice identité $I_n$.\n\nNous devons vérifier si $I_n \\in O_n(\\mathbb{R})$. On calcule :\n\n$$ {}^tI_n I_n = I_n I_n = I_n $$\n\nLa condition est vérifiée, donc $I_n \\in O_n(\\mathbb{R})$.\n\n**3. Existence de l'inverse**\n\nSoit $A \\in O_n(\\mathbb{R})$. Par définition, ${}^tAA = I_n$.\n\nCette relation montre que $A$ est inversible et que son inverse est $A^{-1} = {}^tA$.\n\nIl nous reste à vérifier que cet inverse $A^{-1}$ est bien dans $O_n(\\mathbb{R})$.\n\nNous devons donc montrer que ${}^t(A^{-1})(A^{-1}) = I_n$.\n\nSubstituons $A^{-1}$ par ${}^tA$ :\n\n$$ {}^t({}^tA)({}^tA) = A({}^tA) $$\n\nNous savons que ${}^tAA = I_n$. En multipliant à droite par $A^{-1}$ (qui est ${}^tA$), on obtient ${}^tA = A^{-1}$. Et en multipliant ${}^tAA=I_n$ à gauche par $A$, on obtient $A({}^tA A) = A I_n$, donc $(A{}^tA)A=A$. Comme A est inversible, on peut multiplier par $A^{-1}$ à droite pour obtenir $A{}^tA = I_n$.\n\nAinsi, ${}^t(A^{-1})(A^{-1}) = I_n$, et donc $A^{-1} \\in O_n(\\mathbb{R})$.\n\n**Conclusion :** Les trois axiomes étant satisfaits, $O_n(\\mathbb{R})$ est un groupe pour la multiplication matricielle.\n\n",
      "options": []
    },
    {
      "id": "6",
      "stackId": "03b056e9",
      "content": "#### Déterminant d'une matrice orthogonale\n\nProuver que si $M \\in O_n(\\mathbb{R})$, alors $\\det(M) = \\pm 1$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPartez de l'équation qui définit une matrice orthogonale : ${}^tMM = I_n$.\n\nAppliquez la fonction déterminant à chaque côté de l'équation. Utilisez ensuite deux propriétés fondamentales du déterminant : $\\det(AB) = \\det(A)\\det(B)$ et $\\det({}^tM) = \\det(M)$.\n\n</details>",
      "solution": "\n\nSoit $M$ une matrice appartenant au groupe orthogonal $O_n(\\mathbb{R})$.\n\n**Étape 1 : Utiliser la définition de $O_n(\\mathbb{R})$**\n\nPar définition, $M$ satisfait la relation :\n\n$$ {}^tMM = I_n $$\n\noù $I_n$ est la matrice identité d'ordre $n$.\n\n**Étape 2 : Appliquer le déterminant**\n\nAppliquons la fonction déterminant aux deux membres de cette équation :\n\n$$ \\det({}^tMM) = \\det(I_n) $$\n\n**Étape 3 : Utiliser les propriétés du déterminant**\n\nNous utilisons deux propriétés du déterminant :\n\n1.  Le déterminant d'un produit de matrices est le produit de leurs déterminants : $\\det(AB) = \\det(A)\\det(B)$.\n2.  Le déterminant d'une matrice transposée est égal au déterminant de la matrice originale : $\\det({}^tM) = \\det(M)$.\n\nAppliquons la première propriété au membre de gauche :\n\n$$ \\det({}^tM)\\det(M) = \\det(I_n) $$\n\nAppliquons la seconde propriété :\n\n$$ \\det(M)\\det(M) = \\det(I_n) $$\n\n$$ (\\det(M))^2 = \\det(I_n) $$\n\n**Étape 4 : Calculer le déterminant de l'identité et résoudre**\n\nLe déterminant de la matrice identité est 1.\n\n$$ (\\det(M))^2 = 1 $$\n\nL'équation $x^2 = 1$ pour une variable réelle $x = \\det(M)$ a exactement deux solutions : $x=1$ et $x=-1$.\n\n**Conclusion :**\n\nLe déterminant d'une matrice orthogonale $M$ ne peut prendre que les valeurs 1 ou -1.\n\n$$ \\det(M) \\in \\{-1, 1\\} $$\n\n",
      "options": []
    },
    {
      "id": "7",
      "stackId": "03b056e9",
      "content": "#### Caractérisation des matrices orthogonales par leurs colonnes\n\nProuver qu'une matrice $M \\in M_n(\\mathbb{R})$ est orthogonale si et seulement si ses vecteurs colonnes forment une base orthonormée de $\\mathbb{R}^n$ pour le produit scalaire canonique.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nNotez $C_1, C_2, \\dots, C_n$ les vecteurs colonnes de $M$. La matrice transposée ${}^tM$ a pour lignes les transposées de ces vecteurs, c'est-à-dire ${}^tC_1, {}^tC_2, \\dots, {}^tC_n$.\n\nCalculez le produit matriciel $P = {}^tMM$. Exprimez l'élément $P_{ij}$ (à la ligne $i$ et colonne $j$ de $P$) en fonction des colonnes de $M$. Rappelez-vous que le produit scalaire canonique de deux vecteurs colonnes $U$ et $V$ est ${}^tUV$.\n\nComparez la condition \"$P = I_n$\" avec la définition d'une base orthonormée.\n\n</details>",
      "solution": "\n\nSoit $M$ une matrice de $M_n(\\mathbb{R})$. Notons ses vecteurs colonnes $C_1, C_2, \\dots, C_n$, où chaque $C_j \\in \\mathbb{R}^n$.\n\n$$ M = \\begin{pmatrix} | & | & & | \\\\ C_1 & C_2 & \\dots & C_n \\\\ | & | & & | \\end{pmatrix} $$\n\nLa matrice transposée ${}^tM$ a pour lignes les transposées de ces vecteurs colonnes :\n\n$$ {}^tM = \\begin{pmatrix} - & {}^tC_1 & - \\\\ - & {}^tC_2 & - \\\\ & \\vdots & \\\\ - & {}^tC_n & - \\end{pmatrix} $$\n\n**Étape 1 : Calculer le produit ${}^tMM$**\n\nEffectuons le produit matriciel $P = {}^tMM$. L'élément $P_{ij}$ situé à la $i$-ème ligne et $j$-ème colonne de $P$ est obtenu en multipliant la $i$-ème ligne de ${}^tM$ par la $j$-ème colonne de $M$.\n\nLa $i$-ème ligne de ${}^tM$ est ${}^tC_i$.\n\nLa $j$-ème colonne de $M$ est $C_j$.\n\nAinsi, l'élément $P_{ij}$ est le produit matriciel ${}^tC_i C_j$. Ce produit est une matrice $1 \\times 1$, que l'on identifie à un scalaire. Il correspond exactement au produit scalaire canonique de $C_i$ et $C_j$.\n\n$$ P_{ij} = {}^tC_i C_j = \\varphi(C_i, C_j) $$\n\nLa matrice produit est donc :\n\n$$ {}^tMM = \\begin{pmatrix} \\varphi(C_1, C_1) & \\varphi(C_1, C_2) & \\dots & \\varphi(C_1, C_n) \\\\ \\varphi(C_2, C_1) & \\varphi(C_2, C_2) & \\dots & \\varphi(C_2, C_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\varphi(C_n, C_1) & \\varphi(C_n, C_2) & \\dots & \\varphi(C_n, C_n) \\end{pmatrix} $$\n\n**Étape 2 : Relier la condition d'orthogonalité à la condition sur les colonnes**\n\nPar définition, $M$ est une matrice orthogonale si et seulement si ${}^tMM = I_n$.\n\nLa matrice identité $I_n$ est la matrice dont les coefficients $(I_n)_{ij}$ sont donnés par le symbole de Kronecker $\\delta_{ij}$ :\n\n$$ (I_n)_{ij} = \\delta_{ij} = \\begin{cases} 1 & \\text{si } i=j \\\\ 0 & \\text{si } i \\neq j \\end{cases} $$\n\nEn identifiant les coefficients de la matrice ${}^tMM$ avec ceux de $I_n$, la condition ${}^tMM = I_n$ est équivalente à :\n\n$$ \\varphi(C_i, C_j) = \\delta_{ij} \\quad \\text{pour tous } i, j \\in \\{1, \\dots, n\\} $$\n\n**Étape 3 : Interpréter la condition sur les colonnes**\n\nLa condition $\\varphi(C_i, C_j) = \\delta_{ij}$ signifie que :\n\n- Si $i=j$, $\\varphi(C_i, C_i) = \\|C_i\\|^2 = 1$, donc les vecteurs colonnes sont de norme 1 (unitaires).\n- Si $i \\neq j$, $\\varphi(C_i, C_j) = 0$, donc deux vecteurs colonnes distincts sont orthogonaux.\n\nL'ensemble de ces conditions est précisément la définition d'une famille orthonormée. Une famille orthonormée de $n$ vecteurs dans un espace de dimension $n$ comme $\\mathbb{R}^n$ forme une base.\n\n**Conclusion :**\n\nLa matrice $M$ est orthogonale si et seulement si la famille de ses vecteurs colonnes $(C_1, \\dots, C_n)$ est une base orthonormée de $\\mathbb{R}^n$.\n\n",
      "options": []
    },
    {
      "id": "8",
      "stackId": "03b056e9",
      "content": "#### Justification de la définition de l'angle non-orienté\n\nProuver que pour deux vecteurs non nuls $x, y$ d'un espace euclidien $E$, la quantité $\\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|}$ est bien définie et appartient toujours à l'intervalle $[-1, 1]$, justifiant ainsi l'utilisation de la fonction $\\arccos$.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nLe point crucial de cette preuve est l'inégalité de Cauchy-Schwarz. Elle stipule que pour tous vecteurs $x, y$ d'un espace euclidien, on a :\n\n$$ |\\varphi(x,y)| \\le \\|x\\|\\|y\\| $$\n\nComment cette inégalité permet-elle d'encadrer la fraction en question ?\n\n</details>",
      "solution": "\n\nSoit $(E, \\varphi)$ un espace euclidien, et soient $x, y$ deux vecteurs non nuls de $E$.\n\nLa définition de l'angle non-orienté $\\theta$ entre $x$ et $y$ est donnée par la relation $\\cos(\\theta) = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|}$.\n\nPour que cette définition soit mathématiquement valide, il faut que l'argument de la fonction $\\arccos$ (qui est $\\cos(\\theta)$) soit toujours dans son domaine de définition, c'est-à-dire l'intervalle $[-1, 1]$.\n\n**Étape 1 : S'assurer que l'expression est bien définie**\n\nLes vecteurs $x$ et $y$ sont supposés non nuls. Par conséquent, leurs normes $\\|x\\|$ et $\\|y\\|$ sont strictement positives. Le dénominateur $\\|x\\|\\|y\\|$ n'est donc jamais nul, et la fraction est bien définie.\n\n**Étape 2 : Appliquer l'inégalité de Cauchy-Schwarz**\n\nL'inégalité de Cauchy-Schwarz est une propriété fondamentale de tout produit scalaire. Elle s'énonce ainsi :\n\n$$ |\\varphi(x, y)| \\leq \\|x\\| \\|y\\| $$\n\noù $|\\cdot|$ désigne la valeur absolue.\n\n**Étape 3 : Manipuler l'inégalité**\n\nPuisque $\\|x\\|\\|y\\|$ est un nombre réel strictement positif, nous pouvons diviser les deux membres de l'inégalité par cette quantité sans changer le sens de l'inégalité :\n\n$$ \\frac{|\\varphi(x, y)|}{\\|x\\| \\|y\\|} \\leq 1 $$\n\nEn utilisant la propriété $|a/b| = |a|/|b|$, on peut réécrire le membre de gauche :\n\n$$ \\left| \\frac{\\varphi(x, y)}{\\|x\\| \\|y\\|} \\right| \\leq 1 $$\n\n**Étape 4 : Conclure**\n\nL'inégalité $|A| \\leq 1$ est équivalente à $-1 \\leq A \\leq 1$.\n\nEn posant $A = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|}$, nous avons donc :\n\n$$ -1 \\leq \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|} \\leq 1 $$\n\n**Conclusion :**\n\nLa quantité $\\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|}$ est toujours un nombre réel compris dans l'intervalle $[-1, 1]$. Par conséquent, il existe un unique angle $\\theta$ dans l'intervalle $[0, \\pi]$ tel que $\\cos(\\theta)$ soit égal à cette valeur. La définition de l'angle non-orienté par la fonction $\\arccos$ est donc bien fondée.\n\n",
      "options": []
    },
    {
      "id": "9",
      "stackId": "03b056e9",
      "content": "#### Préservation des angles par les isométries\n\nProuver qu'une isométrie $f$ d'un espace euclidien préserve l'angle non-orienté entre deux vecteurs non nuls.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nSoient $x, y$ deux vecteurs non nuls. L'angle $\\theta(x,y)$ est défini par $\\cos(\\theta(x,y)) = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|}$.\n\nPour montrer que l'angle est préservé, il suffit de montrer que $\\cos(\\theta(f(x),f(y))) = \\cos(\\theta(x,y))$.\n\nExprimez $\\cos(\\theta(f(x),f(y)))$ en utilisant la définition et simplifiez l'expression en vous servant des propriétés d'une isométrie.\n\n</details>",
      "solution": "\n\nSoit $f$ une isométrie d'un espace euclidien $E$. Soient $x, y$ deux vecteurs non nuls de $E$.\n\nL'angle non-orienté $\\theta(x,y)$ entre $x$ et $y$ est l'unique réel dans $[0, \\pi]$ tel que :\n\n$$ \\cos(\\theta(x,y)) = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|} $$\n\nNous voulons prouver que l'angle entre les images $f(x)$ et $f(y)$ est le même, c'est-à-dire $\\theta(f(x),f(y)) = \\theta(x,y)$.\n\n**Étape 1 : Définir l'angle entre les vecteurs images**\n\nSi $x$ et $y$ sont non nuls, et comme $f$ est une isométrie (donc injective), leurs images $f(x)$ et $f(y)$ sont aussi non nulles. On peut donc définir l'angle $\\theta(f(x),f(y))$ entre elles :\n\n$$ \\cos(\\theta(f(x),f(y))) = \\frac{\\varphi(f(x),f(y))}{\\|f(x)\\|\\|f(y)\\|} $$\n\n**Étape 2 : Utiliser les propriétés de l'isométrie**\n\nUne isométrie $f$ a deux propriétés clés que nous allons utiliser :\n\n1.  Elle préserve le produit scalaire : $\\varphi(f(x),f(y)) = \\varphi(x,y)$.\n2.  Elle préserve la norme : $\\|f(v)\\| = \\|v\\|$ pour tout vecteur $v$.\n\nAppliquons ces propriétés à l'expression de $\\cos(\\theta(f(x),f(y)))$.\n\nLe numérateur devient :\n\n$$ \\varphi(f(x),f(y)) = \\varphi(x,y) $$\n\nLe dénominateur devient :\n\n$$ \\|f(x)\\|\\|f(y)\\| = \\|x\\|\\|y\\| $$\n\n**Étape 3 : Comparer les cosinus**\n\nEn substituant ces résultats dans la formule, on obtient :\n\n$$ \\cos(\\theta(f(x),f(y))) = \\frac{\\varphi(x,y)}{\\|x\\|\\|y\\|} $$\n\nOn reconnaît dans le membre de droite l'expression de $\\cos(\\theta(x,y))$.\n\n$$ \\cos(\\theta(f(x),f(y))) = \\cos(\\theta(x,y)) $$\n\n**Étape 4 : Conclure sur l'égalité des angles**\n\nLa fonction cosinus est injective sur l'intervalle $[0, \\pi]$. Puisque par définition, les angles non-orientés $\\theta(x,y)$ et $\\theta(f(x),f(y))$ appartiennent tous deux à cet intervalle, l'égalité de leurs cosinus implique l'égalité des angles eux-mêmes.\n\n$$ \\theta(f(x),f(y)) = \\theta(x,y) $$\n\n**Conclusion :** Toute isométrie préserve l'angle non-orienté entre deux vecteurs.\n\n",
      "options": []
    },
    {
      "id": "10",
      "stackId": "03b056e9",
      "content": "#### L'orientation comme relation d'équivalence\n\nProuver que la relation $\\mathcal{R}$ définie sur l'ensemble des bases d'un $\\mathbb{R}$-espace vectoriel $E$ par \"$\\mathcal{B} \\mathcal{R} \\mathcal{B}' \\iff \\det(P_{\\mathcal{B},\\mathcal{B}'}) > 0$\" est une relation d'équivalence.\n\n<details class=\"hint\">\n\n<summary>Indice</summary>\n\nPour prouver que $\\mathcal{R}$ est une relation d'équivalence, vous devez vérifier les trois propriétés suivantes :\n\n1.  **Réflexivité :** $\\mathcal{B} \\mathcal{R} \\mathcal{B}$. Quelle est la matrice de passage $P_{\\mathcal{B},\\mathcal{B}}$ de la base $\\mathcal{B}$ à elle-même ? Quel est son déterminant ?\n2.  **Symétrie :** Si $\\mathcal{B} \\mathcal{R} \\mathcal{B}'$, a-t-on $\\mathcal{B}' \\mathcal{R} \\mathcal{B}$ ? Utilisez la relation $P_{\\mathcal{B}',\\mathcal{B}} = (P_{\\mathcal{B},\\mathcal{B}'})^{-1}$ et la propriété du déterminant d'une inverse.\n3.  **Transitivité :** Si $\\mathcal{B} \\mathcal{R} \\mathcal{B}'$ et $\\mathcal{B}' \\mathcal{R} \\mathcal{B}''$, a-t-on $\\mathcal{B} \\mathcal{R} \\mathcal{B}''$ ? Utilisez la formule de Chasles pour les matrices de passage : $P_{\\mathcal{B},\\mathcal{B}''} = P_{\\mathcal{B},\\mathcal{B}'} P_{\\mathcal{B}',\\mathcal{B}''}$.\n\n</details>",
      "solution": "\n\nSoit $E$ un $\\mathbb{R}$-espace vectoriel de dimension finie. Soit $\\mathcal{E}$ l'ensemble de toutes les bases de $E$. La relation $\\mathcal{R}$ est définie sur $\\mathcal{E}$ par :\n\n$$ \\forall \\mathcal{B}, \\mathcal{B}' \\in \\mathcal{E}, \\quad \\mathcal{B} \\mathcal{R} \\mathcal{B}' \\iff \\det(P_{\\mathcal{B},\\mathcal{B}'}) > 0 $$\n\noù $P_{\\mathcal{B},\\mathcal{B}'}$ est la matrice de passage de la base $\\mathcal{B}$ à la base $\\mathcal{B}'$.\n\n**1. Réflexivité**\n\nNous devons montrer que $\\mathcal{B} \\mathcal{R} \\mathcal{B}$ pour toute base $\\mathcal{B}$.\n\nLa matrice de passage de la base $\\mathcal{B}$ à elle-même est la matrice identité $I_n$.\n\n$$ P_{\\mathcal{B},\\mathcal{B}} = I_n $$\n\nLe déterminant de la matrice identité est 1.\n\n$$ \\det(P_{\\mathcal{B},\\mathcal{B}}) = \\det(I_n) = 1 $$\n\nPuisque $1 > 0$, la condition est satisfaite. Donc, $\\mathcal{B} \\mathcal{R} \\mathcal{B}$. La relation est réflexive.\n\n**2. Symétrie**\n\nSupposons que $\\mathcal{B} \\mathcal{R} \\mathcal{B}'$. Nous devons montrer que $\\mathcal{B}' \\mathcal{R} \\mathcal{B}$.\n\nL'hypothèse $\\mathcal{B} \\mathcal{R} \\mathcal{B}'$ signifie que $\\det(P_{\\mathcal{B},\\mathcal{B}'}) > 0$.\n\nLa matrice de passage de $\\mathcal{B}'$ à $\\mathcal{B}$ est l'inverse de la matrice de passage de $\\mathcal{B}$ à $\\mathcal{B}'$ :\n\n$$ P_{\\mathcal{B}',\\mathcal{B}} = (P_{\\mathcal{B},\\mathcal{B}'})^{-1} $$\n\nLe déterminant de l'inverse d'une matrice est l'inverse du déterminant de la matrice :\n\n$$ \\det(P_{\\mathcal{B}',\\mathcal{B}}) = \\det((P_{\\mathcal{B},\\mathcal{B}'})^{-1}) = \\frac{1}{\\det(P_{\\mathcal{B},\\mathcal{B}'})} $$\n\nPar hypothèse, $\\det(P_{\\mathcal{B},\\mathcal{B}'})$ est un nombre réel strictement positif. L'inverse d'un réel strictement positif est aussi un réel strictement positif.\n\nDonc, $\\det(P_{\\mathcal{B}',\\mathcal{B}}) > 0$.\n\nCeci montre que $\\mathcal{B}' \\mathcal{R} \\mathcal{B}$. La relation est symétrique.\n\n**3. Transitivité**\n\nSupposons que $\\mathcal{B} \\mathcal{R} \\mathcal{B}'$ et $\\mathcal{B}' \\mathcal{R} \\mathcal{B}''$. Nous devons montrer que $\\mathcal{B} \\mathcal{R} \\mathcal{B}''$.\n\nLes hypothèses se traduisent par :\n\n1.  $\\det(P_{\\mathcal{B},\\mathcal{B}'}) > 0$\n2.  $\\det(P_{\\mathcal{B}',\\mathcal{B}''}) > 0$\n\nLa formule de changement de base (relation de Chasles) nous dit que :\n\n$$ P_{\\mathcal{B},\\mathcal{B}''} = P_{\\mathcal{B},\\mathcal{B}'} P_{\\mathcal{B}',\\mathcal{B}''} $$\n\nEn utilisant la propriété $\\det(AB) = \\det(A)\\det(B)$, on a :\n\n$$ \\det(P_{\\mathcal{B},\\mathcal{B}''}) = \\det(P_{\\mathcal{B},\\mathcal{B}'}) \\times \\det(P_{\\mathcal{B}',\\mathcal{B}''}) $$\n\nPar hypothèse, nous multiplions deux nombres réels strictement positifs. Le produit de deux nombres réels strictement positifs est un nombre réel strictement positif.\n\nDonc, $\\det(P_{\\mathcal{B},\\mathcal{B}''}) > 0$.\n\nCeci montre que $\\mathcal{B} \\mathcal{R} \\mathcal{B}''$. La relation est transitive.\n\n**Conclusion :**\n\nLa relation $\\mathcal{R}$ est réflexive, symétrique et transitive. C'est donc une relation d'équivalence sur l'ensemble des bases de $E$.\n\n",
      "options": []
    }
  ]
}