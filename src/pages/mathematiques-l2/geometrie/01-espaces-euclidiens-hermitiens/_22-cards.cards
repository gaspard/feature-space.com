---
title: Fiches de révision (B)
order: 21
level: pro
chapter: B - Concepts
course: Géométrie
tags: ["cards", "flashcards", "pro"]
---

# Cartes: B - Concepts

---

Énoncez et commentez la signification du théorème de Fréchet-von Neumann-Jordan concernant la relation entre normes et produits scalaires.

<details>

<summary>Réponse</summary>

**Théorème de Fréchet-von Neumann-Jordan :**

Une norme $\|\cdot\|$ sur un $\mathbb{K}$-espace vectoriel $E$ (où $\mathbb{K} = \mathbb{R}$ ou $\mathbb{C}$) dérive d'un produit scalaire si et seulement si elle satisfait l'**identité du parallélogramme** pour tous $x, y \in E$ :

$$ \|x+y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2) $$

**Signification et Implications :**

Ce théorème fournit une caractérisation géométrique simple et puissante des espaces préhilbertiens parmi tous les espaces vectoriels normés.

1.  **Critère de "Hilbertianité"** : Il permet de déterminer si un espace de Banach (espace normé complet) est en fait un espace de Hilbert. Si la norme vérifie l'identité du parallélogramme, alors l'espace est un espace de Hilbert.

2.  **Unicité et Reconstruction** : Si la norme vérifie cette identité, non seulement un produit scalaire existe, mais il est unique. Il peut être reconstruit explicitement à l'aide des **identités de polarisation**. Par exemple, dans le cas euclidien :

    $$ \langle x, y \rangle = \frac{1}{4} \left( \|x+y\|^2 - \|x-y\|^2 \right) $$

3.  **Interprétation Géométrique** : L'identité elle-même a une signification géométrique claire : la somme des carrés des longueurs des diagonales d'un parallélogramme est égale à la somme des carrés des longueurs de ses quatre côtés. Le théorème établit que cette propriété purement géométrique est la condition nécessaire et suffisante pour qu'une structure de norme soit compatible avec une structure de produit scalaire (qui définit les angles et l'orthogonalité).

**Exemple d'application du critère :** L'espace $L_p(\Omega)$ pour $p \ne 2$ n'est pas un espace de Hilbert car sa norme ne vérifie pas l'identité du parallélogramme.

</details>

---

Fournissez une preuve détaillée de l'inégalité de Cauchy-Schwarz dans un espace hermitien $(E, \langle \cdot, \cdot \rangle)$, en précisant la condition d'égalité.

<details>

<summary>Réponse</summary>

**Énoncé :** Pour tous vecteurs $x, y \in E$, on a $|\langle x, y \rangle| \le \|x\| \|y\|$. L'égalité a lieu si et seulement si $x$ et $y$ sont colinéaires.

**Preuve :**

Soient $x, y \in E$.

Si $y = 0_E$, l'inégalité est triviale car $\langle x, 0_E \rangle = 0$ et $\|x\|\|0_E\| = 0$. On a $0 \le 0$.

Supposons $y \ne 0_E$. Pour tout scalaire $\lambda \in \mathbb{C}$, la positivité de la norme issue du produit scalaire implique :

$$ 0 \le \|x - \lambda y\|^2 $$

Développons le produit scalaire :

$$ \|x - \lambda y\|^2 = \langle x - \lambda y, x - \lambda y \rangle = \langle x, x \rangle - \langle x, \lambda y \rangle - \langle \lambda y, x \rangle + \langle \lambda y, \lambda y \rangle $$

$$ = \|x\|^2 - \bar{\lambda} \langle x, y \rangle - \lambda \langle y, x \rangle + \lambda \bar{\lambda} \|y\|^2 $$

$$ = \|x\|^2 - \bar{\lambda} \langle x, y \rangle - \lambda \overline{\langle x, y \rangle} + |\lambda|^2 \|y\|^2 $$

Cette expression est un réel positif pour tout $\lambda \in \mathbb{C}$. L'idée est de choisir un $\lambda$ qui minimise cette expression. La minimisation d'un polynôme du second degré est une approche, mais un choix plus direct et élégant est de poser :

$$ \lambda = \frac{\langle x, y \rangle}{\|y\|^2} $$

Ce choix est possible car $y \ne 0_E$, donc $\|y\|^2 \ne 0$.

En substituant cette valeur de $\lambda$ :

$$ 0 \le \|x\|^2 - \frac{\overline{\langle x, y \rangle}}{\|y\|^2} \langle x, y \rangle - \frac{\langle x, y \rangle}{\|y\|^2} \overline{\langle x, y \rangle} + \frac{|\langle x, y \rangle|^2}{\|y\|^4} \|y\|^2 $$

$$ 0 \le \|x\|^2 - \frac{|\langle x, y \rangle|^2}{\|y\|^2} - \frac{|\langle x, y \rangle|^2}{\|y\|^2} + \frac{|\langle x, y \rangle|^2}{\|y\|^2} $$

$$ 0 \le \|x\|^2 - \frac{|\langle x, y \rangle|^2}{\|y\|^2} $$

En multipliant par $\|y\|^2 > 0$ et en réarrangeant, on obtient :

$$ |\langle x, y \rangle|^2 \le \|x\|^2 \|y\|^2 $$

En prenant la racine carrée, on conclut : $|\langle x, y \rangle| \le \|x\| \|y\|$.

**Cas d'égalité :**

L'égalité a lieu si et seulement si l'inégalité initiale est une égalité, c'est-à-dire $\|x - \lambda y\|^2 = 0$.

Ceci est équivalent à $x - \lambda y = 0_E$, soit $x = \lambda y$.

Cela signifie que $x$ et $y$ sont colinéaires (liés).

</details>

---

Expliquez comment le procédé de Gram-Schmidt est à la base de la décomposition QR d'une matrice inversible et décrivez la nature des matrices Q et R.

<details>

<summary>Réponse</summary>

Le procédé d'orthonormalisation de Gram-Schmidt et la décomposition QR sont deux facettes du même concept.

Soit $A \in GL_n(\mathbb{K})$ une matrice inversible. On peut voir ses colonnes $v_1, \dots, v_n$ comme une base de $\mathbb{K}^n$.

Le procédé de Gram-Schmidt transforme cette base $(v_1, \dots, v_n)$ en une base orthonormée $(e_1, \dots, e_n)$. À chaque étape $k$, le vecteur $v_k$ s'exprime comme une combinaison linéaire des vecteurs orthonormés déjà construits $(e_1, \dots, e_k)$ :

$$ v_k = \langle v_k, e_1 \rangle e_1 + \langle v_k, e_2 \rangle e_2 + \dots + \langle v_k, e_{k-1} \rangle e_{k-1} + \|u_k\| e_k $$

où $u_k = v_k - \sum_{j=1}^{k-1} \langle v_k, e_j \rangle e_j$.

Cette relation montre que chaque $v_k$ est une combinaison linéaire de $e_1, \dots, e_k$. On peut écrire ces relations sous forme matricielle. Soit $A$ la matrice dont les colonnes sont les $v_k$, et $Q$ la matrice dont les colonnes sont les $e_k$. La relation ci-dessus se traduit par :

$$ A = QR $$

**Nature des matrices :**

1.  **Matrice Q :**

    Ses colonnes $(e_1, \dots, e_n)$ forment une base orthonormée de $\mathbb{K}^n$ pour le produit scalaire canonique.

    -   Si $\mathbb{K} = \mathbb{R}$, $Q$ est une **matrice orthogonale**, c'est-à-dire ${}^tQ Q = I_n$ (ou $Q^{-1} = {}^tQ$).
    -   Si $\mathbb{K} = \mathbb{C}$, $Q$ est une **matrice unitaire**, c'est-à-dire $Q^* Q = I_n$ (ou $Q^{-1} = Q^* = {}^t\bar{Q}$).

2.  **Matrice R :**

    La relation de Gram-Schmidt $v_k = \sum_{j=1}^k r_{jk} e_j$ montre que les coefficients de la $k$-ième colonne de $A$ dans la base $(e_j)$ sont nuls pour $j>k$. La matrice de passage de la base $(e_j)$ à la base $(v_k)$ est donc triangulaire supérieure. Cette matrice de passage est $R$.

    Les coefficients de $R$ sont donnés par $r_{jk} = \langle v_k, e_j \rangle$ pour $j < k$ et $r_{kk} = \|u_k\|$.

    Puisque la famille $(v_k)$ est libre, tous les $u_k$ sont non nuls, donc les coefficients diagonaux $r_{kk}$ sont strictement positifs.

    $R$ est donc une **matrice triangulaire supérieure à coefficients diagonaux strictement positifs**.

**Conclusion :** L'algorithme de Gram-Schmidt fournit une preuve constructive et une méthode de calcul pour la décomposition QR, qui est fondamentale en analyse numérique pour la résolution de systèmes linéaires et le calcul de valeurs propres.

</details>

---

Comment une forme bilinéaire $\varphi$ sur un espace vectoriel $E$ peut-elle être interprétée dans le langage des tenseurs ?

<details>

<summary>Réponse</summary>

En géométrie différentielle et en physique théorique, les formes bilinéaires sont interprétées comme des **tenseurs de type (0,2)**.

Soit $E$ un espace vectoriel de dimension finie sur un corps $\mathbb{K}$, et $E^*$ son espace dual (l'espace des formes linéaires sur $E$).

1.  **Définition d'un tenseur de type (0,2) :**

    Un tenseur $T$ de type (0,2) sur $E$ est une application multilinéaire qui prend deux vecteurs de $E$ et renvoie un scalaire :

    $$ T: E \times E \to \mathbb{K} $$

    Cette définition coïncide exactement avec celle d'une forme bilinéaire. Ainsi, l'ensemble des formes bilinéaires sur $E$ est précisément l'espace des tenseurs de type (0,2).

2.  **Identification avec $E^* \otimes E^*$ :**

    L'espace des tenseurs de type (0,2) est canoniquement isomorphe au produit tensoriel de l'espace dual avec lui-même : $E^* \otimes E^*$.

    Si $(e_1, \dots, e_n)$ est une base de $E$ et $(e^1, \dots, e^n)$ est la base duale correspondante de $E^*$ (définie par $e^i(e_j) = \delta^i_j$), alors toute forme bilinéaire $\varphi$ peut s'écrire de manière unique sous la forme :

    $$ \varphi = \sum_{i,j=1}^n \varphi_{ij} (e^i \otimes e^j) $$

    Les coefficients $\varphi_{ij}$ sont donnés par $\varphi_{ij} = \varphi(e_i, e_j)$. Ce sont exactement les coefficients de la matrice de la forme bilinéaire dans la base $(e_i)$.

    L'action du tenseur $\varphi$ sur un couple de vecteurs $(u, v)$ est alors :

    $$ \varphi(u,v) = \left( \sum_{i,j} \varphi_{ij} (e^i \otimes e^j) \right)(u,v) = \sum_{i,j} \varphi_{ij} e^i(u) e^j(v) $$

    Si $u=\sum_k u^k e_k$ et $v=\sum_l v^l e_l$, alors $e^i(u) = u^i$ et $e^j(v) = v^j$, et on retrouve l'expression habituelle $\varphi(u,v) = \sum_{i,j} \varphi_{ij} u^i v^j$.

**Concepts liés :**

-   Un produit scalaire est un tenseur métrique, un tenseur de type (0,2) qui est de plus symétrique et défini positif.
-   Cette interprétation est cruciale en géométrie riemannienne, où un produit scalaire (une "métrique") est défini sur chaque espace tangent d'une variété.

</details>

---

Expliquez pourquoi en dimension finie, le bi-orthogonal d'un sous-espace $F$ est $F$ lui-même, i.e. $(F^\perp)^\perp = F$, et donnez un contre-exemple en dimension infinie.

<details>

<summary>Réponse</summary>

**Cas de la dimension finie :**

Soit $E$ un espace euclidien ou hermitien de dimension finie, et $F$ un sous-espace vectoriel de $E$.

1.  **Inclusion $F \subseteq (F^\perp)^\perp$ :**

    Soit $x \in F$. Par définition de $F^\perp$, tout vecteur $y \in F^\perp$ vérifie $\langle x, y \rangle = 0$. Mais la définition de $(F^\perp)^\perp$ est l'ensemble des vecteurs orthogonaux à *tous* les vecteurs de $F^\perp$. Puisque $x$ est orthogonal à tous les $y \in F^\perp$, $x$ appartient à $(F^\perp)^\perp$. L'inclusion est donc toujours vraie, quelle que soit la dimension.

2.  **Égalité par les dimensions :**

    Le théorème fondamental de la projection orthogonale en dimension finie nous donne :

    $$ \dim(F) + \dim(F^\perp) = \dim(E) $$

    On peut appliquer ce même théorème au sous-espace $F^\perp$ :

    $$ \dim(F^\perp) + \dim((F^\perp)^\perp) = \dim(E) $$

    En comparant les deux égalités, on déduit immédiatement que $\dim(F) = \dim((F^\perp)^\perp)$.

    Puisque nous avons l'inclusion $F \subseteq (F^\perp)^\perp$ et l'égalité des dimensions, on conclut que $F = (F^\perp)^\perp$.

**Contre-exemple en dimension infinie :**

L'égalité $(F^\perp)^\perp = \overline{F}$ (l'adhérence de $F$) est vraie dans un espace de Hilbert. Si $F$ n'est pas fermé, l'égalité avec $F$ est fausse.

Soit $E = l^2(\mathbb{N})$, l'espace de Hilbert des suites de carré sommable.

Soit $F = c_{00}$, le sous-espace vectoriel des suites à support fini (nulles à partir d'un certain rang).

$F$ est un sous-espace vectoriel de $E$, mais il n'est pas fermé. En fait, il est dense dans $E$.

-   **Calcul de $F^\perp$ :**

    Soit $u = (u_n) \in F^\perp$. Pour tout $k \in \mathbb{N}$, la suite $e_k$ (qui vaut 1 en position $k$ et 0 ailleurs) est dans $F$.

    Donc, $\langle u, e_k \rangle = u_k = 0$ pour tout $k$.

    La seule suite dont tous les termes sont nuls est la suite nulle. Donc $F^\perp = \{0_E\}$.

-   **Calcul de $(F^\perp)^\perp$ :**

    L'orthogonal du sous-espace nul est l'espace entier :

    $$ (F^\perp)^\perp = (\{0_E\})^\perp = E $$

    On a donc $(F^\perp)^\perp = E$, alors que $F = c_{00}$. Clairement, $E \ne F$, ce qui fournit le contre-exemple.

</details>

---

Démontrez qu'un projecteur $p$ sur un espace euclidien ou hermitien $E$ est un projecteur orthogonal si et seulement s'il est autoadjoint ($p=p^*$).

<details>

<summary>Réponse</summary>

Soit $p \in \mathcal{L}(E)$ un projecteur, i.e., $p^2 = p$. On note $F = \text{Im}(p)$ et $G = \text{Ker}(p)$. On sait que $E = F \oplus G$.

**($\Rightarrow$) Supposons que $p$ est un projecteur orthogonal.**

Par définition, cela signifie que son noyau est l'orthogonal de son image : $G = F^\perp$. Donc $E = F \oplus F^\perp$.

Montrons que $p$ est autoadjoint, c'est-à-dire $\forall x, y \in E, \langle p(x), y \rangle = \langle x, p(y) \rangle$.

Soient $x, y \in E$. On les décompose selon $F \oplus F^\perp$ :

$x = x_F + x_{F^\perp}$ avec $x_F = p(x) \in F$ et $x_{F^\perp} \in F^\perp$.

$y = y_F + y_{F^\perp}$ avec $y_F = p(y) \in F$ et $y_{F^\perp} \in F^\perp$.

Calculons le premier terme :

$$ \langle p(x), y \rangle = \langle x_F, y_F + y_{F^\perp} \rangle = \langle x_F, y_F \rangle + \langle x_F, y_{F^\perp} \rangle $$

Comme $x_F \in F$ et $y_{F^\perp} \in F^\perp$, leur produit scalaire est nul. Donc :

$$ \langle p(x), y \rangle = \langle x_F, y_F \rangle $$

Calculons le second terme :

$$ \langle x, p(y) \rangle = \langle x_F + x_{F^\perp}, y_F \rangle = \langle x_F, y_F \rangle + \langle x_{F^\perp}, y_F \rangle $$

Comme $x_{F^\perp} \in F^\perp$ et $y_F \in F$, leur produit scalaire est nul. Donc :

$$ \langle x, p(y) \rangle = \langle x_F, y_F \rangle $$

Les deux termes sont égaux, donc $p=p^*$.

**($\Leftarrow$) Supposons que $p$ est un projecteur autoadjoint ($p^2=p$ et $p=p^*$).**

Nous devons montrer que $p$ est un projecteur orthogonal, c'est-à-dire que son noyau est l'orthogonal de son image : $\text{Ker}(p) = (\text{Im}(p))^\perp$.

On sait que pour tout endomorphisme $f$, on a la relation $(\text{Im}(f))^\perp = \text{Ker}(f^*)$.

Appliquons cela à $p$. Puisque $p=p^*$, on a :

$$ (\text{Im}(p))^\perp = \text{Ker}(p^*) = \text{Ker}(p) $$

Ceci est exactement la définition d'un projecteur orthogonal.

</details>

---

Quelle est la signification géométrique du déterminant de Gram ? Prouvez-le pour une famille de deux vecteurs dans un espace euclidien $\mathbb{R}^n$.

<details>

<summary>Réponse</summary>

**Signification géométrique :**

Le déterminant de la matrice de Gram d'une famille de $k$ vecteurs $(v_1, \dots, v_k)$ dans un espace euclidien est égal au **carré du volume** du parallélotope $k$-dimensionnel engendré par ces vecteurs.

En particulier :

-   Pour $k=1$, $\det(G) = \langle v_1, v_1 \rangle = \|v_1\|^2$, le carré de la longueur.
-   Pour $k=2$, $\det(G)$ est le carré de l'aire du parallélogramme engendré par $v_1, v_2$.
-   Pour $k=3$, $\det(G)$ est le carré du volume du parallélépipède engendré par $v_1, v_2, v_3$.

Une conséquence directe est que le déterminant de Gram est nul si et seulement si la famille de vecteurs est liée (le parallélotope est "aplati" et son volume est nul).

**Preuve pour deux vecteurs $u, v \in \mathbb{R}^n$ :**

La matrice de Gram de la famille $(u,v)$ est :

$$ G = \begin{pmatrix} \langle u, u \rangle & \langle u, v \rangle \\ \langle v, u \rangle & \langle v, v \rangle \end{pmatrix} = \begin{pmatrix} \|u\|^2 & \langle u, v \rangle \\ \langle u, v \rangle & \|v\|^2 \end{pmatrix} $$

Son déterminant est :

$$ \det(G) = \|u\|^2 \|v\|^2 - \langle u, v \rangle^2 $$

Soit $\theta$ l'angle entre les vecteurs $u$ et $v$. On sait que $\langle u, v \rangle = \|u\| \|v\| \cos(\theta)$. En substituant :

$$ \det(G) = \|u\|^2 \|v\|^2 - (\|u\| \|v\| \cos(\theta))^2 $$

$$ \det(G) = \|u\|^2 \|v\|^2 (1 - \cos^2(\theta)) $$

$$ \det(G) = \|u\|^2 \|v\|^2 \sin^2(\theta) $$

L'aire $\mathcal{A}$ du parallélogramme engendré par $u$ et $v$ est donnée par "base $\times$ hauteur". En prenant $u$ comme base, la hauteur est $\|v\| |\sin(\theta)|$.

$$ \mathcal{A} = \|u\| \cdot (\|v\| |\sin(\theta)|) $$

Ainsi, le carré de l'aire est :

$$ \mathcal{A}^2 = (\|u\| \|v\| \sin(\theta))^2 = \|u\|^2 \|v\|^2 \sin^2(\theta) $$

On a donc bien $\det(G) = \mathcal{A}^2$, ce qui prouve le résultat.

</details>

---

Interprétez la somme partielle de la série de Fourier d'une fonction comme une projection orthogonale dans un espace préhilbertien approprié.

<details>

<summary>Réponse</summary>

Considérons l'espace $E = \mathcal{C}_{2\pi}$ des fonctions continues $2\pi$-périodiques de $\mathbb{R}$ dans $\mathbb{C}$. On munit cet espace du produit scalaire hermitien :

$$ \langle f, g \rangle = \frac{1}{2\pi} \int_0^{2\pi} f(t)\overline{g(t)} dt $$

Dans cet espace, la famille de fonctions $(e_k)_{k \in \mathbb{Z}}$ définie par $e_k(t) = e^{ikt}$ forme une **famille orthonormée**.

Soit $F_n$ le sous-espace vectoriel de $E$ engendré par les $2n+1$ premières fonctions de cette famille :

$$ F_n = \text{Vect}(e_k(t) = e^{ikt})_{k=-n, \dots, n} $$

$F_n$ est l'espace des polynômes trigonométriques de degré au plus $n$. La famille $(e_{-n}, \dots, e_n)$ est une base orthonormée de $F_n$.

La **projection orthogonale** $p_{F_n}(f)$ d'une fonction $f \in E$ sur le sous-espace $F_n$ est donnée par la formule générale de projection sur une base orthonormée :

$$ p_{F_n}(f) = \sum_{k=-n}^{n} \langle f, e_k \rangle e_k $$

Calculons les coefficients $\langle f, e_k \rangle$ :

$$ \langle f, e_k \rangle = \frac{1}{2\pi} \int_0^{2\pi} f(t)\overline{e_k(t)} dt = \frac{1}{2\pi} \int_0^{2\pi} f(t)e^{-ikt} dt $$

Ce coefficient est précisément le **$k$-ième coefficient de Fourier de $f$**, noté $c_k(f)$.

Ainsi, la projection orthogonale de $f$ sur $F_n$ est :

$$ p_{F_n}(f)(t) = \sum_{k=-n}^{n} c_k(f) e^{ikt} $$

Cette expression est exactement la **$n$-ième somme partielle de la série de Fourier de $f$**, notée $S_n(f)$.

**Conclusion et Interprétation :**

La somme partielle de Fourier $S_n(f)$ est la meilleure approximation de $f$ par un polynôme trigonométrique de degré au plus $n$, au sens de la norme $L^2$ (minimisation de l'erreur quadratique moyenne). C'est une conséquence directe du **théorème de la meilleure approximation**, qui stipule que la projection orthogonale sur un sous-espace est l'unique point de ce sous-espace qui minimise la distance à la fonction d'origine.

$$ \int_0^{2\pi} |f(t) - S_n(f)(t)|^2 dt = \min_{P \in F_n} \int_0^{2\pi} |f(t) - P(t)|^2 dt $$

</details>

---

Dérivez l'identité de polarisation pour un produit scalaire sur un espace vectoriel complexe (cas hermitien).

<details>

<summary>Réponse</summary>

L'objectif est d'exprimer $\langle x, y \rangle$ uniquement en fonction de la norme $\|\cdot\|$ définie par $\|v\|^2 = \langle v, v \rangle$.

Partons du développement de $\|x+y\|^2$ et $\|x-y\|^2$ :

$$ \|x+y\|^2 = \langle x+y, x+y \rangle = \|x\|^2 + \langle x,y \rangle + \langle y,x \rangle + \|y\|^2 $$

$$ \|x-y\|^2 = \langle x-y, x-y \rangle = \|x\|^2 - \langle x,y \rangle - \langle y,x \rangle + \|y\|^2 $$

En soustrayant la deuxième ligne de la première, on obtient :

$$ \|x+y\|^2 - \|x-y\|^2 = 2\langle x,y \rangle + 2\langle y,x \rangle $$

Puisque la forme est hermitienne, $\langle y,x \rangle = \overline{\langle x,y \rangle}$.

$$ \|x+y\|^2 - \|x-y\|^2 = 2(\langle x,y \rangle + \overline{\langle x,y \rangle}) = 4 \text{Re}(\langle x,y \rangle) \quad (*)$$

Nous avons isolé la partie réelle de $\langle x,y \rangle$. Pour trouver la partie imaginaire, nous utilisons une astuce en remplaçant $y$ par $iy$ :

$$ \langle x, iy \rangle = \bar{i} \langle x,y \rangle = -i \langle x,y \rangle $$

On sait que $\text{Im}(z) = \text{Re}(-iz)$. Donc, $\text{Im}(\langle x,y \rangle) = \text{Re}(-i\langle x,y \rangle) = \text{Re}(\langle x, iy \rangle)$.

Utilisons notre résultat $(*)$ en remplaçant $y$ par $iy$ :

$$ 4 \text{Re}(\langle x,iy \rangle) = \|x+iy\|^2 - \|x-iy\|^2 $$

Donc,

$$ 4 \text{Im}(\langle x,y \rangle) = \|x+iy\|^2 - \|x-iy\|^2 \quad (**) $$

Puisque $\langle x,y \rangle = \text{Re}(\langle x,y \rangle) + i \text{Im}(\langle x,y \rangle)$, on peut combiner $(*)$ et $(**)$ :

$$ 4 \langle x,y \rangle = 4 \text{Re}(\langle x,y \rangle) + i (4 \text{Im}(\langle x,y \rangle)) $$

$$ 4 \langle x,y \rangle = (\|x+y\|^2 - \|x-y\|^2) + i (\|x+iy\|^2 - \|x-iy\|^2) $$

Finalement, on obtient l'**identité de polarisation hermitienne** :

$$ \langle x, y \rangle = \frac{1}{4} \left( \|x+y\|^2 - \|x-y\|^2 + i(\|x+iy\|^2 - \|x-iy\|^2) \right) $$

</details>

---

Comparez les formes bilinéaires symétriques et les formes sesquilinéaires hermitiennes, en insistant sur le rôle du corps ($\mathbb{R}$ vs $\mathbb{C}$) et l'implication pour la valeur de $\varphi(x,x)$.

<details>

<summary>Réponse</summary>

La distinction entre ces deux types de formes est fondamentale et est dictée par la nature du corps sous-jacent.

| Caractéristique | Forme Bilinéaire Symétrique (sur $\mathbb{K}$) | Forme Sesquilinéaire Hermitienne (sur $\mathbb{C}$) |

| :--- | :--- | :--- |

| **Définition** | $\varphi(x,y) = \varphi(y,x)$ | $\varphi(x,y) = \overline{\varphi(y,x)}$ |

| **Linéarité** | Linéaire dans les deux arguments. $\varphi(x, \lambda y) = \lambda \varphi(x,y)$ | Linéaire dans le premier argument, **anti-linéaire** dans le second. $\varphi(x, \lambda y) = \bar{\lambda} \varphi(x,y)$ |

| **Matrice (dans une base $\mathcal{B}$)** | $M$ est symétrique : ${}^tM = M$ | $M$ est hermitienne : $M^* = {}^t\bar{M} = M$ |

| **Valeur de $\varphi(x,x)$** | Peut être n'importe quel scalaire de $\mathbb{K}$. Si $\mathbb{K}=\mathbb{C}$, $\varphi(x,x)$ peut être complexe. | $\varphi(x,x) = \overline{\varphi(x,x)}$, donc **$\varphi(x,x)$ est toujours un nombre réel**. |

**Analyse et Implications :**

1.  **Le problème avec les formes bilinéaires symétriques sur $\mathbb{C}$ :**

    Si l'on essayait de définir un produit scalaire sur $\mathbb{C}^n$ en utilisant une forme bilinéaire symétrique comme $\varphi(x,y) = \sum x_i y_i$, la "norme au carré" associée serait $\varphi(x,x) = \sum x_i^2$.

    Cette valeur n'est pas nécessairement réelle. Par exemple, pour $x=(i, 0, \dots, 0)$, $\varphi(x,x) = i^2 = -1$. Cela pose un problème majeur pour définir une norme (qui doit être un réel positif) et une distance. La notion de "positivité" devient ambiguë.

2.  **La solution de la sesquilinéarité :**

    La forme sesquilinéaire hermitienne $\varphi(x,y) = \sum x_i \bar{y}_i$ résout ce problème. La norme au carré associée est :

    $$ \varphi(x,x) = \sum x_i \bar{x}_i = \sum |x_i|^2 $$

    Cette quantité est non seulement **réelle**, mais aussi **positive**. C'est la propriété clé qui permet de définir une norme cohérente $\|x\| = \sqrt{\varphi(x,x)}$ sur un espace vectoriel complexe.

**Conclusion :**

La conjugaison complexe dans la définition de la forme hermitienne et de l'anti-linéarité n'est pas une complication arbitraire. C'est la modification nécessaire pour garantir que la grandeur $\varphi(x,x)$, qui représente le carré de la "longueur" d'un vecteur, soit un nombre réel positif, ce qui est une condition indispensable pour construire une géométrie métrique sur les espaces complexes. Sur $\mathbb{R}$, la conjugaison est l'identité, donc les deux notions coïncident.

</details>