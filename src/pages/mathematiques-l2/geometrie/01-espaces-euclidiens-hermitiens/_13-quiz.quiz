---
id: '15352066'
type: quiz
order: 13
title: Espaces Euclidiens et Hermitiens - quiz (A)
tags:
  - Espaces Euclidiens
  - Espaces Hermitiens
  - Produit scalaire
  - Gram-Schmidt
  - Projection orthogonale
  - Endomorphismes adjoints
  - Théorème de Riesz
createdAt: '2025-10-12T18:13:04.161Z'
level: regular
course: Géométrie
courseId: d9494343
chapter: Espaces Euclidiens et Hermitiens
chapterId: 67b3d760
---
# Quiz: Espaces Euclidiens et Hermitiens

---

#### Définition d'une forme sesquilinéaire

Soit $E$ un espace vectoriel sur $\mathbb{C}$. Une application $\varphi: E \times E \to \mathbb{C}$ est une forme sesquilinéaire si, pour tous vecteurs $u, v, w \in E$ et tout scalaire $\lambda \in \mathbb{C}$ :

- [ ] **A)** Elle est linéaire par rapport à ses deux variables, i.e., $\varphi(\lambda u + v, w) = \lambda\varphi(u,w) + \varphi(v,w)$ et $\varphi(u, \lambda v + w) = \lambda\varphi(u,v) + \varphi(u,w)$.
- [x] **B)** Elle est linéaire par rapport à sa première variable et semi-linéaire par rapport à sa seconde, i.e., $\varphi(\lambda u + v, w) = \lambda\varphi(u,w) + \varphi(v,w)$ et $\varphi(u, \lambda v + w) = \bar{\lambda}\varphi(u,v) + \varphi(u,w)$.
- [ ] **C)** Elle est hermitienne, i.e., $\varphi(x, y) = \overline{\varphi(y, x)}$ pour tous $x, y \in E$.
- [ ] **D)** Elle est semi-linéaire par rapport à ses deux variables, i.e., $\varphi(\lambda u + v, w) = \bar{\lambda}\varphi(u,w) + \varphi(v,w)$ et $\varphi(u, \lambda v + w) = \bar{\lambda}\varphi(u,v) + \varphi(u,w)$.

<details>

<summary>Solution</summary>

**Réponse : [B]**

Cette question teste la définition d'une forme sesquilinéaire, qui est fondamentale pour les espaces hermitiens.

- **A)** Incorrect. C'est la définition d'une forme **bilinéaire**. La linéarité s'applique aux deux variables sans conjugaison.

- **B)** Correct. C'est la définition précise d'une forme sesquilinéaire sur un $\mathbb{C}$-espace vectoriel. Le préfixe "sesqui-" signifie "un et demi", car elle est linéaire pour la première variable (1) et "à moitié" linéaire (semi-linéaire) pour la seconde.

- **C)** Incorrect. C'est la condition pour qu'une forme sesquilinéaire soit dite **hermitienne**. C'est une propriété supplémentaire, pas la définition de base d'une forme sesquilinéaire.

- **D)** Incorrect. Une forme qui est semi-linéaire par rapport à ses deux variables n'est pas la définition standard d'une forme sesquilinéaire.

</details>

---

#### Identification d'une forme bilinéaire symétrique

Parmi les applications suivantes de $\mathbb{R}^2 \times \mathbb{R}^2$ dans $\mathbb{R}$, laquelle est une forme bilinéaire symétrique ? On note $x=(x_1, x_2)$ et $y=(y_1, y_2)$.

- [x] **A)** $\varphi(x, y) = 3x_1 y_1 + x_2 y_2$
- [ ] **B)** $\varphi(x, y) = x_1^2 + y_1^2$
- [ ] **C)** $\varphi(x, y) = x_1 y_2$
- [ ] **D)** $\varphi(x, y) = x_1 y_2 - x_2 y_1$

<details>

<summary>Solution</summary>

**Réponse : [A]**

Cette question teste la capacité à vérifier les propriétés de bilinéarité et de symétrie sur des exemples concrets.

- **A)** Correct.
    - **Bilinéarité** : $\varphi(\lambda u+v, y) = 3(\lambda u_1+v_1)y_1 + (\lambda u_2+v_2)y_2 = \lambda(3u_1y_1+u_2y_2) + (3v_1y_1+v_2y_2) = \lambda\varphi(u,y)+\varphi(v,y)$. La linéarité à droite est similaire.
    - **Symétrie** : $\varphi(x, y) = 3x_1 y_1 + x_2 y_2 = 3y_1 x_1 + y_2 x_2 = \varphi(y, x)$.

- **B)** Incorrect. Cette forme n'est pas linéaire. Par exemple, $\varphi(2x, y) = (2x_1)^2 + y_1^2 = 4x_1^2 + y_1^2$, ce qui est différent de $2\varphi(x, y) = 2(x_1^2 + y_1^2)$.

- **C)** Incorrect. Cette forme est bilinéaire, mais elle n'est pas symétrique : $\varphi(y, x) = y_1 x_2$, ce qui est différent de $\varphi(x, y) = x_1 y_2$ en général.

- **D)** Incorrect. Cette forme est bilinéaire. C'est le déterminant. Elle n'est pas symétrique, mais antisymétrique : $\varphi(y, x) = y_1 x_2 - y_2 x_1 = -(x_1 y_2 - x_2 y_1) = -\varphi(x, y)$.

</details>

---

#### Définition d'un produit scalaire

Quelle est la définition complète d'un produit scalaire sur un espace vectoriel réel $E$ ?

- [ ] **A)** C'est une forme bilinéaire.
- [ ] **B)** C'est une forme bilinéaire symétrique et positive.
- [x] **C)** C'est une forme bilinéaire symétrique, définie et positive.
- [ ] **D)** C'est une forme sesquilinéaire hermitienne.

<details>

<summary>Solution</summary>

**Réponse : [C]**

La question demande la définition complète et rigoureuse d'un produit scalaire dans le cas réel.

- **A)** Incorrect. C'est trop vague. Une forme bilinéaire n'est pas nécessairement symétrique ou définie positive.

- **B)** Incorrect. Il manque une condition cruciale : le caractère "défini". Une forme bilinéaire symétrique et positive peut être nulle pour des vecteurs non nuls (par exemple, $\varphi(x,y)=x_1y_1$ sur $\mathbb{R}^2$ est nulle pour le vecteur non nul $(0,1)$).

- **C)** Correct. C'est la définition exacte. Les trois propriétés sont nécessaires :
    1.  **Bilinéaire symétrique** : pour la structure algébrique.
    2.  **Positive** ($\langle x,x \rangle \ge 0$) : pour que la notion de "longueur au carré" soit positive.
    3.  **Définie** ($\langle x,x \rangle = 0 \iff x=0$) : pour que seul le vecteur nul ait une longueur nulle.

- **D)** Incorrect. C'est la base de la définition d'un produit scalaire sur un espace vectoriel **complexe** (hermitien), pas réel (euclidien).

</details>

---

#### Forme non valide comme produit scalaire

Lesquelles de ces formes définies sur $\mathbb{R}^2 \times \mathbb{R}^2$ ne sont PAS des produits scalaires ? On note $x=(x_1, x_2)$ et $y=(y_1, y_2)$.

- [ ] **A)** $\langle x, y \rangle = 2x_1 y_1 + 3x_2 y_2$
- [ ] **B)** $\langle x, y \rangle = (x_1+x_2)(y_1+y_2) + x_2 y_2$
- [x] **C)** $\langle x, y \rangle = x_1 y_1 - x_2 y_2$
- [x] **D)** $\langle x, y \rangle = x_1 y_1 + x_1 y_2 + x_2 y_1$

<details>

<summary>Solution</summary>

**Réponses : [C, D]**

Pour être un produit scalaire, une forme bilinéaire symétrique doit être définie positive.

- **A)** Incorrect. C'est un produit scalaire valide.
    - Symétrie : évidente.
    - Définie positive : $\langle x, x \rangle = 2x_1^2 + 3x_2^2 \ge 0$. Cette somme est nulle si et seulement si $x_1^2=0$ et $x_2^2=0$, c'est-à-dire $x=(0,0)$.

- **B)** Incorrect. C'est un produit scalaire valide.
    - Symétrie : évidente.
    - Définie positive : $\langle x, x \rangle = (x_1+x_2)^2 + x_2^2 \ge 0$. Si $\langle x, x \rangle=0$, alors $(x_1+x_2)^2=0$ et $x_2^2=0$. Cela implique $x_2=0$, et donc $x_1+0=0 \implies x_1=0$. Le seul vecteur pour lequel c'est nul est $x=(0,0)$.

- **C)** Correct. Cette forme n'est pas un produit scalaire car elle n'est pas positive. Pour le vecteur $x=(0, 1)$, on a $\langle x, x \rangle = 0^2 - 1^2 = -1 < 0$.

- **D)** Correct. Cette forme est symétrique ($\langle y,x \rangle = y_1x_1 + y_1x_2 + y_2x_1 = \langle x,y \rangle$), mais elle n'est pas positive. Par exemple, pour $x=(1,-1)$, $\langle x,x \rangle = 1^2 + 1(-1)+(-1)1 = 1-1-1 = -1 < 0$. Elle ne peut donc pas être un produit scalaire.

</details>

---

#### Inégalité de Cauchy-Schwarz

Soit $(E, \langle \cdot, \cdot \rangle)$ un espace préhilbertien. L'inégalité de Cauchy-Schwarz s'écrit $|\langle x, y \rangle| \le \|x\| \|y\|$. Quand a-t-on égalité ?

- [x] **A)** Si et seulement si $x$ et $y$ sont colinéaires.
- [ ] **B)** Si et seulement si $x$ et $y$ sont orthogonaux.
- [ ] **C)** Uniquement si $x$ ou $y$ est le vecteur nul.
- [ ] **D)** Si et seulement si $x$ et $y$ sont des vecteurs unitaires.

<details>

<summary>Solution</summary>

**Réponse : [A]**

Cette question porte sur le cas d'égalité de l'inégalité de Cauchy-Schwarz, qui est une partie essentielle du théorème.

- **A)** Correct. Le cas d'égalité $|\langle x, y \rangle| = \|x\| \|y\|$ se produit si et seulement si les vecteurs $x$ et $y$ sont linéairement dépendants (colinéaires). Cela signifie qu'il existe un scalaire $\lambda$ tel que $x = \lambda y$ ou $y = \lambda x$.

- **B)** Incorrect. Si $x$ et $y$ sont orthogonaux, alors $\langle x, y \rangle = 0$. L'inégalité devient $0 \le \|x\| \|y\|$, ce qui est toujours vrai. C'est le cas où le membre de gauche est minimal, pas nécessairement un cas d'égalité (sauf si l'un des vecteurs est nul).

- **C)** Incorrect. Si $x$ ou $y$ est le vecteur nul, les deux côtés de l'inégalité sont nuls, donc il y a bien égalité. Cependant, ce n'est pas la condition *nécessaire et suffisante*. L'égalité peut aussi se produire pour des vecteurs non nuls, à condition qu'ils soient colinéaires (par exemple, $x=(1,1)$ et $y=(2,2)$).

- **D)** Incorrect. Le fait que les vecteurs soient unitaires $(\|x\|=\|y\|=1)$ n'a pas de lien direct avec le cas d'égalité. Pour deux vecteurs unitaires, l'inégalité devient $|\langle x, y \rangle| \le 1$, avec égalité si et seulement s'ils sont colinéaires ($x = \pm y$ dans le cas réel).

</details>

---

#### Norme et produit scalaire

Quelle propriété, parmi les suivantes, caractérise une norme $\|\cdot\|$ qui est associée à un produit scalaire (c'est-à-dire, pour laquelle il existe un produit scalaire $\langle \cdot, \cdot \rangle$ tel que $\|x\| = \sqrt{\langle x,x \rangle}$) ?

- [ ] **A)** L'inégalité triangulaire : $\|x+y\| \le \|x\| + \|y\|$
- [x] **B)** L'identité du parallélogramme : $\|x+y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2)$
- [ ] **C)** L'homogénéité : $\|\lambda x\| = |\lambda| \|x\|$
- [ ] **D)** Le théorème de Pythagore : si $\langle x,y \rangle=0$, alors $\|x+y\|^2 = \|x\|^2 + \|y\|^2$

<details>

<summary>Solution</summary>

**Réponse : [B]**

Cette question demande d'identifier la propriété qui est une condition nécessaire ET suffisante pour qu'une norme dérive d'un produit scalaire.

- **A)** Incorrect. L'inégalité triangulaire est une propriété que **toutes** les normes doivent vérifier par définition. Elle ne distingue pas les normes euclidiennes des autres.

- **B)** Correct. L'identité du parallélogramme est la condition nécessaire et suffisante. Si une norme vérifie cette identité, alors on peut construire un produit scalaire (via les identités de polarisation) qui induit cette norme. Inversement, toute norme issue d'un produit scalaire vérifie cette identité.

- **C)** Incorrect. L'homogénéité est également une propriété que **toutes** les normes doivent vérifier par définition.

- **D)** Incorrect. Le théorème de Pythagore est une **conséquence** du fait qu'une norme dérive d'un produit scalaire, mais ce n'est pas une condition suffisante. Il existe des normes non-euclidiennes pour lesquelles on peut définir une notion d'orthogonalité qui satisfait Pythagore sans que l'identité du parallélogramme soit vérifiée pour tous les vecteurs.

</details>

---

#### Calcul de norme

Dans l'espace hermitien $\mathbb{C}^3$ muni de son produit scalaire canonique $\langle x, y \rangle = \sum_{i=1}^3 x_i \bar{y_i}$, calculez la norme du vecteur $v = (1, i, 1+i)$.

- [ ] **A)** $\sqrt{3}$
- [ ] **B)** $4$
- [ ] **C)** $\sqrt{2}$
- [x] **D)** $2$

<details>

<summary>Solution</summary>

**Réponse : [D]**

La norme d'un vecteur $v$ est donnée par $\|v\| = \sqrt{\langle v, v \rangle}$. Dans un espace hermitien, $\langle v, v \rangle = \sum v_i \bar{v_i} = \sum |v_i|^2$.

1.  Calculons le carré de la norme :

    $\|v\|^2 = |1|^2 + |i|^2 + |1+i|^2$.

2.  Rappelons que pour un nombre complexe $z = a+ib$, son module au carré est $|z|^2 = a^2+b^2$.
    - $|1|^2 = 1^2 = 1$.
    - $|i|^2 = 0^2+1^2 = 1$.
    - $|1+i|^2 = 1^2+1^2 = 2$.

3.  Additionnons les termes :

    $\|v\|^2 = 1 + 1 + 2 = 4$.

4.  La norme est la racine carrée :

    $\|v\| = \sqrt{4} = 2$.

- **A)** Incorrect. $\sqrt{3}$ serait la norme si toutes les composantes avaient un module de 1.
- **B)** Incorrect. 4 est le carré de la norme, pas la norme elle-même.
- **C)** Incorrect. $\sqrt{2}$ est le module de la troisième composante.

</details>

---

#### Base orthonormée

Soit $\mathcal{B}=(e_1, \dots, e_n)$ une base orthonormée d'un espace euclidien $E$. Si un vecteur $x \in E$ s'écrit $x = \sum_{i=1}^n x_i e_i$, quelles affirmations sont vraies ?

- [x] **A)** Les coordonnées $x_i$ sont données par $x_i = \langle x, e_i \rangle$.
- [x] **B)** La norme au carré de $x$ est calculée par $\|x\|^2 = \sum_{i=1}^n x_i^2$.
- [x] **C)** Pour un autre vecteur $y = \sum_{i=1}^n y_i e_i$, on a $\langle x, y \rangle = \sum_{i=1}^n x_i y_i$.
- [ ] **D)** L'existence d'une telle base implique que $E$ est $\mathbb{R}^n$.

<details>

<summary>Solution</summary>

**Réponses : [A, B, C]**

Cette question porte sur les avantages calculatoires majeurs des bases orthonormées (B.O.N).

- **A)** Correct. C'est la formule pour trouver les coordonnées dans une B.O.N. On a $\langle x, e_j \rangle = \langle \sum_i x_i e_i, e_j \rangle = \sum_i x_i \langle e_i, e_j \rangle = \sum_i x_i \delta_{ij} = x_j$.

- **B)** Correct. C'est l'identité de Parseval. En utilisant la bilinéarité et le fait que $\langle e_i, e_j \rangle = \delta_{ij}$, on a $\|x\|^2 = \langle \sum_i x_i e_i, \sum_j x_j e_j \rangle = \sum_i \sum_j x_i x_j \langle e_i, e_j \rangle = \sum_i x_i^2$.

- **C)** Correct. C'est l'expression du produit scalaire dans une B.O.N, qui devient identique au produit scalaire canonique sur les vecteurs de coordonnées. Le calcul est similaire à celui pour la norme.

- **D)** Incorrect. Tout espace euclidien de dimension finie $n$ possède une base orthonormée. Un tel espace est isomorphe à $\mathbb{R}^n$, mais il peut s'agir d'un autre type d'espace, comme un espace de polynômes ou de fonctions.

</details>

---

#### Procédé de Gram-Schmidt

On applique le procédé de Gram-Schmidt à une base $(v_1, \dots, v_n)$ pour obtenir une base orthonormée $(e_1, \dots, e_n)$. À l'étape $k+1$, on calcule le vecteur $e'_{k+1} = v_{k+1} - \sum_{j=1}^{k} \langle v_{k+1}, e_j \rangle e_j$. Quelle est la propriété principale de ce vecteur $e'_{k+1}$ ?

- [ ] **A)** Il est colinéaire à $v_{k+1}$.
- [ ] **B)** Il est unitaire.
- [ ] **C)** Il est orthogonal à tous les vecteurs $v_j$ pour $j \le k$.
- [x] **D)** Il est orthogonal à tous les vecteurs $e_j$ pour $j \le k$.

<details>

<summary>Solution</summary>

**Réponse : [D]**

Cette question teste la compréhension de l'étape clé du procédé de Gram-Schmidt.

- **A)** Incorrect. Le vecteur $e'_{k+1}$ est obtenu en soustrayant de $v_{k+1}$ sa projection sur le sous-espace engendré par $(e_1, \dots, e_k)$. Il n'y a donc aucune raison pour qu'il soit colinéaire à $v_{k+1}$, sauf cas très particulier.

- **B)** Incorrect. $e'_{k+1}$ est le résultat de l'étape d'orthogonalisation. Il n'est généralement pas unitaire. L'étape suivante de l'algorithme consiste justement à le normaliser en posant $e_{k+1} = e'_{k+1} / \|e'_{k+1}\|$.

- **C)** Incorrect. $e'_{k+1}$ est construit pour être orthogonal à la base **orthonormée** $(e_1, \dots, e_k)$ déjà construite, pas à la base de départ $(v_1, \dots, v_k)$.

- **D)** Correct. Par construction, le terme $\sum_{j=1}^{k} \langle v_{k+1}, e_j \rangle e_j$ est le projeté orthogonal de $v_{k+1}$ sur l'espace $\text{Vect}(e_1, \dots, e_k)$. En soustrayant ce projeté à $v_{k+1}$, on obtient un vecteur qui est orthogonal à cet espace, et donc orthogonal à chacun des vecteurs $e_1, \dots, e_k$.

</details>

---

#### Projection orthogonale

Soit $F$ un sous-espace d'un espace euclidien $E$ et $x \in E$. Le vecteur $P_F(x)$, le projeté orthogonal de $x$ sur $F$, est caractérisé de manière unique par le fait que :

- [x] **A)** $P_F(x)$ est le vecteur de $F$ qui minimise la distance $\|x - y\|$ pour $y \in F$.
- [ ] **B)** $P_F(x)$ est le seul vecteur de $F$ tel que $x - P_F(x)$ soit orthogonal à $x$.
- [ ] **C)** La distance de $x$ à $F$ est égale à $\|P_F(x)\|$.
- [ ] **D)** $P_F(x) = \sum_{i=1}^p \langle x, v_i \rangle v_i$ pour n'importe quelle base $(v_1, \dots, v_p)$ de $F$.

<details>

<summary>Solution</summary>

**Réponse : [A]**

Cette question concerne la propriété fondamentale qui définit géométriquement la projection orthogonale.

- **A)** Correct. C'est le théorème de la meilleure approximation. Le projeté orthogonal $P_F(x)$ est l'unique point de $F$ qui est le plus proche de $x$. La distance de $x$ à $F$ est atteinte en ce point.

- **B)** Incorrect. La condition correcte est que le vecteur $x - P_F(x)$ doit être orthogonal à **tout vecteur de $F$**, pas seulement au vecteur $x$.

- **C)** Incorrect. La distance de $x$ à $F$ est la longueur du vecteur qui "relie" $x$ à son projeté, c'est-à-dire $d(x,F) = \|x - P_F(x)\|$.

- **D)** Incorrect. Cette formule n'est valide que si la base $(v_1, \dots, v_p)$ de $F$ est **orthonormée**. Pour une base quelconque, la formule est beaucoup plus compliquée.

</details>

---

#### Matrice de l'adjoint

Soit $f$ un endomorphisme d'un espace hermitien $E$ (complexe). Sa matrice dans une base **orthonormée** $\mathcal{B}$ est $A$. Quelle est la matrice de son adjoint $f^*$ dans la même base $\mathcal{B}$ ?

- [ ] **A)** $A^{-1}$ (l'inverse de A)
- [ ] **B)** ${}^tA$ (la transposée de A)
- [x] **C)** $\overline{{}^tA}$ (la transconjuguée de A)
- [ ] **D)** $\overline{A}$ (la conjuguée de A)

<details>

<summary>Solution</summary>

**Réponse : [C]**

La question porte sur le calcul de la matrice de l'endomorphisme adjoint, qui dépend du corps ($\mathbb{R}$ ou $\mathbb{C}$).

- **A)** Incorrect. La matrice de l'adjoint n'est l'inverse que dans le cas particulier où l'endomorphisme est unitaire.

- **B)** Incorrect. La transposée ${}^tA$ est la matrice de l'adjoint dans le cas d'un espace **euclidien** (réel). L'énoncé précise qu'on est dans un espace hermitien (complexe).

- **C)** Correct. Dans un espace hermitien, la matrice de l'adjoint $f^*$ dans une base orthonormée est la transconjuguée (transposée puis conjuguée, ou l'inverse) de la matrice de $f$. Cette matrice est souvent notée $A^*$.

- **D)** Incorrect. La conjugaison seule ne suffit pas, il faut aussi transposer la matrice.

</details>

---

#### Endomorphismes orthogonaux

Soit $f$ un endomorphisme d'un espace euclidien $E$ de dimension finie. Quelles sont les affirmations équivalentes au fait que $f$ est un endomorphisme orthogonal ?

- [x] **A)** $f$ conserve le produit scalaire, i.e., $\langle f(x), f(y) \rangle = \langle x, y \rangle$ pour tous $x, y \in E$.
- [x] **B)** $f$ conserve la norme, i.e., $\|f(x)\| = \|x\|$ pour tout $x \in E$.
- [ ] **C)** La matrice de $f$ dans une base orthonormée est symétrique.
- [x] **D)** L'adjoint de $f$ est son inverse, i.e., $f^* = f^{-1}$.

<details>

<summary>Solution</summary>

**Réponses : [A, B, D]**

Un endomorphisme orthogonal (ou isométrie vectorielle) peut être défini de plusieurs manières équivalentes.

- **A)** Correct. C'est la définition la plus fondamentale. Un endomorphisme orthogonal est une transformation qui préserve la structure géométrique de l'espace, y compris les angles et les longueurs, ce qui est encapsulé dans la conservation du produit scalaire.

- **B)** Correct. En dimension finie, la conservation de la norme est équivalente à la conservation du produit scalaire (cela se démontre avec les identités de polarisation). Une transformation qui conserve les longueurs conserve aussi les angles.

- **C)** Incorrect. Une matrice symétrique dans une base orthonormée correspond à un endomorphisme **auto-adjoint**, pas à un endomorphisme orthogonal.

- **D)** Correct. La condition $\langle f(x), f(y) \rangle = \langle x, y \rangle$ peut se réécrire $\langle x, f^*(f(y)) \rangle = \langle x, y \rangle$. Comme c'est vrai pour tout $x$, cela implique $f^*(f(y))=y$ pour tout $y$, donc $f^* \circ f = \text{Id}$. En dimension finie, cela équivaut à $f^{-1} = f^*$.

</details>

---

#### Classification d'endomorphismes

Un endomorphisme $f$ d'un espace euclidien tel que $f=f^*$ est dit :

- [ ] **A)** Normal
- [x] **B)** Auto-adjoint
- [ ] **C)** Orthogonal
- [ ] **D)** Unitaire

<details>

<summary>Solution</summary>

**Réponse : [B]**

Cette question est une question de reconnaissance de vocabulaire.

- **A)** Incorrect. Un endomorphisme normal vérifie $f \circ f^* = f^* \circ f$. Tout endomorphisme auto-adjoint est normal (car si $f=f^*$, alors $f \circ f^* = f^2 = f^* \circ f$), mais la définition $f=f^*$ est plus spécifique.

- **B)** Correct. C'est la définition d'un endomorphisme auto-adjoint. Dans le cas réel, on dit aussi endomorphisme symétrique.

- **C)** Incorrect. Un endomorphisme orthogonal vérifie $f^* \circ f = \text{Id}$.

- **D)** Incorrect. Le terme "unitaire" est utilisé pour les espaces hermitiens (complexes). C'est l'équivalent de "orthogonal".

</details>

---

#### Théorème de Riesz

Qu'affirme le théorème de représentation de Riesz dans un espace euclidien $E$ de dimension finie ?

- [ ] **A)** Tout espace euclidien admet une base orthonormée.
- [ ] **B)** Tout endomorphisme est diagonalisable dans une base orthonormée.
- [ ] **C)** Il existe une correspondance unique entre les vecteurs de $E$ et les endomorphismes auto-adjoints sur $E$.
- [x] **D)** Toute forme linéaire $l$ sur $E$ peut s'écrire de façon unique comme $l(x) = \langle x, y_l \rangle$ pour un certain vecteur $y_l \in E$.

<details>

<summary>Solution</summary>

**Réponse : [D]**

Cette question teste la connaissance de l'énoncé d'un théorème important qui relie un espace à son dual.

- **A)** Incorrect. C'est un résultat important, mais il découle du procédé de Gram-Schmidt, ce n'est pas le théorème de Riesz.

- **B)** Incorrect. C'est une version simplifiée du **théorème spectral**, qui ne s'applique qu'à une classe particulière d'endomorphismes (les endomorphismes normaux).

- **C)** Incorrect. Le théorème de Riesz établit une correspondance entre les vecteurs et les **formes linéaires**, pas les endomorphismes.

- **D)** Correct. C'est précisément l'énoncé du théorème. Il établit un isomorphisme canonique entre un espace euclidien $E$ et son dual $E^*$, où chaque forme linéaire est "représentée" par un produit scalaire avec un vecteur unique.

</details>
