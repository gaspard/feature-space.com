---
id: e9d6c32b
type: cards
order: 22
title: Normes sur Rⁿ et suites convergentes - fiches de révision (B)
tags:
  - normes
  - suites
  - convergence
  - topologie
  - analyse
createdAt: '2025-10-12T16:01:06.272Z'
level: pro
course: Topologie I
courseId: 34e61f8e
chapter: Normes sur Rⁿ et suites convergentes
chapterId: 629d2832
---
# Fiches de révision "Normes sur Rⁿ et suites convergentes" (B)

---

Démontrer que pour toute norme $N$ sur un espace vectoriel $E$, on a l'inégalité triangulaire renversée : $|N(x) - N(y)| \le N(x - y)$ pour tous $x, y \in E$.

<details>

<summary>Réponse</summary>

La démonstration repose sur une application judicieuse de l'inégalité triangulaire standard.

1.  On écrit $x$ comme $x = (x - y) + y$. En appliquant l'inégalité triangulaire, on obtient :

    $N(x) = N((x - y) + y) \le N(x - y) + N(y)$.

    En réarrangeant les termes, cela nous donne une première inégalité :

    $N(x) - N(y) \le N(x - y)$.

2.  De manière symétrique, on écrit $y$ comme $y = (y - x) + x$. L'inégalité triangulaire donne :

    $N(y) = N((y - x) + x) \le N(y - x) + N(x)$.

    En utilisant la propriété d'homogénéité absolue, $N(y-x) = N((-1)(x-y)) = |-1|N(x-y) = N(x-y)$.

    L'inégalité devient donc $N(y) \le N(x - y) + N(x)$, ce qui se réarrange en :

    $N(y) - N(x) \le N(x - y)$.

    Ceci est équivalent à $-(N(x) - N(y)) \le N(x - y)$.

3.  En combinant les deux inégalités obtenues, $N(x) - N(y) \le N(x - y)$ et $-(N(x) - N(y)) \le N(x - y)$, on conclut que :

    $|N(x) - N(y)| \le N(x - y)$.

</details>

---

Énoncer le théorème de l'équivalence des normes en dimension finie et esquisser les grandes lignes de sa preuve.

<details>

<summary>Réponse</summary>

**Théorème :** Sur un espace vectoriel réel de dimension finie, toutes les normes sont équivalentes.

**Esquisse de la preuve :**

Soit $E$ un $\mathbb{R}$-espace vectoriel de dimension finie $n$. Il suffit de montrer que toute norme $N$ sur $E$ est équivalente à une norme de référence, par exemple la norme infinie $\|\cdot\|_\infty$ relative à une base fixée $(e_1, \dots, e_n)$.

Soit $x = \sum_{i=1}^n x_i e_i \in E$, on pose $\|x\|_\infty = \max_{1 \le i \le n} |x_i|$. On doit trouver $\alpha, \beta > 0$ tels que $\alpha \|x\|_\infty \le N(x) \le \beta \|x\|_\infty$.

1.  **Majoration de $N(x)$ :**

    En utilisant l'inégalité triangulaire et l'homogénéité de $N$ :

    $N(x) = N(\sum x_i e_i) \le \sum N(x_i e_i) = \sum |x_i| N(e_i)$.

    Comme $|x_i| \le \|x\|_\infty$ pour tout $i$, on a :

    $N(x) \le \sum \|x\|_\infty N(e_i) = \|x\|_\infty \left( \sum N(e_i) \right)$.

    En posant $\beta = \sum_{i=1}^n N(e_i)$, qui est une constante positive finie, on a $N(x) \le \beta \|x\|_\infty$.

2.  **Minoration de $N(x)$ :**

    C'est la partie la plus délicate. Elle repose sur un argument de compacité.

    -   On montre d'abord que l'application $N: (E, \|\cdot\|_\infty) \to \mathbb{R}$ est continue. En utilisant l'inégalité triangulaire renversée et la majoration déjà établie :

        $|N(x) - N(y)| \le N(x-y) \le \beta \|x-y\|_\infty$.

        Ceci prouve que $N$ est $\beta$-lipschitzienne, donc continue.

    -   Considérons la sphère unité pour la norme infinie : $S_\infty = \{x \in E \mid \|x\|_\infty = 1\}$.
    -   En dimension finie, la sphère unité $S_\infty$ est une partie fermée et bornée. Par le théorème de Borel-Lebesgue, $S_\infty$ est compacte.
    -   La fonction $N$, étant continue sur le compact $S_\infty$, y atteint son minimum. Soit $\alpha = \min_{x \in S_\infty} N(x)$.
    -   Par la propriété de séparation de la norme, $N(x) > 0$ pour tout $x \in S_\infty$ (car $x \neq 0_E$). Comme $S_\infty$ est compact, ce minimum est strictement positif : $\alpha > 0$.
    -   Pour tout $x \in E, x \neq 0_E$, le vecteur $u = \frac{x}{\|x\|_\infty}$ est sur la sphère $S_\infty$.
    -   On a donc $N(u) \ge \alpha$. Par homogénéité de $N$:

        $N\left(\frac{x}{\|x\|_\infty}\right) = \frac{N(x)}{\|x\|_\infty} \ge \alpha \implies N(x) \ge \alpha \|x\|_\infty$.

    L'inégalité est aussi vraie pour $x=0_E$.

On a bien montré l'existence de $\alpha, \beta > 0$ vérifiant l'encadrement.

</details>

---

Comment prouver l'inégalité de Cauchy-Schwarz dans $\mathbb{R}^n$ et quel est le cas d'égalité ?

<details>

<summary>Réponse</summary>

**Proposition (Inégalité de Cauchy-Schwarz) :**

Pour tous vecteurs $x, y \in \mathbb{R}^n$, $|\langle x, y \rangle| \le \|x\|_2 \|y\|_2$. L'égalité a lieu si et seulement si $x$ et $y$ sont colinéaires.

**Démonstration :**

La preuve repose sur l'étude du signe d'un polynôme du second degré.

1.  Pour tout scalaire $t \in \mathbb{R}$, considérons la norme au carré du vecteur $x+ty$ :

    $P(t) = \|x + ty\|_2^2 = \langle x+ty, x+ty \rangle$.

2.  Par la bilinéarité du produit scalaire, on développe l'expression :

    $P(t) = \langle x,x \rangle + 2t\langle x,y \rangle + t^2\langle y,y \rangle = \|x\|_2^2 + 2\langle x, y \rangle t + \|y\|_2^2 t^2$.

3.  Par définition d'une norme, $\|x+ty\|_2^2 \ge 0$ pour tout $t \in \mathbb{R}$. Donc, le polynôme $P(t)$ est toujours positif ou nul.

4.  Un polynôme du second degré $At^2 + Bt + C$ est de signe constant si et seulement si son discriminant $\Delta = B^2 - 4AC$ est négatif ou nul.

    Ici, $A = \|y\|_2^2$, $B = 2\langle x, y \rangle$, et $C = \|x\|_2^2$.

    Le discriminant est $\Delta = (2\langle x, y \rangle)^2 - 4(\|y\|_2^2)(\|x\|_2^2) \le 0$.

5.  Cette inégalité se simplifie en $4\langle x, y \rangle^2 \le 4\|x\|_2^2 \|y\|_2^2$, ce qui donne $\langle x, y \rangle^2 \le \|x\|_2^2 \|y\|_2^2$. En prenant la racine carrée, on obtient $|\langle x, y \rangle| \le \|x\|_2 \|y\|_2$.

**Cas d'égalité :**

L'égalité a lieu si et seulement si $\Delta = 0$.

-   Si $y = 0_E$, l'égalité est triviale ($0=0$) et les vecteurs sont colinéaires.
-   Si $y \neq 0_E$, $\Delta = 0$ signifie que le polynôme $P(t)$ a une racine réelle unique $t_0 = -\frac{\langle x, y \rangle}{\|y\|_2^2}$.
-   Pour cette valeur $t_0$, on a $P(t_0) = \|x + t_0 y\|_2^2 = 0$.
-   Par la propriété de séparation de la norme, $\|x + t_0 y\|_2 = 0$ est équivalent à $x + t_0 y = 0_E$.
-   Ceci signifie que $x = -t_0 y$, donc les vecteurs $x$ et $y$ sont colinéaires.

Réciproquement, si $x = \lambda y$ pour un scalaire $\lambda$, on peut vérifier que $|\langle \lambda y, y \rangle| = |\lambda| \|y\|_2^2$ et $\|\lambda y\|_2 \|y\|_2 = |\lambda| \|y\|_2^2$, donc il y a égalité.

</details>

---

En utilisant l'identité du parallélogramme, montrez que la norme 1, $\|x\|_1 = \sum_{i=1}^n |x_i|$, sur $\mathbb{R}^n$ (pour $n \ge 2$) n'est pas induite par un produit scalaire.

<details>

<summary>Réponse</summary>

Une norme $\|\cdot\|$ est induite par un produit scalaire si et seulement si elle vérifie l'**identité du parallélogramme** pour tous les vecteurs $x, y$ de l'espace :

$$ \|x+y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2) $$

Pour montrer que $\|\cdot\|_1$ n'est pas induite par un produit scalaire, il suffit de trouver un contre-exemple à cette identité.

Prenons l'espace $\mathbb{R}^n$ avec $n \ge 2$. Choisissons deux vecteurs simples de la base canonique, par exemple $e_1 = (1, 0, \dots, 0)$ et $e_2 = (0, 1, \dots, 0)$.

Calculons les termes de l'identité pour $x = e_1$ et $y = e_2$ avec la norme $\|\cdot\|_1$ :

-   $\|e_1\|_1 = |1| + |0| + \dots = 1$.
-   $\|e_2\|_1 = |0| + |1| + \dots = 1$.
-   $e_1 + e_2 = (1, 1, 0, \dots, 0)$, donc $\|e_1 + e_2\|_1 = |1| + |1| + \dots = 2$.
-   $e_1 - e_2 = (1, -1, 0, \dots, 0)$, donc $\|e_1 - e_2\|_1 = |1| + |-1| + \dots = 2$.

Maintenant, évaluons les deux côtés de l'identité du parallélogramme :

-   **Membre de gauche :** $\|e_1+e_2\|_1^2 + \|e_1-e_2\|_1^2 = 2^2 + 2^2 = 4 + 4 = 8$.
-   **Membre de droite :** $2(\|e_1\|_1^2 + \|e_2\|_1^2) = 2(1^2 + 1^2) = 2(1 + 1) = 4$.

Puisque $8 \neq 4$, l'identité du parallélogramme n'est pas vérifiée. Par conséquent, la norme $\|\cdot\|_1$ sur $\mathbb{R}^n$ (pour $n \ge 2$) n'est induite par aucun produit scalaire.

</details>

---

Expliquez la relation entre une norme et la géométrie de sa boule unité, en mentionnant les concepts de convexité, symétrie et absorption. Comment la jauge de Minkowski formalise-t-elle cette relation ?

<details>

<summary>Réponse</summary>

La géométrie de la boule unité fermée $B = \{x \in E \mid \|x\| \le 1\}$ d'un espace vectoriel normé $(E, \|\cdot\|)$ capture entièrement les propriétés de la norme.

**Propriétés géométriques de la boule unité :**

1.  **Convexité :** $B$ est un ensemble convexe.

    *Démonstration :* Soient $x, y \in B$ (i.e., $\|x\| \le 1$ et $\|y\| \le 1$) et $t \in [0, 1]$. On doit montrer que $tx + (1-t)y \in B$.

    Par l'inégalité triangulaire et l'homogénéité :

    $\|tx + (1-t)y\| \le \|tx\| + \|(1-t)y\| = t\|x\| + (1-t)\|y\| \le t(1) + (1-t)(1) = 1$.

    Donc $tx + (1-t)y \in B$.

2.  **Symétrie par rapport à l'origine (ensemble équilibré) :** Si $x \in B$, alors $-x \in B$.

    *Démonstration :* Si $\|x\| \le 1$, alors $\|-x\| = |-1|\|x\| = \|x\| \le 1$, donc $-x \in B$. Plus généralement, si $|\lambda| \le 1$, alors $\lambda x \in B$.

3.  **Partie absorbante contenant l'origine en son intérieur :** L'origine $0_E$ est dans l'intérieur de $B$. Cela signifie qu'il existe un "voisinage" de $0_E$ entièrement contenu dans $B$. De manière équivalente, pour tout $x \in E$, il existe $\lambda > 0$ tel que $\lambda x \in B$.

**La jauge de Minkowski (ou fonctionnelle de jauge) :**

La relation est en fait une dualité. Réciproquement, toute partie $C \subset E$ qui est convexe, équilibrée, et absorbante peut être utilisée pour définir une norme. Cette norme est appelée la **jauge de Minkowski** de $C$, notée $p_C$, et est définie par :

$$ p_C(x) = \inf \{ \lambda > 0 \mid x \in \lambda C \} $$

où $\lambda C = \{\lambda y \mid y \in C\}$.

-   Si $C$ possède ces propriétés, alors $p_C$ est une semi-norme.
-   Si, de plus, $C$ est bornée (au sens topologique usuel, par exemple dans $\mathbb{R}^n$), alors $p_C$ est une norme.

La boule unité de la norme $p_C$ est précisément l'ensemble $C$ (ou son adhérence si $C$ n'est pas fermée). Ainsi, la notion de norme et celle d'ensemble convexe, équilibré et absorbant sont intimement liées.

</details>

---

Démontrer que si deux normes $N_1$ et $N_2$ sur un espace $E$ sont équivalentes, alors une suite $(x^k)$ converge vers $a$ pour $N_1$ si et seulement si elle converge vers $a$ pour $N_2$.

<details>

<summary>Réponse</summary>

**Hypothèse :** Les normes $N_1$ et $N_2$ sont équivalentes. Il existe donc deux constantes réelles $\alpha, \beta > 0$ telles que pour tout $x \in E$ :

$$ \alpha N_1(x) \le N_2(x) \le \beta N_1(x) $$

**$(\implies)$ Supposons que $(x^k)$ converge vers $a$ pour la norme $N_1$.**

Par définition, cela signifie que $\lim_{k \to \infty} N_1(x^k - a) = 0$.

Formellement, $\forall \varepsilon > 0, \exists K \in \mathbb{N}$ tel que pour tout $k \ge K$, $N_1(x^k - a) < \varepsilon$.

Nous voulons montrer que $\lim_{k \to \infty} N_2(x^k - a) = 0$.

En utilisant la partie droite de l'inégalité d'équivalence sur le vecteur $x^k - a$, on a :

$$ 0 \le N_2(x^k - a) \le \beta N_1(x^k - a) $$

Puisque $N_1(x^k - a) \to 0$ et $\beta$ est une constante positive, le produit $\beta N_1(x^k - a)$ tend également vers 0.

Par le théorème des gendarmes (ou théorème d'encadrement), on conclut que $\lim_{k \to \infty} N_2(x^k - a) = 0$.

La suite $(x^k)$ converge donc bien vers $a$ pour la norme $N_2$.

**$(\impliedby)$ Supposons que $(x^k)$ converge vers $a$ pour la norme $N_2$.**

La démarche est symétrique. Par définition, $\lim_{k \to \infty} N_2(x^k - a) = 0$.

Nous voulons montrer que $\lim_{k \to \infty} N_1(x^k - a) = 0$.

En utilisant la partie gauche de l'inégalité d'équivalence, $\alpha N_1(x^k - a) \le N_2(x^k - a)$, on peut la réécrire comme :

$$ 0 \le N_1(x^k - a) \le \frac{1}{\alpha} N_2(x^k - a) $$

Puisque $N_2(x^k - a) \to 0$ et $1/\alpha$ est une constante positive, le produit $\frac{1}{\alpha} N_2(x^k - a)$ tend vers 0.

De nouveau, par le théorème des gendarmes, on conclut que $\lim_{k \to \infty} N_1(x^k - a) = 0$.

La suite $(x^k)$ converge donc bien vers $a$ pour la norme $N_1$.

**Conclusion :** L'équivalence des normes préserve la notion de convergence et la limite elle-même. C'est pourquoi en dimension finie, où toutes les normes sont équivalentes, la topologie (et donc les notions de convergence, continuité, ouverts, etc.) est unique.

</details>

---

Prouver que la convergence d'une suite de vecteurs dans $\mathbb{R}^n$ est équivalente à la convergence de chacune de ses suites de coordonnées.

<details>

<summary>Réponse</summary>

Soit $(x^k)_{k \in \mathbb{N}}$ une suite de vecteurs de $\mathbb{R}^n$, où $x^k = (x_1^k, x_2^k, \dots, x_n^k)$, et soit $a = (a_1, a_2, \dots, a_n) \in \mathbb{R}^n$. Nous devons prouver l'équivalence suivante :

$$ \lim_{k \to \infty} x^k = a \iff \forall j \in \{1, \dots, n\}, \lim_{k \to \infty} x_j^k = a_j $$

La preuve repose sur l'utilisation de la norme infinie $\|\cdot\|_\infty$ et de ses propriétés. La convergence dans $\mathbb{R}^n$ étant indépendante de la norme choisie (car toutes sont équivalentes), nous pouvons travailler avec $\|\cdot\|_\infty$ sans perte de généralité.

Par définition, $\lim_{k \to \infty} x^k = a$ est équivalent à $\lim_{k \to \infty} \|x^k - a\|_\infty = 0$.

Rappelons que $\|x^k - a\|_\infty = \max_{1 \le j \le n} |x_j^k - a_j|$.

**$(\implies)$ Convergence vectorielle implique convergence des coordonnées :**

Supposons que $\lim_{k \to \infty} x^k = a$, c'est-à-dire $\lim_{k \to \infty} \|x^k - a\|_\infty = 0$.

Pour n'importe quel indice $j \in \{1, \dots, n\}$, nous avons l'inégalité suivante par définition du maximum :

$$ 0 \le |x_j^k - a_j| \le \max_{1 \le i \le n} |x_i^k - a_i| = \|x^k - a\|_\infty $$

Puisque $\|x^k - a\|_\infty \to 0$ lorsque $k \to \infty$, le théorème d'encadrement (ou des gendarmes) implique que $|x_j^k - a_j| \to 0$ pour chaque $j$.

Cela signifie que $\lim_{k \to \infty} x_j^k = a_j$ pour tout $j \in \{1, \dots, n\}$.

**$(\impliedby)$ Convergence des coordonnées implique convergence vectorielle :**

Supposons que pour tout $j \in \{1, \dots, n\}$, on a $\lim_{k \to \infty} x_j^k = a_j$.

Cela signifie que $\lim_{k \to \infty} |x_j^k - a_j| = 0$ pour chaque $j$.

Nous voulons montrer que $\lim_{k \to \infty} \|x^k - a\|_\infty = 0$.

Soit $\varepsilon > 0$. Pour chaque $j \in \{1, \dots, n\}$, il existe un rang $K_j \in \mathbb{N}$ tel que pour tout $k \ge K_j$, on a $|x_j^k - a_j| < \varepsilon$.

Posons $K = \max(K_1, K_2, \dots, K_n)$. Pour tout $k \ge K$, l'inégalité $|x_j^k - a_j| < \varepsilon$ est vraie simultanément pour *tous* les $j \in \{1, \dots, n\}$.

Par conséquent, pour $k \ge K$, on a :

$$ \|x^k - a\|_\infty = \max_{1 \le j \le n} |x_j^k - a_j| < \varepsilon $$

Ceci est la définition formelle de $\lim_{k \to \infty} \|x^k - a\|_\infty = 0$. La suite $(x^k)$ converge donc vers $a$.

</details>

---

Construire une suite de fonctions dans l'espace $C([0, 1], \mathbb{R})$ qui démontre que les normes $\| \cdot \|_\infty$ et $\| \cdot \|_1$ ne sont pas équivalentes.

<details>

<summary>Réponse</summary>

Pour montrer que les normes $\|f\|_\infty = \sup_{t \in [0, 1]} |f(t)|$ et $\|f\|_1 = \int_0^1 |f(t)|dt$ ne sont pas équivalentes sur $C([0, 1], \mathbb{R})$, il faut montrer qu'il n'existe pas de constante $\alpha > 0$ telle que $\alpha \|f\|_\infty \le \|f\|_1$ pour toute fonction $f$. Pour ce faire, nous construisons une suite de fonctions $(f_n)_{n \in \mathbb{N}}$ pour laquelle le rapport $\frac{\|f_n\|_1}{\|f_n\|_\infty}$ tend vers 0.

Considérons la suite de fonctions $(f_n)_{n \ge 1}$ définies par $f_n(t) = t^n$.

Chaque $f_n$ est un polynôme, donc elle est continue sur $[0, 1]$.

Calculons les normes $\|\cdot\|_\infty$ et $\|\cdot\|_1$ pour $f_n$:

1.  **Norme infinie ($\| \cdot \|_\infty$) :**

    La fonction $f_n(t) = t^n$ est croissante et positive sur $[0, 1]$. Son maximum est donc atteint en $t=1$.

    $\|f_n\|_\infty = \sup_{t \in [0, 1]} |t^n| = 1^n = 1$.

    La norme infinie de chaque fonction de la suite est constante et égale à 1.

2.  **Norme 1 ($\| \cdot \|_1$) :**

    On calcule l'intégrale de $|f_n(t)|$. Puisque $t \ge 0$ sur $[0,1]$, $|t^n|=t^n$.

    $\|f_n\|_1 = \int_0^1 t^n dt = \left[ \frac{t^{n+1}}{n+1} \right]_0^1 = \frac{1}{n+1} - 0 = \frac{1}{n+1}$.

    La norme 1 de la fonction $f_n$ tend vers 0 lorsque $n \to \infty$.

**Argument de non-équivalence :**

Supposons par l'absurde que les normes sont équivalentes. Il existerait alors une constante $\alpha > 0$ telle que pour toute fonction $f \in C([0, 1], \mathbb{R})$:

$$ \alpha \|f\|_\infty \le \|f\|_1 $$

Appliquons cette inégalité à notre suite de fonctions $(f_n)$:

$$ \alpha \|f_n\|_\infty \le \|f_n\|_1 \implies \alpha \cdot 1 \le \frac{1}{n+1} $$

Cette inégalité, $\alpha \le \frac{1}{n+1}$, devrait être vraie pour tout $n \ge 1$.

Cependant, lorsque $n \to \infty$, le terme $\frac{1}{n+1}$ tend vers 0.

En passant à la limite, on obtiendrait $\alpha \le 0$.

Ceci contredit notre hypothèse que $\alpha$ est une constante *strictement positive*.

La supposition de l'équivalence des normes mène à une contradiction. Donc, les normes $\| \cdot \|_\infty$ et $\| \cdot \|_1$ ne sont pas équivalentes sur $C([0, 1], \mathbb{R})$.

</details>

---

Démontrer que dans tout espace vectoriel normé $(E, \|\cdot\|)$, une suite convergente est nécessairement une suite de Cauchy.

<details>

<summary>Réponse</summary>

Soit $(x^k)_{k \in \mathbb{N}}$ une suite d'éléments de $E$ qui converge vers une limite $a \in E$.

Par définition de la convergence, on a :

$$ \forall \varepsilon' > 0, \exists K \in \mathbb{N}, \forall k \ge K \implies \|x^k - a\| < \varepsilon' $$

Nous voulons démontrer que $(x^k)$ est une suite de Cauchy, c'est-à-dire :

$$ \forall \varepsilon > 0, \exists K' \in \mathbb{N}, \forall p, q \ge K' \implies \|x^p - x^q\| < \varepsilon $$

**Démonstration :**

Soit $\varepsilon > 0$ un réel quelconque.

Posons $\varepsilon' = \varepsilon/2$. Puisque $\varepsilon' > 0$, la définition de la convergence de $(x^k)$ vers $a$ nous assure qu'il existe un rang $K \in \mathbb{N}$ tel que pour tout indice $k \ge K$, on a :

$$ \|x^k - a\| < \frac{\varepsilon}{2} $$

Maintenant, considérons deux indices quelconques $p$ et $q$ tels que $p \ge K$ et $q \ge K$.

Nous voulons majorer la distance $\|x^p - x^q\|$. Pour ce faire, nous utilisons l'astuce d'introduire la limite $a$ et d'appliquer l'inégalité triangulaire :

$$ \|x^p - x^q\| = \|(x^p - a) + (a - x^q)\| \le \|x^p - a\| + \|a - x^q\| $$

Par homogénéité, $\|a - x^q\| = \|(-1)(x^q - a)\| = |-1|\|x^q - a\| = \|x^q - a\|$.

L'inégalité devient :

$$ \|x^p - x^q\| \le \|x^p - a\| + \|x^q - a\| $$

Puisque $p \ge K$ et $q \ge K$, nous pouvons utiliser la propriété issue de la convergence :

-   $\|x^p - a\| < \varepsilon/2$
-   $\|x^q - a\| < \varepsilon/2$

En substituant ces majorations, on obtient :

$$ \|x^p - x^q\| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon $$

Nous avons donc montré que pour tout $\varepsilon > 0$, en choisissant $K' = K$ (le rang donné par la convergence pour $\varepsilon/2$), on a bien :

$$ \forall p, q \ge K', \quad \|x^p - x^q\| < \varepsilon $$

Ceci est précisément la définition d'une suite de Cauchy.

</details>

---

Pourquoi l'application $f(x) = (\sum_{i=1}^n |x_i|^p)^{1/p}$ pour $0 < p < 1$ ne définit-elle pas une norme sur $\mathbb{R}^n$ (pour $n \ge 2$) ?

<details>

<summary>Réponse</summary>

L'application $f(x) = \|x\|_p$ pour $0 < p < 1$ ne définit pas une norme car elle viole l'**inégalité triangulaire**. Les axiomes de séparation et d'homogénéité absolue sont, eux, bien vérifiés.

Pour le démontrer, il suffit de fournir un contre-exemple explicite dans $\mathbb{R}^n$ (avec $n \ge 2$).

Prenons l'espace $\mathbb{R}^2$ et choisissons $p = 1/2$.

Considérons les vecteurs de la base canonique $x = (1, 0)$ et $y = (0, 1)$.

Calculons les "p-normes" de ces vecteurs et de leur somme :

-   **Pour $x = (1, 0)$ :**

    $\|x\|_{1/2} = (|1|^{1/2} + |0|^{1/2})^{1/(1/2)} = (1 + 0)^2 = 1^2 = 1$.

-   **Pour $y = (0, 1)$ :**

    $\|y\|_{1/2} = (|0|^{1/2} + |1|^{1/2})^{1/(1/2)} = (0 + 1)^2 = 1^2 = 1$.

-   **Pour la somme $x+y = (1, 1)$ :**

    $\|x+y\|_{1/2} = (|1|^{1/2} + |1|^{1/2})^{1/(1/2)} = (1 + 1)^2 = 2^2 = 4$.

Maintenant, testons l'inégalité triangulaire $\|x+y\|_p \le \|x\|_p + \|y\|_p$ :

-   $\|x+y\|_{1/2} = 4$.
-   $\|x\|_{1/2} + \|y\|_{1/2} = 1 + 1 = 2$.

On observe que $4 > 2$, c'est-à-dire :

$$ \|x+y\|_{1/2} > \|x\|_{1/2} + \|y\|_{1/2} $$

L'inégalité triangulaire est violée. Par conséquent, $\|x\|_p$ pour $0 < p < 1$ n'est pas une norme.

**Note conceptuelle :**

La fonction $t \mapsto t^p$ est concave pour $0 < p < 1$, alors qu'elle est convexe pour $p \ge 1$. La convexité est la propriété sous-jacente qui garantit l'inégalité triangulaire pour les normes p (via l'inégalité de Minkowski). L'échec de la convexité pour $p<1$ conduit à l'échec de l'inégalité triangulaire. Ces objets sont parfois appelés des **quasi-normes**.

</details>
